{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='./embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_index(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_index(self, file_path):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ce8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSBasic(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            query_embedding = self.embedder_model.batch_embed([query])\n",
    "            if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                query_embedding = query_embedding[0]\n",
    "            elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        results = self._search_chunks(query_embedding, top_k)\n",
    "        \n",
    "        return [\n",
    "            Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "            for res in results\n",
    "        ]\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed1060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the FAISS vectorstore\\nfaiss_store = FAISS(embedder_model=your_embedding_model)\\n\\n# Create vectorstore from Document objects (new method)\\nfaiss_store.create_vectorstore(document_list, normalize_embeddings=True)\\n\\n# Or use the original method\\nfaiss_store.create_vector_store(document_list, your_embedding_model)\\n\\n# Search for relevant documents\\nresults = faiss_store.get_relevant_documents(\"your query\", top_k=5)\\n\\n# Save and load\\nfaiss_store.save_index(\"my_index\")\\nfaiss_store.load_index(\"my_index\", your_embedding_model)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FAISSImproved(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "        # New attributes for enhanced functionality\n",
    "        self.docstore = None\n",
    "        self.index_to_docstore_id = None\n",
    "        self.documents = None  # Store original Document objects\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def create_vectorstore(self, docs, normalize_embeddings=True):\n",
    "        \"\"\"\n",
    "        Create a FAISS vector store from a list of Document objects.\n",
    "        Each document should have metadata like pdf_id, chunk_id, etc.\n",
    "        \n",
    "        Args:\n",
    "            docs: List of Document objects\n",
    "            normalize_embeddings: Whether to normalize embeddings for cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            self: Returns the FAISS instance for method chaining\n",
    "        \"\"\"\n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required. Set it during initialization or call set_embedder_model()\")\n",
    "        \n",
    "        # Extract texts from Document objects\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        # Initialize FAISS Index\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity if requested\n",
    "        if normalize_embeddings:\n",
    "            faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store original Document objects and create mappings\n",
    "        self.documents = docs\n",
    "        self.docstore = {str(i): doc for i, doc in enumerate(docs)}\n",
    "        self.index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "        \n",
    "        # Also maintain backward compatibility with chunks_dict\n",
    "        self.chunks_dict = {i: doc.page_content for i, doc in enumerate(docs)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created vectorstore with {self.total_vectors} documents of dim {self.dimension}\")\n",
    "        print(f\"[FAISS] Normalization: {'enabled' if normalize_embeddings else 'disabled'}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() or create_vectorstore() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            # Use embed_query if available, otherwise fall back to batch_embed\n",
    "            if hasattr(self.embedder_model, 'embed_query'):\n",
    "                query_embedding = self.embedder_model.embed_query(query)\n",
    "            else:\n",
    "                query_embedding = self.embedder_model.batch_embed([query])\n",
    "                if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                    query_embedding = query_embedding[0]\n",
    "                elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                    query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        if self.docstore is not None:\n",
    "            # Use enhanced docstore-based retrieval\n",
    "            results = self._search_with_docstore(query_embedding, top_k)\n",
    "        else:\n",
    "            # Fall back to original chunk-based retrieval\n",
    "            results = self._search_chunks(query_embedding, top_k)\n",
    "            # Convert to Document objects for consistency\n",
    "            results = [\n",
    "                Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "                for res in results\n",
    "            ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _search_with_docstore(self, query_embedding, top_k=5):\n",
    "        \"\"\"Enhanced search function using docstore - returns Document objects\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results using docstore\n",
    "        documents = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx in self.index_to_docstore_id:\n",
    "                docstore_id = self.index_to_docstore_id[faiss_idx]\n",
    "                if docstore_id in self.docstore:\n",
    "                    doc = self.docstore[docstore_id]\n",
    "                    similarity = float(distances[0][i])\n",
    "                    \n",
    "                    # Create a copy of the document with updated metadata\n",
    "                    enhanced_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                    enhanced_metadata[\"similarity\"] = similarity\n",
    "                    enhanced_metadata[\"retrieval_index\"] = faiss_idx\n",
    "                    \n",
    "                    enhanced_doc = Document(\n",
    "                        page_content=doc.page_content,\n",
    "                        metadata=enhanced_metadata\n",
    "                    )\n",
    "                    documents.append(enhanced_doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata (enhanced to include new attributes)\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type,\n",
    "            'docstore': self.docstore,\n",
    "            'index_to_docstore_id': self.index_to_docstore_id,\n",
    "            'documents': self.documents\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Load enhanced attributes if they exist (backward compatibility)\n",
    "        self.docstore = metadata.get('docstore', None)\n",
    "        self.index_to_docstore_id = metadata.get('index_to_docstore_id', None)\n",
    "        self.documents = metadata.get('documents', None)\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        if self.docstore is not None:\n",
    "            print(f\"[FAISS] Enhanced docstore mode enabled\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None,\n",
    "            'has_docstore': self.docstore is not None,\n",
    "            'has_documents': self.documents is not None\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize the FAISS vectorstore\n",
    "faiss_store = FAISS(embedder_model=your_embedding_model)\n",
    "\n",
    "# Create vectorstore from Document objects (new method)\n",
    "faiss_store.create_vectorstore(document_list, normalize_embeddings=True)\n",
    "\n",
    "# Or use the original method\n",
    "faiss_store.create_vector_store(document_list, your_embedding_model)\n",
    "\n",
    "# Search for relevant documents\n",
    "results = faiss_store.get_relevant_documents(\"your query\", top_k=5)\n",
    "\n",
    "# Save and load\n",
    "faiss_store.save_index(\"my_index\")\n",
    "faiss_store.load_index(\"my_index\", your_embedding_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f5d36",
   "metadata": {},
   "source": [
    "### LLM Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2c89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        self.model_name = model_name\n",
    "        self.cache_folder = cache_folder\n",
    "        self.device = ('cpu'\n",
    "            # 'cuda' if torch.cuda.is_available()\n",
    "            # else 'mps' if torch.backends.mps.is_available()\n",
    "            # else 'cpu'\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OLLAMA_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n",
    "        return model\n",
    "\n",
    "\n",
    "class Hugging_Face_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"  \n",
    "        )\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab7c4d",
   "metadata": {},
   "source": [
    "## Strategy Pattern Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71cb45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStrategy(ABC):\n",
    "    \"\"\"Abstract base class defining the strategy interface.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self, *args, **kwargs):\n",
    "        \"\"\"Execute the strategy. Must be implemented by concrete strategies.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5638e6",
   "metadata": {},
   "source": [
    "#### Chatting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "469a4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChattingStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self._build_chain()\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Source {i} | PDF {doc.metadata.get('pdf_id', '?')}]: {doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, 1)\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Please provide a comprehensive answer based on the context above. You MUST follow this exact format:\n",
    "\n",
    "            RESPONSE:\n",
    "            [Your main answer here]\n",
    "\n",
    "            REASONING:\n",
    "            [Explain your reasoning and how you used the context]\n",
    "\n",
    "            SOURCES:\n",
    "            [List the source numbers you referenced, for example: 1, 3, 5]\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        def retrieve_context(inputs):\n",
    "            docs = self.vector_store.get_relevant_documents(inputs[\"question\"], top_k=self.top_k)\n",
    "            return self.format_docs(docs)\n",
    "\n",
    "        self.chain = ({\n",
    "                \"context\": RunnableLambda(retrieve_context), \n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def parse_structured_response(self, response_text):\n",
    "        cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)\n",
    "        cleaned_response = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_response.strip())\n",
    "\n",
    "        sections = {'response': '', 'reasoning': '', 'sources': ''}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        lines = cleaned_response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith('RESPONSE:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'response'\n",
    "                current_content = [line[9:].strip()]\n",
    "            elif line.upper().startswith('REASONING:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'reasoning'\n",
    "                current_content = [line[10:].strip()]\n",
    "            elif line.upper().startswith('SOURCES:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'sources'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif current_section and line:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "\n",
    "        source_ids = [int(x) for x in re.findall(r'\\d+', sections['sources'])] if sections['sources'] else []\n",
    "\n",
    "        return {\n",
    "            'answer': sections['response'],\n",
    "            'reasoning': sections['reasoning'],\n",
    "            'sources': source_ids,\n",
    "            'raw_response': cleaned_response\n",
    "        }\n",
    "\n",
    "    def validate_input(self, question):\n",
    "        \"\"\"Validate that the question is a non-empty string.\"\"\"\n",
    "        return isinstance(question, str) and len(question.strip()) > 0\n",
    "\n",
    "    def run(self, question):\n",
    "        \"\"\"Main method to run the chain and parse result.\"\"\"\n",
    "        if not self.validate_input(question):\n",
    "            raise ValueError(\"Question must be a non-empty string\")\n",
    "        \n",
    "        response = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        parsed = self.parse_structured_response(response)\n",
    "        print(f\"Parsed response: {parsed}\")  \n",
    "\n",
    "    \n",
    "        source_docs = self.vector_store.get_relevant_documents(question, top_k=self.top_k)\n",
    "        parsed['source_documents'] = source_docs\n",
    "        parsed['source_texts'] = [doc.page_content for doc in source_docs]\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71af65b",
   "metadata": {},
   "source": [
    "#### Summerization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "769e1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationStrategy(TaskStrategy):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Summarize the following document using this format:\n",
    "\n",
    "            **Main Topic:** [One sentence describing what this document is about]\n",
    "\n",
    "            **Key Points:**\n",
    "            - [Most important point]\n",
    "            - [Second most important point]  \n",
    "            - [Third most important point]\n",
    "\n",
    "            **Details:** [Supporting information, numbers, examples]\n",
    "\n",
    "            **Conclusion:** [Main takeaway or implication]\n",
    "\n",
    "            Document: {context}\"\"\")\n",
    "                    ])\n",
    "\n",
    "    def validate_input(self, document):\n",
    "        \"\"\"Validate that the document is a non-empty string.\"\"\"\n",
    "        return isinstance(document, str) and len(document.strip()) > 0\n",
    "\n",
    "    def run(self, document):\n",
    "        \"\"\"Summarize the given document.\"\"\"            \n",
    "        # Format prompt manually\n",
    "        formatted_prompt = self.prompt.format(context=document)\n",
    "        \n",
    "        # Directly invoke the LLM\n",
    "        result = self.llm.invoke(formatted_prompt)\n",
    "        \n",
    "        print(result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deff017",
   "metadata": {},
   "source": [
    "#### Question Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57ad9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionStrategy(TaskStrategy):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant tasked with generating question-answer pairs for study purposes.\n",
    "\n",
    "Text:\n",
    "{context}\n",
    "\n",
    "Generate 5-6 meaningful questions based only on the above text. \n",
    "\n",
    "IMPORTANT: Format your output exactly as shown below with no additional text, explanations, or formatting:\n",
    "\n",
    "Q1: [question text]\n",
    "Q2: [question text]\n",
    "Q3: [question text]\n",
    "\"\"\")\n",
    "        self.qa_chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def parse_qa_pairs(self, qa_output):\n",
    "        qa_pairs = []\n",
    "        lines = qa_output.strip().split('\\n')\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            q_match = re.match(r'Q(\\d+):\\s*(.+)', lines[i])\n",
    "            if q_match and i + 1 < len(lines):\n",
    "                question = q_match.group(2).strip()\n",
    "                a_match = re.match(f'A{q_match.group(1)}:\\s*(.+)', lines[i + 1])\n",
    "                if a_match:\n",
    "                    answer = a_match.group(1).strip()\n",
    "                    qa_pairs.append({'question': question, 'answer': answer})\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return qa_pairs\n",
    "\n",
    "    def validate_input(self, doc):\n",
    "        \"\"\"Validate that the document is a Document instance with content.\"\"\"\n",
    "        return (isinstance(doc, Document) and \n",
    "                hasattr(doc, 'page_content') and \n",
    "                len(doc.page_content.strip()) > 0)\n",
    "\n",
    "    def run(self, doc: Document):\n",
    "        \"\"\"Generate questions from the given document.\"\"\"\n",
    "        if not self.validate_input(doc):\n",
    "            raise ValueError(\"Input must be a Document with non-empty page_content\")\n",
    "            \n",
    "        try:\n",
    "            qa_output = self.qa_chain.invoke({\"context\": doc.page_content})\n",
    "            parsed_qa = self.parse_qa_pairs(qa_output)\n",
    "            print(qa_output)\n",
    "            print(parsed_qa)\n",
    "\n",
    "            return {\n",
    "                \"pdf_id\": doc.metadata.get(\"pdf_id\"),\n",
    "                \"chunk_id\": doc.metadata.get(\"chunk_id\"),\n",
    "                \"text\": doc.page_content,\n",
    "                \"qa_output\": qa_output,\n",
    "                \"parsed_qa\": parsed_qa\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ QA generation failed for chunk {doc.metadata}: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a15e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskProcessor:\n",
    "    \"\"\"Context class that uses different task strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy=None):  \n",
    "        self._strategy = strategy      \n",
    "    \n",
    "    @property\n",
    "    def strategy(self):\n",
    "        return self._strategy\n",
    "         \n",
    "    @strategy.setter\n",
    "    def strategy(self, strategy):\n",
    "        self._strategy = strategy\n",
    "         \n",
    "    def execute_task(self, *args, **kwargs):\n",
    "        if self._strategy is None:      # ✅ Add this check\n",
    "            raise ValueError(\"No strategy set\")\n",
    "        return self._strategy.run(*args, **kwargs)\n",
    "         \n",
    "    def switch_strategy(self, new_strategy):\n",
    "        self.strategy = new_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Testing cell\n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   6.880526781082153\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5194a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.0015227794647216797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/f9lh6rmd4q1648_pfl1636zw0000gn/T/ipykernel_14412/422372000.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "llm=OLLAMA_LLM('llama3:8b','llm_cache').load_model()\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6187b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n",
      "Time Taken to process:   60.11000680923462\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "basic_fais=FAISSBasic(multilingual_embedder)\n",
    "basic_fais.create_vector_store(chunked_docs)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n",
      "Time Taken to process:   58.29465293884277\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "fais_improved = FAISSImproved()\n",
    "fais_improved.set_embedder_model(multilingual_embedder)\n",
    "fais_improved.create_vector_store(chunked_docs)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3cb1e",
   "metadata": {},
   "source": [
    "#### Strategy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a145a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TaskProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d297857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.0007348060607910156\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "chatting_strategy = ChattingStrategy(llm, basic_fais, multilingual_embedder)\n",
    "summarization_strategy = SummarizationStrategy(llm)\n",
    "question_strategy = QuestionStrategy(llm)\n",
    "processor = TaskProcessor()\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e86984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Parsed response: {'answer': 'The market research report evaluates leading solutions for multilingual document translation, focusing on tools that support PDF, Word, Excel, and scanned images while preserving layout and formatting. The key features assessed include role assignment, approval workflows, versioning, markup versions, auto-changing status after submittals, financial oversight, bids management, document management system (DMS), dashboards, reporting, notifications, and project archiving.', 'reasoning': \"I based my answer on the provided context, specifically the market research report's analysis of document translation tools. The report highlights individual platform strengths and identifies potential gaps and overlaps by developing a feature matrix that quantifies how well each platform supports a core set of functionalities. I also drew from the specific features evaluated in the report, such as markup versions, auto-changing status after submittals, financial oversight, bids management, and document management system (DMS).\", 'sources': [1, 2, 3], 'raw_response': \"RESPONSE:\\nThe market research report evaluates leading solutions for multilingual document translation, focusing on tools that support PDF, Word, Excel, and scanned images while preserving layout and formatting. The key features assessed include role assignment, approval workflows, versioning, markup versions, auto-changing status after submittals, financial oversight, bids management, document management system (DMS), dashboards, reporting, notifications, and project archiving.\\n\\nREASONING:\\nI based my answer on the provided context, specifically the market research report's analysis of document translation tools. The report highlights individual platform strengths and identifies potential gaps and overlaps by developing a feature matrix that quantifies how well each platform supports a core set of functionalities. I also drew from the specific features evaluated in the report, such as markup versions, auto-changing status after submittals, financial oversight, bids management, and document management system (DMS).\\n\\nSOURCES:\\n1, 2, 3\"}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Time Taken to process:   110.23236584663391\n"
     ]
    }
   ],
   "source": [
    "processor.strategy=chatting_strategy \n",
    "processor.execute_task(\"What are the key findings from the market research report?\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91545baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.strategy=summarization_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ffb3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SummarizationStrategy object at 0x37782a820>\n"
     ]
    }
   ],
   "source": [
    "print(processor.strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5874e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Main Topic:** This market research report evaluates leading document translation tools that support PDF, Word, Excel, and scanned images while preserving layout and formatting. The focus is on tools that handle Arabic, French, and English languages, catering to both B2B and B2C markets.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* No single tool fully meets all requirements for accurate, efficient, and cost-effective document translation across Arabic, French, and English.\n",
      "* Most tools lack OCR capabilities, struggle with mixed language content, or have limitations in layout preservation and translation accuracy.\n",
      "* The report highlights the need for enhanced features such as editing capabilities, conversion features, image translation, selective OCR activation, AI-powered features, process modes, and a split-view translation interface.\n",
      "\n",
      "**Details:**\n",
      "\n",
      "The report tested six document translation tools: Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL. The evaluation involved subjecting each tool to various test cases across Arabic, French, and English languages. The findings revealed strengths and weaknesses in each tool's performance.\n",
      "\n",
      "* Doctranslator offers good layout preservation for English-to-Arabic translations but lacks OCR capabilities and struggles with mixed language content.\n",
      "* Doctranslate.io provides OCR functionality but suffers from slow processing times and poor layout preservation.\n",
      "* TranslaDocs does not support OCR or Arabic language, limiting its applicability.\n",
      "* SmallPDF offers limited translation support and no OCR capabilities.\n",
      "* Doclingo supports OCR with good performance for Arabic-to-English scanned documents but struggles with complex image tables and RTL preservation in English-to-Arabic translations.\n",
      "* DeepL excels at English OCR but does not support Arabic OCR.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The report highlights the need for enhanced features to improve the functionality and market competitiveness of document translation tools. The recommended features aim to address gaps identified in the tested tools, including editing capabilities, conversion features, image translation, selective OCR activation, AI-powered features, process modes, and a split-view translation interface. By implementing these enhancements, document translation tools can better serve B2B and B2C markets, providing users with more accurate, efficient, and cost-effective translation solutions.\n",
      "Document[0] processing time: 42.94 seconds\n",
      "**Main Topic:** Comparative Analysis of Construction-Focused Project Management Platforms\n",
      "\n",
      "The document provides a comprehensive comparison of five project management platforms specifically designed for the construction industry: Monday.com, Wrike, PMWeb, Aconex, and Procore. The analysis focuses on their features, strengths, and weaknesses in managing complex engineering or construction projects.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Monday.com and Wrike**: While initially designed as general-purpose project management tools, they offer intuitive interfaces, flexible dashboards, and excellent task tracking capabilities but fall short on advanced construction-specific workflows such as submittal tracking, formal transmittals, or OCR-based document processing.\n",
      "2. **PMWeb, Aconex, and Procore**: These platforms are built with the complexities of construction in mind. PMWeb offers impressive breadth with strong cost control, scheduling, and portfolio-wide visibility, though it may come with a steeper learning curve. Aconex excels in structured communication, transmittals, and formal document control, making it a go-to choice for large-scale infrastructure projects. Procore stands out for its rich construction-focused toolkit, including automated submittals, AI-powered drawing management, and integrated bid handling.\n",
      "3. **Feature Matrix**: A feature matrix has been developed to quantify how well each platform supports a core set of functionalities, from role assignment and approval workflows to bid management, markup tools, notifications, and financial oversight.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. Assigning roles\n",
      "2. Document approval workflow\n",
      "3. Versioning\n",
      "4. Markup versions\n",
      "5. Auto-changing status after submittals\n",
      "6. Financial oversight\n",
      "7. Bids management\n",
      "8. Document management system (DMS)\n",
      "9. Dashboards\n",
      "10. Reporting\n",
      "11. Notifications\n",
      "12. Project archiving\n",
      "\n",
      "**Conclusion:** The comparative analysis reveals a diverse landscape of project management platforms, each with its own strengths, trade-offs, and degrees of specialization for the construction industry.\n",
      "Document[1] processing time: 53.96 seconds\n",
      "Total processing time: 96.89937090873718 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    \n",
    "    processor.execute_task(document)\n",
    "    \n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total processing time:\", end_time - start_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a0f6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.QuestionStrategy object at 0x37782a130>\n"
     ]
    }
   ],
   "source": [
    "processor.strategy=question_strategy\n",
    "print(processor.strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8def0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 6 questions based on the provided text:\n",
      "\n",
      "Q1: What are the key features evaluated in this market research report for document translation tools?\n",
      "Q2: Which tool is known for its good layout preservation for English to Arabic translations and effective handling of text directionality?\n",
      "Q3: What are the limitations of Doctranslate.io, according to the report's findings?\n",
      "Q4: Which tool supports OCR with good performance for Arabic to English scanned documents but struggles with complex image tables?\n",
      "Q5: What is the recommended feature \"Split PDF\" intended to achieve in document translation tools?\n",
      "Q6: How does DeepL's pricing model differ from that of Doctranslate.io and Doclingo?\n",
      "[]\n",
      "Document[0] processing time: 22.27 seconds\n",
      "Here are 6 questions based on the text:\n",
      "\n",
      "Q1: What is the primary difference between Monday.com and Wrike compared to PMWeb, Aconex, and Procore in terms of project management features?\n",
      "Q2: Which platform offers AI-powered drawing splitting and indexing capabilities?\n",
      "Q3: How does Procore's Submittal Builder tool differ from other platforms' submittal tracking features?\n",
      "Q4: What are the key limitations of solar-focused platforms like Ra Power Management, SenseHawk, and Payaca compared to general-purpose project management tools?\n",
      "Q5: Which platform has a steeper learning curve due to its extensive feature set and customization options?\n",
      "Q6: How do Monday.com and Wrike compare in terms of their ability to support advanced construction-specific workflows such as submittal tracking and formal transmittals?\n",
      "[]\n",
      "Document[1] processing time: 30.00 seconds\n",
      "Time Taken to process:   52.277095079422\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    processor.execute_task(document)\n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
