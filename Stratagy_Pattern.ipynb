{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='./embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_index(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_index(self, file_path):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ce8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSBasic(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            query_embedding = self.embedder_model.batch_embed([query])\n",
    "            if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                query_embedding = query_embedding[0]\n",
    "            elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        results = self._search_chunks(query_embedding, top_k)\n",
    "        \n",
    "        return [\n",
    "            Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "            for res in results\n",
    "        ]\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed1060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the FAISS vectorstore\\nfaiss_store = FAISS(embedder_model=your_embedding_model)\\n\\n# Create vectorstore from Document objects (new method)\\nfaiss_store.create_vectorstore(document_list, normalize_embeddings=True)\\n\\n# Or use the original method\\nfaiss_store.create_vector_store(document_list, your_embedding_model)\\n\\n# Search for relevant documents\\nresults = faiss_store.get_relevant_documents(\"your query\", top_k=5)\\n\\n# Save and load\\nfaiss_store.save_index(\"my_index\")\\nfaiss_store.load_index(\"my_index\", your_embedding_model)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FAISSImproved(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "        # New attributes for enhanced functionality\n",
    "        self.docstore = None\n",
    "        self.index_to_docstore_id = None\n",
    "        self.documents = None  # Store original Document objects\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def create_vectorstore(self, docs, normalize_embeddings=True):\n",
    "        \"\"\"\n",
    "        Create a FAISS vector store from a list of Document objects.\n",
    "        Each document should have metadata like pdf_id, chunk_id, etc.\n",
    "        \n",
    "        Args:\n",
    "            docs: List of Document objects\n",
    "            normalize_embeddings: Whether to normalize embeddings for cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            self: Returns the FAISS instance for method chaining\n",
    "        \"\"\"\n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required. Set it during initialization or call set_embedder_model()\")\n",
    "        \n",
    "        # Extract texts from Document objects\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        # Initialize FAISS Index\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity if requested\n",
    "        if normalize_embeddings:\n",
    "            faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store original Document objects and create mappings\n",
    "        self.documents = docs\n",
    "        self.docstore = {str(i): doc for i, doc in enumerate(docs)}\n",
    "        self.index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "        \n",
    "        # Also maintain backward compatibility with chunks_dict\n",
    "        self.chunks_dict = {i: doc.page_content for i, doc in enumerate(docs)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created vectorstore with {self.total_vectors} documents of dim {self.dimension}\")\n",
    "        print(f\"[FAISS] Normalization: {'enabled' if normalize_embeddings else 'disabled'}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() or create_vectorstore() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            # Use embed_query if available, otherwise fall back to batch_embed\n",
    "            if hasattr(self.embedder_model, 'embed_query'):\n",
    "                query_embedding = self.embedder_model.embed_query(query)\n",
    "            else:\n",
    "                query_embedding = self.embedder_model.batch_embed([query])\n",
    "                if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                    query_embedding = query_embedding[0]\n",
    "                elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                    query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        if self.docstore is not None:\n",
    "            # Use enhanced docstore-based retrieval\n",
    "            results = self._search_with_docstore(query_embedding, top_k)\n",
    "        else:\n",
    "            # Fall back to original chunk-based retrieval\n",
    "            results = self._search_chunks(query_embedding, top_k)\n",
    "            # Convert to Document objects for consistency\n",
    "            results = [\n",
    "                Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "                for res in results\n",
    "            ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _search_with_docstore(self, query_embedding, top_k=5):\n",
    "        \"\"\"Enhanced search function using docstore - returns Document objects\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results using docstore\n",
    "        documents = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx in self.index_to_docstore_id:\n",
    "                docstore_id = self.index_to_docstore_id[faiss_idx]\n",
    "                if docstore_id in self.docstore:\n",
    "                    doc = self.docstore[docstore_id]\n",
    "                    similarity = float(distances[0][i])\n",
    "                    \n",
    "                    # Create a copy of the document with updated metadata\n",
    "                    enhanced_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                    enhanced_metadata[\"similarity\"] = similarity\n",
    "                    enhanced_metadata[\"retrieval_index\"] = faiss_idx\n",
    "                    \n",
    "                    enhanced_doc = Document(\n",
    "                        page_content=doc.page_content,\n",
    "                        metadata=enhanced_metadata\n",
    "                    )\n",
    "                    documents.append(enhanced_doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata (enhanced to include new attributes)\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type,\n",
    "            'docstore': self.docstore,\n",
    "            'index_to_docstore_id': self.index_to_docstore_id,\n",
    "            'documents': self.documents\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Load enhanced attributes if they exist (backward compatibility)\n",
    "        self.docstore = metadata.get('docstore', None)\n",
    "        self.index_to_docstore_id = metadata.get('index_to_docstore_id', None)\n",
    "        self.documents = metadata.get('documents', None)\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        if self.docstore is not None:\n",
    "            print(f\"[FAISS] Enhanced docstore mode enabled\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None,\n",
    "            'has_docstore': self.docstore is not None,\n",
    "            'has_documents': self.documents is not None\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize the FAISS vectorstore\n",
    "faiss_store = FAISS(embedder_model=your_embedding_model)\n",
    "\n",
    "# Create vectorstore from Document objects (new method)\n",
    "faiss_store.create_vectorstore(document_list, normalize_embeddings=True)\n",
    "\n",
    "# Or use the original method\n",
    "faiss_store.create_vector_store(document_list, your_embedding_model)\n",
    "\n",
    "# Search for relevant documents\n",
    "results = faiss_store.get_relevant_documents(\"your query\", top_k=5)\n",
    "\n",
    "# Save and load\n",
    "faiss_store.save_index(\"my_index\")\n",
    "faiss_store.load_index(\"my_index\", your_embedding_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f5d36",
   "metadata": {},
   "source": [
    "### LLM Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        self.model_name = model_name\n",
    "        self.cache_folder = cache_folder\n",
    "        self.device = ('cpu'\n",
    "            # 'cuda' if torch.cuda.is_available()\n",
    "            # else 'mps' if torch.backends.mps.is_available()\n",
    "            # else 'cpu'\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OLLAMA_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n",
    "        return model\n",
    "\n",
    "\n",
    "class Hugging_Face_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"  \n",
    "        )\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab7c4d",
   "metadata": {},
   "source": [
    "## Strategy Pattern Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71cb45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStrategy(ABC):\n",
    "    \"\"\"Abstract base class defining the strategy interface.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self, *args, **kwargs):\n",
    "        \"\"\"Execute the strategy. Must be implemented by concrete strategies.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5638e6",
   "metadata": {},
   "source": [
    "#### Chatting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "469a4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChattingStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self._build_chain()\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Source {i} | PDF {doc.metadata.get('pdf_id', '?')}]: {doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, 1)\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Please provide a comprehensive answer based on the context above. You MUST follow this exact format:\n",
    "\n",
    "            RESPONSE:\n",
    "            [Your main answer here]\n",
    "\n",
    "            REASONING:\n",
    "            [Explain your reasoning and how you used the context]\n",
    "\n",
    "            SOURCES:\n",
    "            [List the source numbers you referenced, for example: 1, 3, 5]\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        def retrieve_context(inputs):\n",
    "            docs = self.vector_store.get_relevant_documents(inputs[\"question\"], top_k=self.top_k)\n",
    "            return self.format_docs(docs)\n",
    "\n",
    "        self.chain = ({\n",
    "                \"context\": RunnableLambda(retrieve_context), \n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def parse_structured_response(self, response_text):\n",
    "        cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)\n",
    "        cleaned_response = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_response.strip())\n",
    "\n",
    "        sections = {'response': '', 'reasoning': '', 'sources': ''}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        lines = cleaned_response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith('RESPONSE:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'response'\n",
    "                current_content = [line[9:].strip()]\n",
    "            elif line.upper().startswith('REASONING:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'reasoning'\n",
    "                current_content = [line[10:].strip()]\n",
    "            elif line.upper().startswith('SOURCES:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'sources'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif current_section and line:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "\n",
    "        source_ids = [int(x) for x in re.findall(r'\\d+', sections['sources'])] if sections['sources'] else []\n",
    "\n",
    "        return {\n",
    "            'answer': sections['response'],\n",
    "            'reasoning': sections['reasoning'],\n",
    "            'sources': source_ids,\n",
    "            'raw_response': cleaned_response\n",
    "        }\n",
    "\n",
    "    def validate_input(self, question):\n",
    "        \"\"\"Validate that the question is a non-empty string.\"\"\"\n",
    "        return isinstance(question, str) and len(question.strip()) > 0\n",
    "\n",
    "    def run(self, question):\n",
    "        \"\"\"Main method to run the chain and parse result.\"\"\"\n",
    "        if not self.validate_input(question):\n",
    "            raise ValueError(\"Question must be a non-empty string\")\n",
    "        \n",
    "        response = self.chain.invoke({\"question\": question})\n",
    "\n",
    "        parsed = self.parse_structured_response(response)\n",
    "        print(f\"Parsed response: {parsed}\")  \n",
    "\n",
    "    \n",
    "        source_docs = self.vector_store.get_relevant_documents(question, top_k=self.top_k)\n",
    "        parsed['source_documents'] = source_docs\n",
    "        parsed['source_texts'] = [doc.page_content for doc in source_docs]\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71af65b",
   "metadata": {},
   "source": [
    "#### Summerization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769e1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationStrategy(TaskStrategy):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Summarize the following document using this format:\n",
    "\n",
    "            **Main Topic:** [One sentence describing what this document is about]\n",
    "\n",
    "            **Key Points:**\n",
    "            - [Most important point]\n",
    "            - [Second most important point]  \n",
    "            - [Third most important point]\n",
    "\n",
    "            **Details:** [Supporting information, numbers, examples]\n",
    "\n",
    "            **Conclusion:** [Main takeaway or implication]\n",
    "\n",
    "            Document: {context}\"\"\")\n",
    "                    ])\n",
    "\n",
    "    def validate_input(self, document):\n",
    "        \"\"\"Validate that the document is a non-empty string.\"\"\"\n",
    "        return isinstance(document, str) and len(document.strip()) > 0\n",
    "\n",
    "    def run(self, document):\n",
    "        \"\"\"Summarize the given document.\"\"\"            \n",
    "        # Format prompt manually\n",
    "        formatted_prompt = self.prompt.format(context=document)\n",
    "        \n",
    "        # Directly invoke the LLM\n",
    "        result = self.llm.invoke(formatted_prompt)\n",
    "        \n",
    "        print(result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deff017",
   "metadata": {},
   "source": [
    "#### Question Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0bade4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, complexity=\"medium\"):\n",
    "        self.llm = llm\n",
    "        self.complexity = complexity\n",
    "        self._set_prompt()\n",
    "    \n",
    "    def _set_prompt(self):\n",
    "        complexity_instructions = {\n",
    "            \"easy\": \"Generate simple, basic questions that test understanding of key facts and definitions.\",\n",
    "            \"medium\": \"Generate moderately challenging questions that require analysis and understanding of concepts.\",\n",
    "            \"hard\": \"Generate complex questions that require critical thinking, analysis, and synthesis of information.\"\n",
    "        }\n",
    "        \n",
    "        instruction = complexity_instructions.get(self.complexity, complexity_instructions[\"medium\"])\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(f\"\"\"\n",
    "        You are a helpful assistant tasked with generating question-answer pairs for study purposes.\n",
    "\n",
    "        Text:\n",
    "        {{context}}\n",
    "\n",
    "        {instruction}\n",
    "        Generate {{Questions}} meaningful questions based only on the above text. \n",
    "\n",
    "        IMPORTANT: Format your output exactly as shown below with no additional text, explanations, or formatting:\n",
    "\n",
    "        Q1: [question text]\n",
    "        Q2: [question text]\n",
    "        Q3: [question text]\n",
    "        \"\"\")\n",
    "        self.qa_chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "\n",
    "    def set_complexity(self, complexity):\n",
    "        \"\"\"Change complexity level with synonym mapping and fuzzy matching.\"\"\"\n",
    "        import difflib\n",
    "        \n",
    "        complexity = complexity.lower().strip()\n",
    "        \n",
    "        # Handle synonyms first\n",
    "        synonyms = {\n",
    "            \"challenging\": \"hard\", \"difficult\": \"hard\", \"tough\": \"hard\",\n",
    "            \"simple\": \"easy\", \"basic\": \"easy\", \"beginner\": \"easy\", \n",
    "            \"moderate\": \"medium\", \"average\": \"medium\", \"normal\": \"medium\"\n",
    "        }\n",
    "        \n",
    "        if complexity in synonyms:\n",
    "            self.complexity = synonyms[complexity]\n",
    "            self._set_prompt()\n",
    "            return\n",
    "        \n",
    "        # Check exact match\n",
    "        valid_options = [\"easy\", \"medium\", \"hard\"]\n",
    "        if complexity in valid_options:\n",
    "            self.complexity = complexity\n",
    "            self._set_prompt()\n",
    "            return\n",
    "        \n",
    "        # Fuzzy matching against synonyms first\n",
    "        all_options = list(synonyms.keys()) + valid_options\n",
    "        matches = difflib.get_close_matches(complexity, all_options, n=1, cutoff=0.6)\n",
    "        \n",
    "        if matches:\n",
    "            best_match = matches[0]\n",
    "            similarity = difflib.SequenceMatcher(None, complexity, best_match).ratio()\n",
    "            print(f\"'{complexity}' matched to '{best_match}' ({similarity:.0%} confidence)\")\n",
    "            \n",
    "            # Map to final complexity\n",
    "            final_complexity = synonyms.get(best_match, best_match)\n",
    "            self.complexity = final_complexity\n",
    "            self._set_prompt()\n",
    "        else:\n",
    "            raise ValueError(\"Please use: 'easy', 'medium', 'hard', or synonyms like 'challenging', 'simple'\")\n",
    "\n",
    "\n",
    "\n",
    "    def parse_qa_pairs(self, qa_output):\n",
    "        qa_pairs = []\n",
    "        lines = qa_output.strip().split('\\n')\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            q_match = re.match(r'Q(\\d+):\\s*(.+)', lines[i])\n",
    "            if q_match and i + 1 < len(lines):\n",
    "                question = q_match.group(2).strip()\n",
    "                a_match = re.match(f'A{q_match.group(1)}:\\s*(.+)', lines[i + 1])\n",
    "                if a_match:\n",
    "                    answer = a_match.group(1).strip()\n",
    "                    qa_pairs.append({'question': question, 'answer': answer})\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return qa_pairs\n",
    "\n",
    "    def validate_input(self, doc):\n",
    "        \"\"\"Validate that the document is a Document instance with content.\"\"\"\n",
    "        return (isinstance(doc, Document) and \n",
    "                hasattr(doc, 'page_content') and \n",
    "                len(doc.page_content.strip()) > 0)\n",
    "    \n",
    "    def run(self, doc, questions, complexity='simple'):\n",
    "        \"\"\"Generate questions from the given document.\"\"\"\n",
    "        if not self.validate_input(doc):\n",
    "            raise ValueError(\"Input must be a Document with non-empty page_content\")\n",
    "        \n",
    "        # Update complexity if provided\n",
    "        if complexity is not None:\n",
    "            self.set_complexity(complexity)\n",
    "            \n",
    "        try:\n",
    "            qa_output = self.qa_chain.invoke({\"context\": doc.page_content,\"Questions\":questions})\n",
    "            parsed_qa = self.parse_qa_pairs(qa_output)\n",
    "            print(qa_output)\n",
    "            print(parsed_qa)\n",
    "\n",
    "            return {\n",
    "                \"pdf_id\": doc.metadata.get(\"pdf_id\"),\n",
    "                \"chunk_id\": doc.metadata.get(\"chunk_id\"),\n",
    "                \"text\": doc.page_content,\n",
    "                \"qa_output\": qa_output,\n",
    "                \"parsed_qa\": parsed_qa\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ QA generation failed for Document {doc.metadata}: {e}\")\n",
    "            return None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a15e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskProcessor:\n",
    "    \"\"\"Context class that uses different task strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy=None):  \n",
    "        self._strategy = strategy      \n",
    "    \n",
    "    @property\n",
    "    def strategy(self):\n",
    "        return self._strategy\n",
    "         \n",
    "    @strategy.setter\n",
    "    def strategy(self, strategy):\n",
    "        self._strategy = strategy\n",
    "         \n",
    "    def execute_task(self, *args, **kwargs):\n",
    "        if self._strategy is None:      # ✅ Add this check\n",
    "            raise ValueError(\"No strategy set\")\n",
    "        return self._strategy.run(*args, **kwargs)\n",
    "         \n",
    "    def switch_strategy(self, new_strategy):\n",
    "        self.strategy = new_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Testing cell\n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   7.962954998016357\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5194a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.004022121429443359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/f9lh6rmd4q1648_pfl1636zw0000gn/T/ipykernel_33159/422372000.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "llm=OLLAMA_LLM('llama3:8b','llm_cache').load_model()\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6187b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# basic_fais=FAISSBasic(multilingual_embedder)\n",
    "# basic_fais.create_vector_store(chunked_docs)\n",
    "# end=time.time()\n",
    "# print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n",
      "Time Taken to process:   58.90743017196655\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "fais_improved = FAISSImproved()\n",
    "fais_improved.set_embedder_model(multilingual_embedder)\n",
    "fais_improved.create_vector_store(chunked_docs)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3cb1e",
   "metadata": {},
   "source": [
    "#### Strategy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a145a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TaskProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d297857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.000865936279296875\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "chatting_strategy = ChattingStrategy(llm, fais_improved, multilingual_embedder)\n",
    "summarization_strategy = SummarizationStrategy(llm)\n",
    "question_strategy = QuestionStrategy(llm)\n",
    "processor = TaskProcessor()\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e86984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Parsed response: {'answer': 'Doctranslate.io charges users with credits.', 'reasoning': 'I based my answer on the pricing plans provided in Source 1. According to the table, Doctranslate.io offers different subscription plans that charge users with translation credits. The plans include Topup -50, Topup -120, Topup -260, and Topup -750, which provide varying amounts of translation credits. This indicates that Doctranslate.io uses a credit-based system for charging its users.', 'sources': [1], 'raw_response': 'RESPONSE:\\nDoctranslate.io charges users with credits.\\n\\nREASONING:\\nI based my answer on the pricing plans provided in Source 1. According to the table, Doctranslate.io offers different subscription plans that charge users with translation credits. The plans include Topup -50, Topup -120, Topup -260, and Topup -750, which provide varying amounts of translation credits. This indicates that Doctranslate.io uses a credit-based system for charging its users.\\n\\nSOURCES:\\n1'}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Time Taken to process:   49.70245695114136\n"
     ]
    }
   ],
   "source": [
    "processor.strategy=chatting_strategy \n",
    "processor.execute_task(\"Which translator charges users with credits?\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91545baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.strategy=summarization_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ffb3c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SummarizationStrategy object at 0x33d513880>\n"
     ]
    }
   ],
   "source": [
    "print(processor.strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5874e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Main Topic:** Market Research Report: Analysis of Document Translation Tools for Multilingual Document Translation\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* The report evaluates leading document translation tools that support PDF, Word, Excel, and scanned images while preserving layout and formatting.\n",
      "* The focus is on tools that handle Arabic, French, and English languages, catering to both B2B and B2C markets.\n",
      "* The key features evaluated include layout preservation, Arabic support, OCR support, pricing model, and translation accuracy.\n",
      "\n",
      "**Details:**\n",
      "\n",
      "The report tested six document translation tools: Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL. Each tool was subjected to a series of test cases, including text-based documents, scanned documents, tables (as text), tables (as images), and scanned documents with stamps or signatures.\n",
      "\n",
      "The report highlights the strengths and weaknesses of each tool, including:\n",
      "\n",
      "* Doctranslator: Offers free services with good layout preservation for English to Arabic translations, but lacks OCR capabilities and struggles with mixed language content.\n",
      "* Doctranslate.io: Provides OCR functionality but suffers from slow processing times and poor layout preservation. It also fails to translate Arabic numerals and mismanages mixed text directions.\n",
      "* TranslaDocs: Does not support OCR or Arabic language, limiting its applicability.\n",
      "* SmallPDF: Offers limited translation support and no OCR capabilities.\n",
      "* Doclingo: Supports OCR with good performance for Arabic to English scanned documents but struggles with complex image tables and RTL preservation in English to Arabic translations.\n",
      "* DeepL: Excellent for English OCR but does not support Arabic, making it unsuitable for this use case.\n",
      "\n",
      "**Conclusion:** The report reveals that no single tool fully meets all requirements for accurate, efficient, and cost-effective document translation across Arabic, French, and English. Based on the evaluation, several opportunities exist to enhance document translation tools to better serve B2B and B2C markets. The recommended features aim to address gaps identified in the tested tools and improve functionality, user experience, and translation quality.\n",
      "Document[0] processing time: 22.09 seconds\n",
      "**Main Topic:** Comparative Analysis of Construction-Focused Project Management Platforms\n",
      "\n",
      "The document provides a comprehensive comparison of five construction-focused project management platforms: Monday.com, Wrike, PMWeb, Aconex, and Procore. The analysis highlights the strengths and weaknesses of each platform in terms of their features, functionalities, and suitability for managing complex engineering or construction projects.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Monday.com and Wrike**: While these general-purpose project management tools offer intuitive interfaces and excellent task tracking capabilities, they fall short on advanced construction-specific workflows such as submittal tracking, formal transmittals, or OCR-based document processing.\n",
      "2. **PMWeb, Aconex, and Procore**: These platforms are built with the complexities of construction in mind and excel in specific areas:\n",
      "\t* PMWeb offers impressive breadth with strong cost control, scheduling, and portfolio-wide visibility, though it may come with a steeper learning curve.\n",
      "\t* Aconex excels in structured communication, transmittals, and formal document control, making it a go-to choice for large-scale infrastructure projects.\n",
      "\t* Procore stands out for its rich construction-focused toolkit, including automated submittals, AI-powered drawing management, and integrated bid handling.\n",
      "3. **Feature Matrix**: A feature matrix has been developed to quantify how well each platform supports a core set of functionalities, including role assignment, approval workflows, bid management, markup tools, notifications, and financial oversight.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. **Choose the right platform**: Select a platform that aligns with your specific project management needs and goals.\n",
      "2. **Consider the trade-offs**: Weigh the benefits and drawbacks of each platform's features and functionalities to ensure you're getting the best fit for your organization.\n",
      "3. **Evaluate the learning curve**: Consider the time and effort required to learn and implement each platform, especially if you have a large team or complex projects.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The comparative analysis highlights the diversity of project management platforms available for the construction industry. By understanding the strengths and weaknesses of each platform, organizations can make informed decisions about which tool best suits their needs and goals.\n",
      "Document[1] processing time: 28.31 seconds\n",
      "Total processing time: 50.4038028717041 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    \n",
    "    processor.execute_task(document)\n",
    "    \n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total processing time:\", end_time - start_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a0f6ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.QuestionStrategy object at 0x33d513790>\n"
     ]
    }
   ],
   "source": [
    "processor.strategy=question_strategy\n",
    "print(processor.strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8def0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 20 simple and basic questions that test understanding of key facts and definitions based on the provided text:\n",
      "\n",
      "Q1: What is the main topic of this market research report?\n",
      "Q2: Which languages are supported by the document translation tools evaluated in this report?\n",
      "Q3: What are the key features evaluated for each tool in this report?\n",
      "Q4: What is the focus of this report, and what markets does it cater to?\n",
      "Q5: Which tool lacks OCR capabilities and struggles with mixed language content?\n",
      "Q6: What is Doctranslator's strength regarding layout preservation?\n",
      "Q7: Does Doctranslate.io support Arabic language?\n",
      "Q8: What is TranslaDocs' limitation in terms of language support?\n",
      "Q9: Is SmallPDF a free service?\n",
      "Q10: Which tool excels in OCR performance for English but not for Arabic?\n",
      "Q11: What is the recommended feature to enable users to add text, shapes, images, and freehand annotations to PDFs?\n",
      "Q12: What is the purpose of the \"Split PDF\" feature?\n",
      "Q13: Can users upload standalone images for translation using Doclingo?\n",
      "Q14: Does Doctranslate.io offer an option to enable or disable OCR within uploaded documents?\n",
      "Q15: What is the main difference between the \"Professional Translation\" and \"Paraphrase\" modes in this report?\n",
      "Q16: Which tool provides a split-screen interface displaying the original document on the left and the translated document on the right?\n",
      "Q17: What is the cost of Doctranslate.io's Topup-50 plan?\n",
      "Q18: How many credits does the Personal Subscription plan offer per year?\n",
      "Q19: What is the maximum file size allowed for uploads using Doclingo Premium?\n",
      "Q20: Which tool offers a 1M chars. monthly limit and a Trans. Engine of ChatGPT/Gemini/Deepseek/Claude?\n",
      "[]\n",
      "Document[1] processing time: 21.04 seconds\n",
      "Here are the 20 questions to test understanding of key facts and definitions:\n",
      "\n",
      "Q1: What is Monday.com?\n",
      "Q2: Which platform has a steep learning curve?\n",
      "Q3: What is PMWeb's strength in terms of cost control, scheduling, and portfolio-wide visibility?\n",
      "Q4: Who is Aconex designed for?\n",
      "Q5: What is Procore's unique feature in terms of drawing management?\n",
      "Q6: Which platform offers AI-powered drawing splitting and indexing?\n",
      "Q7: What is SenseHawk used for?\n",
      "Q8: Which platform has a built-in markup tool?\n",
      "Q9: What is Payaca used for?\n",
      "Q10: Who are the platforms designed for, specifically?\n",
      "Q11: What is Ra Power Management (RaPM) used for?\n",
      "Q12: Which platform has automated submittals?\n",
      "Q13: What is Monday.com's strength in terms of task tracking capabilities?\n",
      "Q14: Which platform offers formal transmittals and OCR-based document processing?\n",
      "Q15: Who are the platforms designed to manage, specifically?\n",
      "Q16: What is Aconex's strength in terms of structured communication?\n",
      "Q17: Which platform has a centralized dashboard for project progress tracking?\n",
      "Q18: What is Procore's unique feature in terms of bid handling?\n",
      "Q19: Which platform offers role-based task assignment and approval workflows?\n",
      "Q20: What is the main difference between Monday.com and Wrike?\n",
      "[]\n",
      "Document[2] processing time: 23.02 seconds\n",
      "Time Taken to process:   44.056331157684326\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    processor.execute_task(document,20)\n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index+1}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f63bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'challenshing' matched to 'challenging' (87% confidence)\n",
      "Here are the 20 complex questions based on the provided text:\n",
      "\n",
      "Q1: What are the key features evaluated in the market research report to assess document translation tools?\n",
      "\n",
      "Q2: Which tool lacks OCR capabilities and struggles with mixed language content?\n",
      "\n",
      "Q3: How does Doctranslator handle Arabic numerals during translation?\n",
      "\n",
      "Q4: What is the primary limitation of TranslaDocs, according to the report?\n",
      "\n",
      "Q5: Can you describe the performance of Doclingo in translating complex image tables?\n",
      "\n",
      "Q6: Why does DeepL not support Arabic OCR, despite its excellent English OCR capabilities?\n",
      "\n",
      "Q7: How do the tools differ in their pricing models and what are the implications for users?\n",
      "\n",
      "Q8: What is the significance of preserving layout and formatting during document translation?\n",
      "\n",
      "Q9: Can you explain how Doctranslate.io handles mixed RTL/LTR directions during translation?\n",
      "\n",
      "Q10: What are the recommended features to enhance document translation tools, according to the report?\n",
      "\n",
      "Q11: How do the tools compare in terms of their ability to translate Arabic numerals correctly?\n",
      "\n",
      "Q12: What is the purpose of the \"Split PDF\" feature and how does it improve document management efficiency?\n",
      "\n",
      "Q13: Can you describe the benefits of implementing AI-powered features in document translation tools?\n",
      "\n",
      "Q14: How do the tools differ in their support for domain-specific translations, if at all?\n",
      "\n",
      "Q15: What are the implications of Doctranslator's poor handling of mixed Arabic/English lines during translation?\n",
      "\n",
      "Q16: Why is it essential to evaluate the performance of OCR capabilities in document translation tools?\n",
      "\n",
      "Q17: Can you explain how the \"Chat with PDF\" feature can improve user experience and translation quality?\n",
      "\n",
      "Q18: What are the limitations of SmallPDF's limited translation support, according to the report?\n",
      "\n",
      "Q19: How do the tools compare in terms of their ability to translate complex image tables correctly?\n",
      "\n",
      "Q20: What are the potential benefits of implementing AI-powered summarization features in document translation tools?\n",
      "[]\n",
      "Document[1] processing time: 21.57 seconds\n",
      "'challenshing' matched to 'challenging' (87% confidence)\n",
      "Here are the 20 complex questions based on the provided text:\n",
      "\n",
      "Q1: How do Monday.com and Wrike's general-purpose project management features compare to PMWeb, Aconex, and Procore's construction-specific tools?\n",
      "\n",
      "Q2: What are the key differences between PMWeb's cost control and scheduling capabilities compared to Aconex's structured communication and formal document control?\n",
      "\n",
      "Q3: Can you identify any gaps or overlaps in the feature sets of Monday.com, Wrike, PMWeb, Aconex, and Procore for managing complex engineering or construction projects?\n",
      "\n",
      "Q4: How do the markup tools in Procore compare to those in Aconex and PMWeb for tracking changes in drawings and documents?\n",
      "\n",
      "Q5: What are the implications of using a platform like Monday.com or Wrike for managing construction projects compared to specialized platforms like PMWeb, Aconex, and Procore?\n",
      "\n",
      "Q6: Can you explain how the auto-changing status feature in Procore works and how it differs from similar features in other platforms?\n",
      "\n",
      "Q7: How do the financial oversight capabilities of PMWeb compare to those of Aconex and Procore for managing project budgets and costs?\n",
      "\n",
      "Q8: What are the benefits and drawbacks of using a platform like Monday.com or Wrike for managing construction projects compared to specialized platforms like PMWeb, Aconex, and Procore?\n",
      "\n",
      "Q9: Can you identify any potential limitations or challenges in using a platform like Procore for managing complex engineering or construction projects?\n",
      "\n",
      "Q10: How do the reporting features in Procore compare to those in Monday.com, Wrike, PMWeb, and Aconex for tracking project progress and performance?\n",
      "\n",
      "Q11: What are the implications of using a platform like PMWeb, Aconex, or Procore for managing construction projects compared to general-purpose project management platforms like Monday.com and Wrike?\n",
      "\n",
      "Q12: Can you explain how the document approval workflow feature in Procore works and how it differs from similar features in other platforms?\n",
      "\n",
      "Q13: How do the versioning capabilities of Procore compare to those of Aconex, PMWeb, and Monday.com for managing changes in documents and drawings?\n",
      "\n",
      "Q14: What are the benefits and drawbacks of using a platform like Monday.com or Wrike for managing construction projects compared to specialized platforms like PMWeb, Aconex, and Procore?\n",
      "\n",
      "Q15: Can you identify any potential limitations or challenges in using a platform like Aconex for managing complex engineering or construction projects?\n",
      "\n",
      "Q16: How do the notification features in Procore compare to those in Monday.com, Wrike, PMWeb, and Aconex for keeping stakeholders informed about project progress and changes?\n",
      "\n",
      "Q17: What are the implications of using a platform like PMWeb, Aconex, or Procore for managing construction projects compared to general-purpose project management platforms like Monday.com and Wrike?\n",
      "\n",
      "Q18: Can you explain how the project archiving feature in Procore works and how it differs from similar features in other platforms?\n",
      "\n",
      "Q19: How do the dashboards in Procore compare to those in Monday.com, Wrike, PMWeb, and Aconex for tracking project progress and performance?\n",
      "\n",
      "Q20: What are the benefits and drawbacks of using a platform like Monday.com or Wrike for managing construction projects compared to specialized platforms like PMWeb, Aconex, and Procore?\n",
      "[]\n",
      "Document[2] processing time: 37.57 seconds\n",
      "Time Taken to process:   59.13273596763611\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    processor.execute_task(document,20,'challenshing')\n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index+1}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea967662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 20 simple and basic questions that test understanding of key facts and definitions based on the provided text:\n",
      "\n",
      "Q1: What is the main focus of this market research report?\n",
      "Q2: Which languages are supported by the document translation tools evaluated in this report?\n",
      "Q3: What are the key features evaluated for each tool in this report?\n",
      "Q4: What is the purpose of OCR (Optical Character Recognition) support in document translation tools?\n",
      "Q5: Which tool lacks OCR capabilities and struggles with mixed language content?\n",
      "Q6: What is the main limitation of Doctranslator, according to this report?\n",
      "Q7: How does Doctranslate.io handle Arabic numerals?\n",
      "Q8: What is the primary advantage of Doclingo's OCR support?\n",
      "Q9: Which tool does not support Arabic language and has no OCR capabilities?\n",
      "Q10: What is the main limitation of DeepL's OCR support, according to this report?\n",
      "Q11: How many tools were tested in this market research report?\n",
      "Q12: What are the five recommended features for enhancing document translation tools, as suggested by this report?\n",
      "Q13: Which tool provides a split-view translation interface?\n",
      "Q14: What is the main advantage of Doctranslator's layout preservation feature?\n",
      "Q15: How does TranslaDocs handle mixed language content?\n",
      "Q16: What is the primary limitation of SmallPDF's translation support?\n",
      "Q17: Which tool offers a free version with limited translation credits?\n",
      "Q18: What is the monthly limit for characters in Doclingo's Premium plan?\n",
      "Q19: How much does DeepL Pro Starter cost per user, when paid annually?\n",
      "Q20: Which tool provides a 1-year credit expiration period?\n",
      "[]\n",
      "Document[1] processing time: 20.56 seconds\n",
      "Here are the 20 questions:\n",
      "\n",
      "Q1: What is Monday.com?\n",
      "Q2: Which platform has AI-powered drawing splitting and indexing?\n",
      "Q3: What is PMWeb's strength in terms of cost control, scheduling, and portfolio-wide visibility?\n",
      "Q4: Which platform excels in structured communication, transmittals, and formal document control?\n",
      "Q5: What is Procore's unique feature for comparing two revisions on-screen?\n",
      "Q6: Which platform has a steep learning curve?\n",
      "Q7: What is Aconex's strength in terms of bid management?\n",
      "Q8: Which platform offers automated submittals?\n",
      "Q9: What is Wrike's strength in terms of task tracking capabilities?\n",
      "Q10: Which platform has OCR-based document processing?\n",
      "Q11: What is Monday.com's strength in terms of dashboards?\n",
      "Q12: Which platform excels in financial oversight?\n",
      "Q13: What is Procore's strength in terms of markup tools?\n",
      "Q14: Which platform has a feature matrix that quantifies its support for core functionalities?\n",
      "Q15: What is Aconex's strength in terms of formal transmittals?\n",
      "Q16: Which platform offers role-based task assignment?\n",
      "Q17: What is PMWeb's strength in terms of workflow approvals?\n",
      "Q18: Which platform excels in project archiving?\n",
      "Q19: What is Wrike's strength in terms of flexible dashboards?\n",
      "Q20: Which platform has a comprehensive feature set for managing complex engineering or construction projects?\n",
      "[]\n",
      "Document[2] processing time: 25.41 seconds\n",
      "Time Taken to process:   45.97092580795288\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for index, document in enumerate(individual_documents):\n",
    "    doc_start_time = time.time()\n",
    "    processor.execute_task(document,20,'simple')\n",
    "    doc_end_time = time.time()\n",
    "    print(f\"Document[{index+1}] processing time: {doc_end_time - doc_start_time:.2f} seconds\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b6641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
