import re
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging
from tqdm import tqdm
from abc import ABC, abstractmethod

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

class BasePreprocessor(ABC):
    def __init__(self):
        logger.info("ğŸ”§ Initializing BasePreprocessor...")
        logger.info("ğŸ“„ Setting up text splitter configuration...")
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=200,
            chunk_overlap=50, 
            length_function=lambda x: len(x.split()),
            separators=["\n\n\n", "\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""],
            keep_separator=False,
            add_start_index=True,
            strip_whitespace=True
        )
        
        logger.info("âœ… Text splitter configured - Chunk size: 200, Overlap: 50")
        logger.info("âœ… BasePreprocessor initialized successfully")

    @abstractmethod
    def load_and_preprocess_data(self, file_path):
        pass

    @abstractmethod
    def process_documents_from_files(self, file_paths):
        pass

    def clean_text(self, text):
        """Clean text by normalizing whitespace and line breaks."""
        logger.debug("ğŸ§¹ Cleaning text...")
        
        try:
            original_length = len(str(text))
            
            # Clean excessive line breaks and whitespace
            cleaned = re.sub(r'\s+', ' ', re.sub(r'\n{3,}', '\n\n', str(text))).strip()
            
            cleaned_length = len(cleaned)
            logger.debug(f"âœ… Text cleaned - Original: {original_length} chars, Cleaned: {cleaned_length} chars")
            
            return cleaned
            
        except Exception as e:
            logger.error(f"âŒ Error cleaning text: {str(e)}")
            raise

    def chunk_documents(self, individual_documents):
        """Split documents into chunks using the configured text splitter."""
        logger.info(f"âœ‚ï¸ Starting document chunking for {len(individual_documents)} documents...")
        
        try:
            chunked_docs = []
            total_chunks = 0
            
            # Process each document with progress tracking
            for doc in tqdm(individual_documents, desc="ğŸ“„ Processing docs", unit="doc"):
                logger.debug(f"ğŸ”„ Chunking document with pdf_id: {doc.metadata.get('pdf_id', 'unknown')}")
                
                # Split document text into chunks
                chunks = self.text_splitter.split_text(doc.page_content)
                logger.debug(f"ğŸ“Š Document split into {len(chunks)} chunks")
                
                # Create Document objects for each chunk with progress
                for i, chunk in enumerate(tqdm(chunks, desc=f"ğŸ”— Creating chunks", unit="chunk", leave=False)):
                    chunked_docs.append(
                        Document(
                            page_content=chunk,
                            metadata={
                                "pdf_id": doc.metadata["pdf_id"],
                                "chunk_id": i
                            }
                        )
                    )
                
                total_chunks += len(chunks)
                logger.debug(f"âœ… Processed document - Created {len(chunks)} chunks (Total so far: {total_chunks})")

            logger.info(f"âœ… Document chunking completed successfully")
            logger.info(f"ğŸ“Š Total Documents: {len(individual_documents)}, Total Chunks: {len(chunked_docs)}")
            print(f"âœ… Total Chunks: {len(chunked_docs)}")

            return chunked_docs
            
        except Exception as e:
            logger.error(f"âŒ Error during document chunking: {str(e)}")
            raise