{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_faiss_index(self, chunks_embed):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def search_faiss(self, query_embedding, top_k=5):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def search_chunks(self, query_embedding, top_k=5):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def save_faiss_index(self, file_index_name):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def load_faiss_index(self, file_index_name):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ce8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_index(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_index(self, file_path):\n",
    "        pass\n",
    "\n",
    "class FAISS(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            query_embedding = self.embedder_model.batch_embed([query])\n",
    "            if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                query_embedding = query_embedding[0]\n",
    "            elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        results = self._search_chunks(query_embedding, top_k)\n",
    "        \n",
    "        return [\n",
    "            Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "            for res in results\n",
    "        ]\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f5d36",
   "metadata": {},
   "source": [
    "### LLM Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        self.model_name = model_name\n",
    "        self.cache_folder = cache_folder\n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OLLAMA_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n",
    "        return model\n",
    "\n",
    "\n",
    "class Hugging_Face_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"  \n",
    "        )\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab7c4d",
   "metadata": {},
   "source": [
    "## Strategy Pattern Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cb45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self, input_text) :\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChattingStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder=None, top_k=5, return_sources=True):\n",
    "        # Get actual LLM instance\n",
    "        self.llm = llm.load_model() if hasattr(llm, 'load_model') else llm\n",
    "        \n",
    "        # Set up vector store\n",
    "        self.vector_store = vector_store\n",
    "        if embedder:\n",
    "            self.vector_store.set_embedder_model(embedder)\n",
    "        \n",
    "        if not self.vector_store.embedder_model:\n",
    "            raise ValueError(\"Embedder model must be provided\")\n",
    "        \n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self._build_chain()\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Source {i} | PDF {doc.metadata.get('pdf_id', '?')}]: {doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, 1)\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Please provide a comprehensive answer based on the context above. You MUST follow this exact format:\n",
    "\n",
    "            RESPONSE:\n",
    "            [Your main answer here]\n",
    "\n",
    "            REASONING:\n",
    "            [Explain your reasoning and how you used the context]\n",
    "\n",
    "            SOURCES:\n",
    "            [List the source numbers you referenced, for example: 1, 3, 5]\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        def retrieve_context(inputs):\n",
    "            # Use the vector store directly - it now has get_relevant_documents method\n",
    "            docs = self.vector_store.get_relevant_documents(inputs[\"question\"], top_k=self.top_k)\n",
    "            return self.format_docs(docs)\n",
    "\n",
    "        self.chain = (\n",
    "            {\n",
    "                \"context\": RunnableLambda(retrieve_context),\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def parse_structured_response(self, response_text):\n",
    "        cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)\n",
    "        cleaned_response = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_response.strip())\n",
    "\n",
    "        sections = {'response': '', 'reasoning': '', 'sources': ''}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        lines = cleaned_response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith('RESPONSE:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'response'\n",
    "                current_content = [line[9:].strip()]\n",
    "            elif line.upper().startswith('REASONING:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'reasoning'\n",
    "                current_content = [line[10:].strip()]\n",
    "            elif line.upper().startswith('SOURCES:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'sources'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif current_section and line:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "\n",
    "        source_ids = [int(x) for x in re.findall(r'\\d+', sections['sources'])] if sections['sources'] else []\n",
    "\n",
    "        return {\n",
    "            'answer': sections['response'],\n",
    "            'reasoning': sections['reasoning'],\n",
    "            'sources': source_ids,\n",
    "            'raw_response': cleaned_response\n",
    "        }\n",
    "\n",
    "    def run(self, question):\n",
    "        \"\"\"Main method to run the chain and parse result.\"\"\"\n",
    "        \n",
    "        response = self.chain.invoke({\"question\": question})\n",
    "        print(\"Past chain call\")\n",
    "        print(f\"Raw LLM response: {response}\")  # Debug: see what the LLM actually returned\n",
    "        \n",
    "        parsed = self.parse_structured_response(response)\n",
    "        print(\"Past parser call\")\n",
    "        print(f\"Parsed response: {parsed}\")  # Debug: see what was parsed\n",
    "        \n",
    "        if not self.return_sources:\n",
    "            return parsed\n",
    "\n",
    "        # Get source documents directly from vector store\n",
    "        source_docs = self.vector_store.get_relevant_documents(question, top_k=self.top_k)\n",
    "        parsed['source_documents'] = source_docs\n",
    "        parsed['source_texts'] = [doc.page_content for doc in source_docs]\n",
    "        return parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5194a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/f9lh6rmd4q1648_pfl1636zw0000gn/T/ipykernel_90806/3805475503.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n"
     ]
    }
   ],
   "source": [
    "llm=OLLAMA_LLM('qwen3:8b','llm_cache').load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.FAISS at 0x381380310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# faiss_engine=FAISS()\n",
    "faiss_store = FAISS()\n",
    "faiss_store.set_embedder_model(multilingual_embedder)\n",
    "faiss_store.create_vector_store(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6210988",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = ChattingStrategy(llm, faiss_store, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95652e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Past chain call\n",
      "Raw LLM response: <think>\n",
      "Okay, the user is asking about the carbon footprint of solar panels. Let me check the provided context to see if there's any information related to that.\n",
      "\n",
      "Looking through the sources: Source 1 talks about renewable energy platforms and solar-focused systems, but nothing about carbon footprint. Source 2 is a market research report on project management systems for PV projects. Source 3 mentions pricing for character limits, which isn't relevant. Source 4 discusses project management features for solar projects, like document control and collaboration. Source 5 talks about pricing for Oracle Aconex, a project management tool. \n",
      "\n",
      "None of the sources mention the carbon footprint of solar panels. They all focus on project management systems, tools for solar projects, and their features. There's no data on environmental impact, emissions, or lifecycle analysis of solar panels here. So, I can't answer the question based on the provided context. I need to inform the user that the information isn't available in the given sources.\n",
      "</think>\n",
      "\n",
      "RESPONSE:  \n",
      "The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and their functionalities, but none address environmental impact metrics or lifecycle analysis of solar panels.  \n",
      "\n",
      "REASONING:  \n",
      "The question about the carbon footprint of solar panels pertains to environmental impact data, such as emissions during manufacturing, operation, or disposal. However, the context exclusively discusses technical tools, platforms, and workflows for solar projects (e.g., document management, collaboration, monitoring) and does not mention any environmental metrics or lifecycle assessments.  \n",
      "\n",
      "SOURCES:  \n",
      "1, 2, 3, 4, 5\n",
      "Past parser call\n",
      "Parsed response: {'answer': 'The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and their functionalities, but none address environmental impact metrics or lifecycle analysis of solar panels.', 'reasoning': 'The question about the carbon footprint of solar panels pertains to environmental impact data, such as emissions during manufacturing, operation, or disposal. However, the context exclusively discusses technical tools, platforms, and workflows for solar projects (e.g., document management, collaboration, monitoring) and does not mention any environmental metrics or lifecycle assessments.', 'sources': [1, 2, 3, 4, 5], 'raw_response': 'RESPONSE:  \\nThe provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and their functionalities, but none address environmental impact metrics or lifecycle analysis of solar panels.  \\n\\nREASONING:  \\nThe question about the carbon footprint of solar panels pertains to environmental impact data, such as emissions during manufacturing, operation, or disposal. However, the context exclusively discusses technical tools, platforms, and workflows for solar projects (e.g., document management, collaboration, monitoring) and does not mention any environmental metrics or lifecycle assessments.  \\n\\nSOURCES:  \\n1, 2, 3, 4, 5'}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n"
     ]
    }
   ],
   "source": [
    "response = strategy.run(\"What is the carbon footprint of solar panels?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcf52d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and their functionalities, but none address environmental impact metrics or lifecycle analysis of solar panels.',\n",
       " 'reasoning': 'The question about the carbon footprint of solar panels pertains to environmental impact data, such as emissions during manufacturing, operation, or disposal. However, the context exclusively discusses technical tools, platforms, and workflows for solar projects (e.g., document management, collaboration, monitoring) and does not mention any environmental metrics or lifecycle assessments.',\n",
       " 'sources': [1, 2, 3, 4, 5],\n",
       " 'raw_response': 'RESPONSE:  \\nThe provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and their functionalities, but none address environmental impact metrics or lifecycle analysis of solar panels.  \\n\\nREASONING:  \\nThe question about the carbon footprint of solar panels pertains to environmental impact data, such as emissions during manufacturing, operation, or disposal. However, the context exclusively discusses technical tools, platforms, and workflows for solar projects (e.g., document management, collaboration, monitoring) and does not mention any environmental metrics or lifecycle assessments.  \\n\\nSOURCES:  \\n1, 2, 3, 4, 5',\n",
       " 'source_documents': [Document(metadata={'similarity': 0.35302403569221497}, page_content='Renewable and Solar -Focused Platforms During our research, we also explored platforms that are specifically designed for the renewable energy sector and others that are solar PV -focused . At first, they seemed promising because they’re built with energy systems in mind. However, after testing and reviewing them, we found that they don’t meet the type of project management needs we’re aiming for , although they market them selves as they have project management tools. Platforms like Ra Power Management (RaPM) , SenseHawk, and Payac a are examples of solar - focused systems. These tools are mainly designed for monitoring the performance of solar plants — such as tracking electricity production, system health, faults, and maintenance alerts. While they’re excellent for operations and post -installation monitoring , they do not support document approvals, workflows, submittals, or collaboration between stakeholde rs like contractors, consultants, and clients. In short, these are more like monitoring or asset management tools , not project management systems'),\n",
       "  Document(metadata={'similarity': 0.3210790753364563}, page_content='MARKET RESEARCH REPORT ON PROJECT MANAGEMENT SYSTEMS Benchmarking Tools for Document Control, Approvals, and Team Collaboration in PV proje cts. Mahi nour Mohammad Abstract This report explores existing project management systems to identify gaps and opportunities for developing a platform tailored to photovoltaic (PV) projects.'),\n",
       "  Document(metadata={'similarity': 0.24218502640724182}, page_content='Quota recharges $1.99 100000 chars . • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . $2.99 200000 chars . $7.99 500000 chars . $9.99 1 M chars . $37.99 5.5M chars . $279.99 50M chars .'),\n",
       "  Document(metadata={'similarity': 0.23647484183311462}, page_content='In too many solar and construction projects, teams juggle emails, spreadsheets, and generic cloud folders just to get a simple drawing reviewed. The result? Lost versions, unanswered questions, and schedule hiccups. This report cuts through the chaos by co mparing five best -in-class platforms —two generalists and three construction -focused —against the 12 must -have features for any professional solar project management syste m. Key Features We’re Tracking 1. Assigning Roles The ability to designate specific users or groups (e.g., “Project Manager,” “Reviewer,” “Electrician Team”) to tasks, submittals, or documents —so everyone knows who’s responsible for what. 2. Document Approval Workflow A structured process that routes documents or deliverables through one or more review and approval steps (with defined approvers, due -dates, and status transitions) before they’re considered “approved.” 3. Versioning Keeping track of each time a file is updated or replaced, so you can see prior iterations (e.g., “Drawing Rev A,” “Rev B”) and roll back if n eeded. 4. Markup Versions The ability to annotate or mark up each version of a document or drawing —circling areas, adding comments, highlighting changes —often directly on the file itself. 5'),\n",
       "  Document(metadata={'similarity': 0.2218591272830963}, page_content=\"Pricing: Just like PMWeb, Oracle Aconex does not publicly list pricing because it's offered as an enterprise solution , and pricing is tailored to the size, complexity, and duration of your project. Factors Affecting Aconex Pricing: • Project size and duration (short -term vs long -term) • Number of users and collaborators • Scope of modules (document control, BIM, cost management, workflow automation) • Cloud storage volume • Required integrations (e.g., ERP systems) • Training, implementation, and support level Estimated Pricing Range (from user reports and RFPs): • Per Project License (Annual): Starting around $15,000 –$30,000+ • Enterprise Multi -Project License: Can go $100,000+/year • Per User (if applied): ~$100 –$150/user/month (not always the model used) Pros: 1- Centralized Document Management 2- Seamless Collaboration Across Organizations 3- Customizable Workflows 4- On-Site Issue Management\")],\n",
       " 'source_texts': ['Renewable and Solar -Focused Platforms During our research, we also explored platforms that are specifically designed for the renewable energy sector and others that are solar PV -focused . At first, they seemed promising because they’re built with energy systems in mind. However, after testing and reviewing them, we found that they don’t meet the type of project management needs we’re aiming for , although they market them selves as they have project management tools. Platforms like Ra Power Management (RaPM) , SenseHawk, and Payac a are examples of solar - focused systems. These tools are mainly designed for monitoring the performance of solar plants — such as tracking electricity production, system health, faults, and maintenance alerts. While they’re excellent for operations and post -installation monitoring , they do not support document approvals, workflows, submittals, or collaboration between stakeholde rs like contractors, consultants, and clients. In short, these are more like monitoring or asset management tools , not project management systems',\n",
       "  'MARKET RESEARCH REPORT ON PROJECT MANAGEMENT SYSTEMS Benchmarking Tools for Document Control, Approvals, and Team Collaboration in PV proje cts. Mahi nour Mohammad Abstract This report explores existing project management systems to identify gaps and opportunities for developing a platform tailored to photovoltaic (PV) projects.',\n",
       "  'Quota recharges $1.99 100000 chars . • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . $2.99 200000 chars . $7.99 500000 chars . $9.99 1 M chars . $37.99 5.5M chars . $279.99 50M chars .',\n",
       "  'In too many solar and construction projects, teams juggle emails, spreadsheets, and generic cloud folders just to get a simple drawing reviewed. The result? Lost versions, unanswered questions, and schedule hiccups. This report cuts through the chaos by co mparing five best -in-class platforms —two generalists and three construction -focused —against the 12 must -have features for any professional solar project management syste m. Key Features We’re Tracking 1. Assigning Roles The ability to designate specific users or groups (e.g., “Project Manager,” “Reviewer,” “Electrician Team”) to tasks, submittals, or documents —so everyone knows who’s responsible for what. 2. Document Approval Workflow A structured process that routes documents or deliverables through one or more review and approval steps (with defined approvers, due -dates, and status transitions) before they’re considered “approved.” 3. Versioning Keeping track of each time a file is updated or replaced, so you can see prior iterations (e.g., “Drawing Rev A,” “Rev B”) and roll back if n eeded. 4. Markup Versions The ability to annotate or mark up each version of a document or drawing —circling areas, adding comments, highlighting changes —often directly on the file itself. 5',\n",
       "  \"Pricing: Just like PMWeb, Oracle Aconex does not publicly list pricing because it's offered as an enterprise solution , and pricing is tailored to the size, complexity, and duration of your project. Factors Affecting Aconex Pricing: • Project size and duration (short -term vs long -term) • Number of users and collaborators • Scope of modules (document control, BIM, cost management, workflow automation) • Cloud storage volume • Required integrations (e.g., ERP systems) • Training, implementation, and support level Estimated Pricing Range (from user reports and RFPs): • Per Project License (Annual): Starting around $15,000 –$30,000+ • Enterprise Multi -Project License: Can go $100,000+/year • Per User (if applied): ~$100 –$150/user/month (not always the model used) Pros: 1- Centralized Document Management 2- Seamless Collaboration Across Organizations 3- Customizable Workflows 4- On-Site Issue Management\"]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfe4fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, tools for solar energy projects, and pricing details, but none address environmental impact metrics such as carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
