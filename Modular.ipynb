{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"âœ… Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_faiss_index(self, chunks_embed):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def search_faiss(self, faiss_index, index_mapping, query_embedding, top_k=5):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_faiss_search(self, chunks, embeddings_dict):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def search_chunks(self, faiss_index, index_mapping, chunks_dict, query_embedding, top_k=5):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_faiss_index(self, faiss_index, file_index_name):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_faiss_index(self, file_index_name):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48680d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISS(VectorStoreBase):\n",
    "   def __init__(self):\n",
    "       self.index = None\n",
    "       self.index_mapping = None\n",
    "       self.chunks_dict = None\n",
    "       self.dimension = None\n",
    "       self.index_type = \"IndexFlatIP\"  \n",
    "       self.total_vectors = 0\n",
    "\n",
    "   def create_vector_store(self, documents, embedder_model):\n",
    "       texts = [doc.page_content for doc in documents]\n",
    "       embeddings = embedder_model.batch_embed(texts)\n",
    "       print(embeddings)\n",
    "\n",
    "       embeddings = np.array(embeddings).astype(\"float32\")\n",
    "       self.dimension = embeddings.shape[1]\n",
    "\n",
    "       self.index = faiss.IndexFlatIP(self.dimension)\n",
    "       self.index.add(embeddings)\n",
    "\n",
    "       self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "       self.index_mapping = list(self.chunks_dict.keys())\n",
    "       self.total_vectors = self.index.ntotal\n",
    "\n",
    "       print(f\"Created FAISS index with {self.index.ntotal} vectors of dimension {self.dimension}\")\n",
    "       return self.index, self.index_mapping, self.chunks_dict\n",
    "\n",
    "   def create_faiss_index(self, chunks_embed):\n",
    "       indices = list(chunks_embed.keys())\n",
    "       embeddings = np.array([chunks_embed[idx] for idx in indices]).astype('float32')\n",
    "       self.dimension = embeddings.shape[1]\n",
    "       self.index = faiss.IndexFlatIP(self.dimension)\n",
    "       self.index.add(embeddings)\n",
    "       self.index_mapping = indices\n",
    "       self.total_vectors = self.index.ntotal\n",
    "\n",
    "       print(f\"Created FAISS index with {self.index.ntotal} vectors of dimension {self.dimension}\")\n",
    "       return self.index, indices\n",
    "\n",
    "   def search_faiss(self, faiss_index, index_mapping, query_embedding, top_k=5):\n",
    "       query_embedding = np.array([query_embedding]).astype('float32')\n",
    "       distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "       results = []\n",
    "       for i in range(top_k):\n",
    "           faiss_idx = indices[0][i]\n",
    "           if faiss_idx != -1:\n",
    "               your_idx = index_mapping[faiss_idx]\n",
    "               distance = distances[0][i]\n",
    "               results.append((your_idx, distance))\n",
    "       return results\n",
    "\n",
    "   def setup_faiss_search(self, chunks, embeddings_dict):\n",
    "       self.chunks_dict = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "       faiss_index, index_mapping = self.create_faiss_index(embeddings_dict)\n",
    "       return faiss_index, self.index_mapping, self.chunks_dict\n",
    "\n",
    "   def search_chunks(self, faiss_index, chunks_dict, query_embedding, top_k=5):\n",
    "       results = self.search_faiss(faiss_index, self.index_mapping, query_embedding, top_k)\n",
    "       formatted_results = []\n",
    "       for chunk_idx, distance in results:\n",
    "           formatted_results.append({\n",
    "               'chunk_id': chunk_idx,\n",
    "               'text': chunks_dict[chunk_idx],\n",
    "               'distance': distance,\n",
    "               'similarity': 1 / (1 + distance)\n",
    "           })\n",
    "       return formatted_results\n",
    "\n",
    "   def save_faiss_index(self, faiss_index, file_index_name):\n",
    "       faiss.write_index(faiss_index, f\"{file_index_name}.faiss\")\n",
    "       print(f\"Index saved to {file_index_name}.faiss\")\n",
    "\n",
    "   def load_faiss_index(self, file_index_name):\n",
    "       self.index = faiss.read_index(f\"{file_index_name}.faiss\")\n",
    "       self.total_vectors = self.index.ntotal\n",
    "       print(f\"Index loaded from {file_index_name}.faiss\")\n",
    "       return self.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "## Files is alist of Documents \n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_engine=FAISS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bbfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06576047  0.11895962 -0.04524058 ... -0.08136054  0.00303138\n",
      "   0.01381639]\n",
      " [-0.04451338  0.04813152 -0.04466828 ... -0.0623714  -0.03471209\n",
      "  -0.00216465]\n",
      " [ 0.00896442  0.0438443  -0.05153349 ... -0.01728812  0.035436\n",
      "   0.0190528 ]\n",
      " ...\n",
      " [-0.12122848 -0.04116969 -0.02783496 ... -0.0202268   0.05930107\n",
      "   0.02544023]\n",
      " [-0.12925795 -0.00389782 -0.05988017 ... -0.0438294   0.06989577\n",
      "   0.0108128 ]\n",
      " [-0.01499175 -0.00426901 -0.02039175 ...  0.01145428  0.02771816\n",
      "   0.02089966]]\n",
      "Created FAISS index with 71 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "index, index_mapping, chunks_dict=faiss_engine.create_vector_store(chunked_docs,multilingual_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fe0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 71 vectors\n",
      "Index Mapping Length: 71\n",
      "Chunks Dict Length: 71\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAISS index contains {index.ntotal} vectors\")\n",
    "print(f\"Index Mapping Length: {len(index_mapping)}\")\n",
    "print(f\"Chunks Dict Length: {len(chunks_dict)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134bcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result indices: [[52 55  7]]\n",
      "Distances: [[0.4538141  0.3971988  0.38543445]]\n",
      "\n",
      "Chunk 52: Document Regis ter: Think of the Document Register as Aconexâ€™s main repository . It's where all the project documents are stored, and itâ€™s a must to upload any document here before it can be shared with others. When you upload a document, you add key details like: â€¢ Document number â€¢ Title â€¢ Revisio\n",
      "\n",
      "Chunk 55: Mail in Aconex: The Mail module is for everyday communication on the project â€” answering questions, discussing ideas, and sharing quick updates. While you can attach documents to em ails, remember that these attachments donâ€™t get uploaded to the document register. The file is simply attached for vie\n",
      "\n",
      "Chunk 7: â€¢ Process Modes 1. Professional Translation: The AI automatically selects the optimal style and format for the translation based on the do cumentâ€™s context, audience, and purpose. For example, legal documents would adopt a formal tone with precise terminology, while marketing materials might use a p\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the document about?\"\n",
    "query_embedding = multilingual_embedder.batch_embed([query])[0]\n",
    "query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_embedding, k=3)\n",
    "print(\"Search result indices:\", I)\n",
    "print(\"Distances:\", D)\n",
    "\n",
    "for i in I[0]:\n",
    "    print(f\"\\nChunk {i}: {chunks_dict.get(i, '[Missing chunk]')[:300]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada29b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
