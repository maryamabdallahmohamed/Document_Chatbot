{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_faiss_index(self, chunks_embed):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def search_faiss(self, faiss_index, index_mapping, query_embedding, top_k=5):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_faiss_search(self, chunks, embeddings_dict):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def search_chunks(self, faiss_index, index_mapping, chunks_dict, query_embedding, top_k=5):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_faiss_index(self, faiss_index, file_index_name):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_faiss_index(self, file_index_name):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48680d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISS(VectorStoreBase):\n",
    "   def __init__(self):\n",
    "       self.index = None\n",
    "       self.index_mapping = None\n",
    "       self.chunks_dict = None\n",
    "       self.dimension = None\n",
    "       self.index_type = \"IndexFlatIP\"  \n",
    "       self.total_vectors = 0\n",
    "\n",
    "   def create_vector_store(self, documents, embedder_model):\n",
    "       texts = [doc.page_content for doc in documents]\n",
    "       embeddings = embedder_model.batch_embed(texts)\n",
    "       print(embeddings)\n",
    "\n",
    "       embeddings = np.array(embeddings).astype(\"float32\")\n",
    "       self.dimension = embeddings.shape[1]\n",
    "\n",
    "       self.index = faiss.IndexFlatIP(self.dimension)\n",
    "       self.index.add(embeddings)\n",
    "\n",
    "       self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "       self.index_mapping = list(self.chunks_dict.keys())\n",
    "       self.total_vectors = self.index.ntotal\n",
    "\n",
    "       print(f\"Created FAISS index with {self.index.ntotal} vectors of dimension {self.dimension}\")\n",
    "       return self.index, self.index_mapping, self.chunks_dict\n",
    "\n",
    "   def create_faiss_index(self, chunks_embed):\n",
    "       indices = list(chunks_embed.keys())\n",
    "       embeddings = np.array([chunks_embed[idx] for idx in indices]).astype('float32')\n",
    "       self.dimension = embeddings.shape[1]\n",
    "       self.index = faiss.IndexFlatIP(self.dimension)\n",
    "       self.index.add(embeddings)\n",
    "       self.index_mapping = indices\n",
    "       self.total_vectors = self.index.ntotal\n",
    "\n",
    "       print(f\"Created FAISS index with {self.index.ntotal} vectors of dimension {self.dimension}\")\n",
    "       return self.index, indices\n",
    "\n",
    "   def search_faiss(self, faiss_index, index_mapping, query_embedding, top_k=5):\n",
    "       query_embedding = np.array([query_embedding]).astype('float32')\n",
    "       distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "       results = []\n",
    "       for i in range(top_k):\n",
    "           faiss_idx = indices[0][i]\n",
    "           if faiss_idx != -1:\n",
    "               your_idx = index_mapping[faiss_idx]\n",
    "               distance = distances[0][i]\n",
    "               results.append((your_idx, distance))\n",
    "       return results\n",
    "\n",
    "   def setup_faiss_search(self, chunks, embeddings_dict):\n",
    "       self.chunks_dict = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "       faiss_index, index_mapping = self.create_faiss_index(embeddings_dict)\n",
    "       return faiss_index, self.index_mapping, self.chunks_dict\n",
    "\n",
    "   def search_chunks(self, faiss_index, chunks_dict, query_embedding, top_k=5):\n",
    "       results = self.search_faiss(faiss_index, self.index_mapping, query_embedding, top_k)\n",
    "       formatted_results = []\n",
    "       for chunk_idx, distance in results:\n",
    "           formatted_results.append({\n",
    "               'chunk_id': chunk_idx,\n",
    "               'text': chunks_dict[chunk_idx],\n",
    "               'distance': distance,\n",
    "               'similarity': 1 / (1 + distance)\n",
    "           })\n",
    "       return formatted_results\n",
    "\n",
    "   def save_faiss_index(self, faiss_index, file_index_name):\n",
    "       faiss.write_index(faiss_index, f\"{file_index_name}.faiss\")\n",
    "       print(f\"Index saved to {file_index_name}.faiss\")\n",
    "\n",
    "   def load_faiss_index(self, file_index_name):\n",
    "       self.index = faiss.read_index(f\"{file_index_name}.faiss\")\n",
    "       self.total_vectors = self.index.ntotal\n",
    "       print(f\"Index loaded from {file_index_name}.faiss\")\n",
    "       return self.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "## Files is alist of Documents \n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_engine=FAISS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bbfa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06576047  0.11895962 -0.04524058 ... -0.08136054  0.00303138\n",
      "   0.01381639]\n",
      " [-0.04451338  0.04813152 -0.04466828 ... -0.0623714  -0.03471209\n",
      "  -0.00216465]\n",
      " [ 0.00896442  0.0438443  -0.05153349 ... -0.01728812  0.035436\n",
      "   0.0190528 ]\n",
      " ...\n",
      " [-0.12122848 -0.04116969 -0.02783496 ... -0.0202268   0.05930107\n",
      "   0.02544023]\n",
      " [-0.12925795 -0.00389782 -0.05988017 ... -0.0438294   0.06989577\n",
      "   0.0108128 ]\n",
      " [-0.01499175 -0.00426901 -0.02039175 ...  0.01145428  0.02771816\n",
      "   0.02089966]]\n",
      "Created FAISS index with 71 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "index, index_mapping, chunks_dict=faiss_engine.create_vector_store(chunked_docs,multilingual_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fe0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 71 vectors\n",
      "Index Mapping Length: 71\n",
      "Chunks Dict Length: 71\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAISS index contains {index.ntotal} vectors\")\n",
    "print(f\"Index Mapping Length: {len(index_mapping)}\")\n",
    "print(f\"Chunks Dict Length: {len(chunks_dict)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134bcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result indices: [[52 55  7]]\n",
      "Distances: [[0.4538141  0.3971988  0.38543445]]\n",
      "\n",
      "Chunk 52: Document Regis ter: Think of the Document Register as Aconex’s main repository . It's where all the project documents are stored, and it’s a must to upload any document here before it can be shared with others. When you upload a document, you add key details like: • Document number • Title • Revisio\n",
      "\n",
      "Chunk 55: Mail in Aconex: The Mail module is for everyday communication on the project — answering questions, discussing ideas, and sharing quick updates. While you can attach documents to em ails, remember that these attachments don’t get uploaded to the document register. The file is simply attached for vie\n",
      "\n",
      "Chunk 7: • Process Modes 1. Professional Translation: The AI automatically selects the optimal style and format for the translation based on the do cument’s context, audience, and purpose. For example, legal documents would adopt a formal tone with precise terminology, while marketing materials might use a p\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the document about?\"\n",
    "query_embedding = multilingual_embedder.batch_embed([query])[0]\n",
    "query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "D, I = index.search(query_embedding, k=3)\n",
    "print(\"Search result indices:\", I)\n",
    "print(\"Distances:\", D)\n",
    "\n",
    "for i in I[0]:\n",
    "    print(f\"\\nChunk {i}: {chunks_dict.get(i, '[Missing chunk]')[:300]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada29b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
