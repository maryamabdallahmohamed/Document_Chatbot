{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='./embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_index(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_index(self, file_path):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ce8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSBasic(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            query_embedding = self.embedder_model.batch_embed([query])\n",
    "            if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                query_embedding = query_embedding[0]\n",
    "            elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        results = self._search_chunks(query_embedding, top_k)\n",
    "        \n",
    "        return [\n",
    "            Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "            for res in results\n",
    "        ]\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed1060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the FAISS vectorstore\\nfaiss_store = FAISS(embedder_model=your_embedding_model)\\n\\n# Create vectorstore from Document objects (new method)\\nfaiss_store.create_vectorstore(document_list, normalize_embeddings=True)\\n\\n# Or use the original method\\nfaiss_store.create_vector_store(document_list, your_embedding_model)\\n\\n# Search for relevant documents\\nresults = faiss_store.get_relevant_documents(\"your query\", top_k=5)\\n\\n# Save and load\\nfaiss_store.save_index(\"my_index\")\\nfaiss_store.load_index(\"my_index\", your_embedding_model)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FAISSImproved(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "        # New attributes for enhanced functionality\n",
    "        self.docstore = None\n",
    "        self.index_to_docstore_id = None\n",
    "        self.documents = None  # Store original Document objects\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def create_vectorstore(self, docs, normalize_embeddings=True):\n",
    "        \"\"\"\n",
    "        Create a FAISS vector store from a list of Document objects.\n",
    "        Each document should have metadata like pdf_id, chunk_id, etc.\n",
    "        \n",
    "        Args:\n",
    "            docs: List of Document objects\n",
    "            normalize_embeddings: Whether to normalize embeddings for cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            self: Returns the FAISS instance for method chaining\n",
    "        \"\"\"\n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required. Set it during initialization or call set_embedder_model()\")\n",
    "        \n",
    "        # Extract texts from Document objects\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        # Initialize FAISS Index\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity if requested\n",
    "        if normalize_embeddings:\n",
    "            faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store original Document objects and create mappings\n",
    "        self.documents = docs\n",
    "        self.docstore = {str(i): doc for i, doc in enumerate(docs)}\n",
    "        self.index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n",
    "        \n",
    "        # Also maintain backward compatibility with chunks_dict\n",
    "        self.chunks_dict = {i: doc.page_content for i, doc in enumerate(docs)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created vectorstore with {self.total_vectors} documents of dim {self.dimension}\")\n",
    "        print(f\"[FAISS] Normalization: {'enabled' if normalize_embeddings else 'disabled'}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() or create_vectorstore() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            # Use embed_query if available, otherwise fall back to batch_embed\n",
    "            if hasattr(self.embedder_model, 'embed_query'):\n",
    "                query_embedding = self.embedder_model.embed_query(query)\n",
    "            else:\n",
    "                query_embedding = self.embedder_model.batch_embed([query])\n",
    "                if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                    query_embedding = query_embedding[0]\n",
    "                elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                    query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        if self.docstore is not None:\n",
    "            # Use enhanced docstore-based retrieval\n",
    "            results = self._search_with_docstore(query_embedding, top_k)\n",
    "        else:\n",
    "            # Fall back to original chunk-based retrieval\n",
    "            results = self._search_chunks(query_embedding, top_k)\n",
    "            # Convert to Document objects for consistency\n",
    "            results = [\n",
    "                Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "                for res in results\n",
    "            ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _search_with_docstore(self, query_embedding, top_k=5):\n",
    "        \"\"\"Enhanced search function using docstore - returns Document objects\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results using docstore\n",
    "        documents = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx in self.index_to_docstore_id:\n",
    "                docstore_id = self.index_to_docstore_id[faiss_idx]\n",
    "                if docstore_id in self.docstore:\n",
    "                    doc = self.docstore[docstore_id]\n",
    "                    similarity = float(distances[0][i])\n",
    "                    \n",
    "                    # Create a copy of the document with updated metadata\n",
    "                    enhanced_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                    enhanced_metadata[\"similarity\"] = similarity\n",
    "                    enhanced_metadata[\"retrieval_index\"] = faiss_idx\n",
    "                    \n",
    "                    enhanced_doc = Document(\n",
    "                        page_content=doc.page_content,\n",
    "                        metadata=enhanced_metadata\n",
    "                    )\n",
    "                    documents.append(enhanced_doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata (enhanced to include new attributes)\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type,\n",
    "            'docstore': self.docstore,\n",
    "            'index_to_docstore_id': self.index_to_docstore_id,\n",
    "            'documents': self.documents\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Load enhanced attributes if they exist (backward compatibility)\n",
    "        self.docstore = metadata.get('docstore', None)\n",
    "        self.index_to_docstore_id = metadata.get('index_to_docstore_id', None)\n",
    "        self.documents = metadata.get('documents', None)\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        if self.docstore is not None:\n",
    "            print(f\"[FAISS] Enhanced docstore mode enabled\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None,\n",
    "            'has_docstore': self.docstore is not None,\n",
    "            'has_documents': self.documents is not None\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize the FAISS vectorstore\n",
    "faiss_store = FAISS(embedder_model=your_embedding_model)\n",
    "\n",
    "# Create vectorstore from Document objects (new method)\n",
    "faiss_store.create_vectorstore(document_list, normalize_embeddings=True)\n",
    "\n",
    "# Or use the original method\n",
    "faiss_store.create_vector_store(document_list, your_embedding_model)\n",
    "\n",
    "# Search for relevant documents\n",
    "results = faiss_store.get_relevant_documents(\"your query\", top_k=5)\n",
    "\n",
    "# Save and load\n",
    "faiss_store.save_index(\"my_index\")\n",
    "faiss_store.load_index(\"my_index\", your_embedding_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f5d36",
   "metadata": {},
   "source": [
    "### LLM Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        self.model_name = model_name\n",
    "        self.cache_folder = cache_folder\n",
    "        self.device = ('cpu'\n",
    "            # 'cuda' if torch.cuda.is_available()\n",
    "            # else 'mps' if torch.backends.mps.is_available()\n",
    "            # else 'cpu'\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OLLAMA_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n",
    "        return model\n",
    "\n",
    "\n",
    "class Hugging_Face_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"  \n",
    "        )\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab7c4d",
   "metadata": {},
   "source": [
    "## Strategy Pattern Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71cb45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self, input_text) :\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5638e6",
   "metadata": {},
   "source": [
    "#### Chatting Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ab0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChattingStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        # Get actual LLM instance\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self._build_chain()\n",
    "\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Source {i} | PDF {doc.metadata.get('pdf_id', '?')}]: {doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, 1)\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Please provide a comprehensive answer based on the context above. You MUST follow this exact format:\n",
    "\n",
    "            RESPONSE:\n",
    "            [Your main answer here]\n",
    "\n",
    "            REASONING:\n",
    "            [Explain your reasoning and how you used the context]\n",
    "\n",
    "            SOURCES:\n",
    "            [List the source numbers you referenced, for example: 1, 3, 5]\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        def retrieve_context(inputs):\n",
    "            # Use the vector store directly \n",
    "            docs = self.vector_store.get_relevant_documents(inputs[\"question\"], top_k=self.top_k)\n",
    "            return self.format_docs(docs)\n",
    "\n",
    "        self.chain = ({\n",
    "                \"context\": RunnableLambda(retrieve_context), \n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def parse_structured_response(self, response_text):\n",
    "        cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)\n",
    "        cleaned_response = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_response.strip())\n",
    "\n",
    "        sections = {'response': '', 'reasoning': '', 'sources': ''}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        lines = cleaned_response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith('RESPONSE:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'response'\n",
    "                current_content = [line[9:].strip()]\n",
    "            elif line.upper().startswith('REASONING:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'reasoning'\n",
    "                current_content = [line[10:].strip()]\n",
    "            elif line.upper().startswith('SOURCES:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'sources'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif current_section and line:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "\n",
    "        source_ids = [int(x) for x in re.findall(r'\\d+', sections['sources'])] if sections['sources'] else []\n",
    "\n",
    "        return {\n",
    "            'answer': sections['response'],\n",
    "            'reasoning': sections['reasoning'],\n",
    "            'sources': source_ids,\n",
    "            'raw_response': cleaned_response\n",
    "        }\n",
    "\n",
    "    def run(self, question):\n",
    "        \"\"\"Main method to run the chain and parse result.\"\"\"\n",
    "        \n",
    "        response = self.chain.invoke({\"question\": question})\n",
    "        print(\"Past chain call\")\n",
    "        print(f\"Raw LLM response: {response}\")  \n",
    "        \n",
    "        parsed = self.parse_structured_response(response)\n",
    "        print(\"Past parser call\")\n",
    "        print(f\"Parsed response: {parsed}\")  \n",
    "        \n",
    "        if not self.return_sources:\n",
    "            return parsed\n",
    "\n",
    "    \n",
    "        source_docs = self.vector_store.get_relevant_documents(question, top_k=self.top_k)\n",
    "        parsed['source_documents'] = source_docs\n",
    "        parsed['source_texts'] = [doc.page_content for doc in source_docs]\n",
    "        return parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71af65b",
   "metadata": {},
   "source": [
    "#### Summerization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769e1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SummarizationStrategy(TaskStrategy):\n",
    "#     def __init__(self, llm):\n",
    "#         self.llm = llm\n",
    "#         self.prompt = ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", \"\"\"Summarize the following document using this format:\n",
    "\n",
    "#             **Main Topic:** [One sentence describing what this document is about]\n",
    "\n",
    "#             **Key Points:**\n",
    "#             - [Most important point]\n",
    "#             - [Second most important point]  \n",
    "#             - [Third most important point]\n",
    "\n",
    "#             **Details:** [Supporting information, numbers, examples]\n",
    "\n",
    "#             **Conclusion:** [Main takeaway or implication]\n",
    "\n",
    "#             Document: {context}\"\"\")\n",
    "#         ])\n",
    "#         self.chain = create_stuff_documents_chain(self.llm, self.prompt)\n",
    "    \n",
    "#     def run(self, document):\n",
    "#         result = self.chain.invoke({\"context\": [document]})\n",
    "#         print(result)\n",
    "\n",
    "\n",
    "class SummarizationStrategy(TaskStrategy):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Summarize the following document using this format:\n",
    "\n",
    "**Main Topic:** [One sentence describing what this document is about]\n",
    "\n",
    "**Key Points:**\n",
    "- [Most important point]\n",
    "- [Second most important point]  \n",
    "- [Third most important point]\n",
    "\n",
    "**Details:** [Supporting information, numbers, examples]\n",
    "\n",
    "**Conclusion:** [Main takeaway or implication]\n",
    "\n",
    "Document: {context}\"\"\")\n",
    "        ])\n",
    "\n",
    "    def run(self, document):\n",
    "        # Format prompt manually\n",
    "        formatted_prompt = self.prompt.format(context=document)\n",
    "        \n",
    "        # Directly invoke the LLM\n",
    "        result = self.llm.invoke(formatted_prompt)\n",
    "        \n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deff017",
   "metadata": {},
   "source": [
    "#### Question Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ad9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "class QuestionStrategy(TaskStrategy):\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant tasked with generating question-answer pairs for study purposes.\n",
    "\n",
    "Text:\n",
    "{context}\n",
    "\n",
    "Generate 5-6 meaningful questions based only on the above text. \n",
    "\n",
    "IMPORTANT: Format your output exactly as shown below with no additional text, explanations, or formatting:\n",
    "\n",
    "Q1: [question text]\n",
    "Q2: [question text]\n",
    "Q3: [question text]\n",
    "\"\"\")\n",
    "        self.qa_chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def parse_qa_pairs(self, qa_output):\n",
    "        qa_pairs = []\n",
    "        lines = qa_output.strip().split('\\n')\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            q_match = re.match(r'Q(\\d+):\\s*(.+)', lines[i])\n",
    "            if q_match and i + 1 < len(lines):\n",
    "                question = q_match.group(2).strip()\n",
    "                a_match = re.match(f'A{q_match.group(1)}:\\s*(.+)', lines[i + 1])\n",
    "                if a_match:\n",
    "                    answer = a_match.group(1).strip()\n",
    "                    qa_pairs.append({'question': question, 'answer': answer})\n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return qa_pairs\n",
    "\n",
    "    def run(self, doc: Document):\n",
    "        try:\n",
    "            qa_output = self.qa_chain.invoke({\"context\": doc.page_content})\n",
    "            parsed_qa = self.parse_qa_pairs(qa_output)\n",
    "            print(qa_output)\n",
    "            print(parsed_qa)\n",
    "\n",
    "            return {\n",
    "                \"pdf_id\": doc.metadata.get(\"pdf_id\"),\n",
    "                \"chunk_id\": doc.metadata.get(\"chunk_id\"),\n",
    "                \"text\": doc.page_content,\n",
    "                \"qa_output\": qa_output,\n",
    "                \"parsed_qa\": parsed_qa\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ QA generation failed for chunk {doc.metadata}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Testing cell\n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   7.843944311141968\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5194a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.0021409988403320312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/f9lh6rmd4q1648_pfl1636zw0000gn/T/ipykernel_75346/422372000.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "llm=OLLAMA_LLM('llama3:8b','llm_cache').load_model()\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6187b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n",
      "Time Taken to process:   58.361016035079956\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "basic_fais=FAISSBasic(multilingual_embedder)\n",
    "basic_fais.create_vector_store(chunked_docs)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n",
      "Time Taken to process:   58.04871392250061\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "fais_improved = FAISSImproved()\n",
    "fais_improved.set_embedder_model(multilingual_embedder)\n",
    "fais_improved.create_vector_store(chunked_docs)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "887f0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.0017290115356445312\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "strategy_basic = ChattingStrategy(llm, basic_fais, multilingual_embedder,top_k=5)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6210988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to process:   0.00011706352233886719\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "strategy_improved = ChattingStrategy(llm, fais_improved, multilingual_embedder,top_k=5)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95652e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Past chain call\n",
      "Raw LLM response: RESPONSE:\n",
      "The carbon footprint of solar panels is not directly addressed in the provided context. The sources focus on project management systems and platforms for renewable energy and solar-focused projects.\n",
      "\n",
      "REASONING:\n",
      "I did not find any information related to the carbon footprint of solar panels in the provided context. The sources are primarily focused on discussing project management systems, their features, and pricing models. While some of the sources mention solar panels or photovoltaic (PV) projects, they do not provide information on the environmental impact or carbon footprint of these technologies.\n",
      "\n",
      "SOURCES:\n",
      "None\n",
      "Past parser call\n",
      "Parsed response: {'answer': 'The carbon footprint of solar panels is not directly addressed in the provided context. The sources focus on project management systems and platforms for renewable energy and solar-focused projects.', 'reasoning': 'I did not find any information related to the carbon footprint of solar panels in the provided context. The sources are primarily focused on discussing project management systems, their features, and pricing models. While some of the sources mention solar panels or photovoltaic (PV) projects, they do not provide information on the environmental impact or carbon footprint of these technologies.', 'sources': [], 'raw_response': 'RESPONSE:\\nThe carbon footprint of solar panels is not directly addressed in the provided context. The sources focus on project management systems and platforms for renewable energy and solar-focused projects.\\n\\nREASONING:\\nI did not find any information related to the carbon footprint of solar panels in the provided context. The sources are primarily focused on discussing project management systems, their features, and pricing models. While some of the sources mention solar panels or photovoltaic (PV) projects, they do not provide information on the environmental impact or carbon footprint of these technologies.\\n\\nSOURCES:\\nNone'}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Time Taken to process:   46.176206827163696\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "response = strategy_basic.run(\"What is the carbon footprint of solar panels?\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b85c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Past chain call\n",
      "Raw LLM response: RESPONSE:\n",
      "The carbon footprint of solar panels is not directly addressed in the provided context. However, it's important to note that solar energy is a renewable and clean source of power, offering a significant reduction in greenhouse gas emissions compared to traditional fossil fuel-based power generation.\n",
      "\n",
      "REASONING:\n",
      "I did not find any information about the carbon footprint of solar panels in the provided context because the sources are focused on project management systems for solar projects, rather than the environmental impact of solar energy itself. The context discusses various platforms and their limitations in supporting project management needs, but does not delve into the environmental benefits or drawbacks of solar energy.\n",
      "\n",
      "SOURCES:\n",
      "None (since this answer is not based on any specific source from the provided context)\n",
      "Past parser call\n",
      "Parsed response: {'answer': \"The carbon footprint of solar panels is not directly addressed in the provided context. However, it's important to note that solar energy is a renewable and clean source of power, offering a significant reduction in greenhouse gas emissions compared to traditional fossil fuel-based power generation.\", 'reasoning': 'I did not find any information about the carbon footprint of solar panels in the provided context because the sources are focused on project management systems for solar projects, rather than the environmental impact of solar energy itself. The context discusses various platforms and their limitations in supporting project management needs, but does not delve into the environmental benefits or drawbacks of solar energy.', 'sources': [], 'raw_response': \"RESPONSE:\\nThe carbon footprint of solar panels is not directly addressed in the provided context. However, it's important to note that solar energy is a renewable and clean source of power, offering a significant reduction in greenhouse gas emissions compared to traditional fossil fuel-based power generation.\\n\\nREASONING:\\nI did not find any information about the carbon footprint of solar panels in the provided context because the sources are focused on project management systems for solar projects, rather than the environmental impact of solar energy itself. The context discusses various platforms and their limitations in supporting project management needs, but does not delve into the environmental benefits or drawbacks of solar energy.\\n\\nSOURCES:\\nNone (since this answer is not based on any specific source from the provided context)\"}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Time Taken to process:   44.53877592086792\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "response = strategy_improved.run(\"What is the carbon footprint of solar panels?\")\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe4fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The carbon footprint of solar panels is not directly addressed in the provided context. However, it's important to note that solar energy is a renewable and clean source of power, offering a significant reduction in greenhouse gas emissions compared to traditional fossil fuel-based power generation.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb48ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization=SummarizationStrategy(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff954633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Main Topic:** Market Research Report: Analysis of Document Translation Tools Evaluating Leading Solutions for Multilingual Document Translation\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* The report evaluates the performance of six document translation tools (Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL) in translating documents from Arabic, French, and English languages.\n",
      "* The tools were tested for their ability to preserve layout, support OCR, translate text accurately, and offer flexible pricing models.\n",
      "* The report highlights the strengths and weaknesses of each tool, including limitations in handling mixed language content, tables, and image-based text.\n",
      "\n",
      "**Details:**\n",
      "\n",
      "* The evaluation involved subjecting each tool to a series of test cases, including translating text-based documents, scanned images, tables (as text and as images), and documents with stamps or signatures.\n",
      "* The report provides detailed findings for each tool, highlighting their pros and cons in terms of layout preservation, OCR support, translation accuracy, and pricing models.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "* No single tool fully meets all requirements for accurate, efficient, and cost-effective document translation across Arabic, French, and English languages.\n",
      "* The report recommends several features to enhance the functionality of document translation tools, including editing capabilities, conversion features, image translation, selective OCR activation, AI-powered features, process modes, and a split-view translation interface.\n",
      "* Implementing these enhancements could significantly improve the market competitiveness of document translation tools.\n",
      "**Main Topic:** Comparative Analysis of Construction-Focused Project Management Platforms\n",
      "\n",
      "This document provides a comprehensive comparison of five project management platforms specifically designed for the construction industry: Monday.com, Wrike, PMWeb, Aconex, and Procore. The analysis assesses each platform's features, strengths, and weaknesses in supporting various aspects of construction project management.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Monday.com and Wrike**: While initially designed as general-purpose project management tools, they offer intuitive interfaces, flexible dashboards, and excellent task tracking capabilities. However, they fall short on advanced construction-specific workflows such as submittal tracking, formal transmittals, or OCR-based document processing.\n",
      "2. **PMWeb, Aconex, and Procore**: These platforms are built with the complexities of construction in mind. They offer robust features for managing complex projects, including automated submittals, AI-powered drawing management, and integrated bid handling.\n",
      "3. **Procore stands out**: For its rich construction-focused toolkit, including automated submittals, AI-powered drawing management, and integrated bid handling.\n",
      "4. **Aconex excels in structured communication**: With features like transmittals, formal document control, making it a go-to choice for large-scale infrastructure projects.\n",
      "5. **PMWeb offers impressive breadth**: With strong cost control, scheduling, and portfolio-wide visibility, though it may come with a steeper learning curve.\n",
      "\n",
      "**Feature Matrix:**\n",
      "\n",
      "A feature matrix has been developed to quantify how well each platform supports a core set of functionalities:\n",
      "\n",
      "| Feature | Monday.com | Wrike | PMWeb | Aconex | Procore |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| Assigning roles | 10% | 85% | 80% | 80% | 80% |\n",
      "| Document approval workflow | 60% | 100% | 100% | 100% | 100% |\n",
      "| Versioning | 100% | 100% | 100% | 100% | 100% |\n",
      "| Markup versions | 50% | 100% | 0% | 90% | 100% |\n",
      "| Auto-changing status after submittals | 60% | 75% | 100% | 80% | 100% |\n",
      "| Financial oversight | 20% | 20% | 100% | 90% | 90% |\n",
      "| Bids management | 0% | 0% | 80% | 80% | 100% |\n",
      "| Document management system (DMS) | 0% | 0% | 80% | 100% | 80% |\n",
      "| Dashboards | 100% | 100% | 50% | 80% | 100% |\n",
      "| Reporting | 80% | 90% | 100% | 100% | 100% |\n",
      "| Notifications | 80% | 90% | 100% | 100% | 100% |\n",
      "| Project archiving | 30% | 100% | 100% | 100% | 70%' |\n",
      "\n",
      "This comparative analysis provides a comprehensive overview of the strengths and weaknesses of each platform, helping construction project managers make informed decisions when selecting a suitable tool for their projects.\n",
      "Time Taken to process:   55.25750184059143\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for doc in individual_documents:\n",
    "    summarization.run(doc)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b61ae329",
   "metadata": {},
   "outputs": [],
   "source": [
    "QuestionModule=QuestionStrategy(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ffb0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 6 questions based on the provided text:\n",
      "\n",
      "Q1: What are the key features evaluated in this market research report for document translation tools?\n",
      "\n",
      "Q2: Which tool offers free services with good layout preservation for English to Arabic translations and effective handling of text directionality?\n",
      "\n",
      "Q3: Does Doctranslate.io provide OCR functionality, and if so, what are its limitations?\n",
      "\n",
      "Q4: What is the main drawback of SmallPDF's translation support, according to the report?\n",
      "\n",
      "Q5: Which tool excels in English OCR performance but lacks Arabic OCR support?\n",
      "\n",
      "Q6: What is the recommended feature for enabling users to add text, shapes, images, and freehand annotations to PDFs?\n",
      "[]\n",
      "Here are 6 meaningful questions based on the provided text:\n",
      "\n",
      "Q1: Which project management platform offers AI-powered drawing splitting and indexing?\n",
      "Q2: What is the primary difference between Monday.com and Wrike, and how do they compare to PMWeb, Aconex, and Procore in terms of construction-specific workflows?\n",
      "Q3: How does Procore's Submittal Builder tool work, and what features does it provide for tracking and managing submittals?\n",
      "Q4: Which platform is specifically designed for the renewable energy sector and others that are solar PV-focused, but do not meet project management needs?\n",
      "Q5: What are some of the key differences between PMWeb, Aconex, and Procore in terms of their construction-specific toolkits and features?\n",
      "Q6: How does Monday.com's feature matrix help identify potential gaps and overlaps among different platforms?\n",
      "[]\n",
      "Time Taken to process:   31.559207916259766\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "for doc in individual_documents:\n",
    "    QuestionModule.run(doc)\n",
    "end=time.time()\n",
    "print(\"Time Taken to process:  \", end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
