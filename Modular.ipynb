{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92876b39",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a52a1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068f9",
   "metadata": {},
   "source": [
    "## Abstract classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b30a8",
   "metadata": {},
   "source": [
    "### Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7caedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=200,\n",
    "            chunk_overlap=50, \n",
    "            length_function=lambda x: len(x.split()),\n",
    "            separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "            keep_separator=False,\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'\\n{3,}', '\\n\\n', str(text))).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def chunk_documents(self, individual_documents):\n",
    "        chunked_docs = []\n",
    "        for doc in individual_documents:\n",
    "            chunks = self.text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"pdf_id\": doc.metadata[\"pdf_id\"],\n",
    "                            \"chunk_id\": i\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "        print(f\"✅ Total Chunks: {len(chunked_docs)}\")\n",
    "        return chunked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONPreprocessor(BasePreprocessor):\n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        clean_texts = [self.clean_text(entry) for entry in raw_data if isinstance(entry, str)]\n",
    "        return \"\\n\".join(clean_texts)\n",
    "    def process_documents_from_files(self, file_paths):\n",
    "        documents = []\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            text = self.load_and_preprocess_data(file_path).strip()\n",
    "            documents.append(\n",
    "                Document(page_content=text, metadata={\"pdf_id\": i})\n",
    "            )\n",
    "\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bf9b",
   "metadata": {},
   "source": [
    "### Embeddings Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80805382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name,model_kwargs={'device': self.device},encode_kwargs={'normalize_embeddings': True},multi_process=True,\n",
    "                                                     show_progress=True,cache_folder='./embedder_model_cache')\n",
    "\n",
    "    @abstractmethod\n",
    "    def embed_documents(self, documents):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_embed(self, texts, batch_size=None): \n",
    "        pass\n",
    "\n",
    "class MultilingualEmbedder(Embedder): \n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__(model_name, batch_size)\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        return self.batch_embed(documents, batch_size=self.batch_size)\n",
    "\n",
    "    def batch_embed(self, texts, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818acf",
   "metadata": {},
   "source": [
    "### Faiss Abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89195008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_faiss_index(self, chunks_embed):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def search_faiss(self, query_embedding, top_k=5):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def search_chunks(self, query_embedding, top_k=5):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def save_faiss_index(self, file_index_name):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def load_faiss_index(self, file_index_name):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase(ABC):\n",
    "    @abstractmethod\n",
    "    def create_vector_store(self, documents, embedder_model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_index(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_index(self, file_path):\n",
    "        pass\n",
    "\n",
    "class FAISS(VectorStoreBase):\n",
    "    def __init__(self, embedder_model=None):\n",
    "        self.index = None\n",
    "        self.chunks_dict = None\n",
    "        self.dimension = None\n",
    "        self.total_vectors = 0\n",
    "        self.index_type = \"IndexFlatIP\"\n",
    "        self.embedder_model = embedder_model\n",
    "    \n",
    "    def create_vector_store(self, documents, embedder_model=None):\n",
    "        \"\"\"Create vector store from documents\"\"\"\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model is required\")\n",
    "        \n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = self.embedder_model.batch_embed(texts)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        \n",
    "        # Ensure embeddings are 2D\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "        \n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store text chunks with their indices\n",
    "        self.chunks_dict = {i: text for i, text in enumerate(texts)}\n",
    "        self.total_vectors = self.index.ntotal\n",
    "        \n",
    "        print(f\"[FAISS] Created index with {self.total_vectors} vectors of dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query, top_k=5):\n",
    "        \"\"\"Main retriever function - returns LangChain Document objects\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not created. Call create_vector_store() first.\")\n",
    "        \n",
    "        if not self.embedder_model:\n",
    "            raise ValueError(\"Embedder model not set\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        if isinstance(query, str):\n",
    "            query_embedding = self.embedder_model.batch_embed([query])\n",
    "            if isinstance(query_embedding, list) and len(query_embedding) > 0:\n",
    "                query_embedding = query_embedding[0]\n",
    "            elif isinstance(query_embedding, np.ndarray) and query_embedding.ndim > 1:\n",
    "                query_embedding = query_embedding[0]\n",
    "        else:\n",
    "            query_embedding = self.embedder_model.batch_embed(query)\n",
    "        \n",
    "        # Search and format results\n",
    "        results = self._search_chunks(query_embedding, top_k)\n",
    "        \n",
    "        return [\n",
    "            Document(page_content=res['text'], metadata={\"similarity\": res['similarity']})\n",
    "            for res in results\n",
    "        ]\n",
    "    \n",
    "    def _search_chunks(self, query_embedding, top_k=5):\n",
    "        \"\"\"Internal search function - returns raw results\"\"\"\n",
    "        # Ensure query_embedding is properly shaped\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "        \n",
    "        # Handle different input shapes\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        elif query_embedding.ndim > 2:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        print(f\"[DEBUG] Query embedding final shape: {query_embedding.shape}\")\n",
    "        print(f\"[DEBUG] Index dimension: {self.dimension}\")\n",
    "        \n",
    "        # Verify dimensions match\n",
    "        if query_embedding.shape[1] != self.dimension:\n",
    "            raise ValueError(f\"Query embedding dimension {query_embedding.shape[1]} doesn't match index dimension {self.dimension}\")\n",
    "        \n",
    "        # Search FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted = []\n",
    "        for i in range(top_k):\n",
    "            faiss_idx = indices[0][i]\n",
    "            if faiss_idx != -1 and faiss_idx < len(self.chunks_dict):\n",
    "                distance = distances[0][i]\n",
    "                formatted.append({\n",
    "                    'chunk_id': faiss_idx,\n",
    "                    'text': self.chunks_dict[faiss_idx],\n",
    "                    'distance': distance,\n",
    "                    'similarity': float(distance)  # For cosine similarity, higher is better\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def search_raw(self, query_embedding, top_k=5):\n",
    "        \"\"\"Search with raw embedding input - useful for advanced use cases\"\"\"\n",
    "        return self._search_chunks(query_embedding, top_k)\n",
    "    \n",
    "    def save_index(self, file_path):\n",
    "        \"\"\"Save both FAISS index and metadata\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'chunks_dict': self.chunks_dict,\n",
    "            'dimension': self.dimension,\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'index_type': self.index_type\n",
    "        }\n",
    "        \n",
    "        with open(f\"{file_path}_metadata.pkl\", 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"[FAISS] Index and metadata saved to {file_path}\")\n",
    "    \n",
    "    def load_index(self, file_path, embedder_model=None):\n",
    "        \"\"\"Load both FAISS index and metadata\"\"\"\n",
    "        if not os.path.exists(f\"{file_path}.faiss\"):\n",
    "            raise FileNotFoundError(f\"Index file {file_path}.faiss not found\")\n",
    "        \n",
    "        if not os.path.exists(f\"{file_path}_metadata.pkl\"):\n",
    "            raise FileNotFoundError(f\"Metadata file {file_path}_metadata.pkl not found\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{file_path}.faiss\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{file_path}_metadata.pkl\", 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        self.chunks_dict = metadata['chunks_dict']\n",
    "        self.dimension = metadata['dimension']\n",
    "        self.total_vectors = metadata['total_vectors']\n",
    "        self.index_type = metadata['index_type']\n",
    "        \n",
    "        # Set embedder model if provided\n",
    "        if embedder_model:\n",
    "            self.embedder_model = embedder_model\n",
    "        \n",
    "        print(f\"[FAISS] Index loaded: {self.total_vectors} vectors, dim {self.dimension}\")\n",
    "        return self\n",
    "    \n",
    "    def set_embedder_model(self, embedder_model):\n",
    "        \"\"\"Set or update the embedder model\"\"\"\n",
    "        self.embedder_model = embedder_model\n",
    "        return self\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get index statistics\"\"\"\n",
    "        return {\n",
    "            'total_vectors': self.total_vectors,\n",
    "            'dimension': self.dimension,\n",
    "            'index_type': self.index_type,\n",
    "            'has_embedder': self.embedder_model is not None\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f5d36",
   "metadata": {},
   "source": [
    "### LLM Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLLM(ABC):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        self.model_name = model_name\n",
    "        self.cache_folder = cache_folder\n",
    "        self.device = (\n",
    "            'cuda' if torch.cuda.is_available()\n",
    "            else 'mps' if torch.backends.mps.is_available()\n",
    "            else 'cpu'\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OLLAMA_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n",
    "        return model\n",
    "\n",
    "\n",
    "class Hugging_Face_LLM(BaseLLM):\n",
    "    def __init__(self, model_name, cache_folder):\n",
    "        super().__init__(model_name, cache_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            cache_dir=self.cache_folder,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"  \n",
    "        )\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab7c4d",
   "metadata": {},
   "source": [
    "## Strategy Pattern Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cb45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self, input_text) :\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChattingStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        # Get actual LLM instance\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self._build_chain()\n",
    "\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"[Source {i} | PDF {doc.metadata.get('pdf_id', '?')}]: {doc.page_content}\"\n",
    "            for i, doc in enumerate(docs, 1)\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Please provide a comprehensive answer based on the context above. You MUST follow this exact format:\n",
    "\n",
    "            RESPONSE:\n",
    "            [Your main answer here]\n",
    "\n",
    "            REASONING:\n",
    "            [Explain your reasoning and how you used the context]\n",
    "\n",
    "            SOURCES:\n",
    "            [List the source numbers you referenced, for example: 1, 3, 5]\n",
    "            \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        def retrieve_context(inputs):\n",
    "            # Use the vector store directly \n",
    "            docs = self.vector_store.get_relevant_documents(inputs[\"question\"], top_k=self.top_k)\n",
    "            return self.format_docs(docs)\n",
    "\n",
    "        self.chain = ({\n",
    "                \"context\": RunnableLambda(retrieve_context), \n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def parse_structured_response(self, response_text):\n",
    "        cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL)\n",
    "        cleaned_response = re.sub(r'<[^>]+>', '', cleaned_response)\n",
    "        cleaned_response = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_response.strip())\n",
    "\n",
    "        sections = {'response': '', 'reasoning': '', 'sources': ''}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        lines = cleaned_response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith('RESPONSE:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'response'\n",
    "                current_content = [line[9:].strip()]\n",
    "            elif line.upper().startswith('REASONING:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'reasoning'\n",
    "                current_content = [line[10:].strip()]\n",
    "            elif line.upper().startswith('SOURCES:'):\n",
    "                if current_section:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                current_section = 'sources'\n",
    "                current_content = [line[8:].strip()]\n",
    "            elif current_section and line:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_section:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "\n",
    "        source_ids = [int(x) for x in re.findall(r'\\d+', sections['sources'])] if sections['sources'] else []\n",
    "\n",
    "        return {\n",
    "            'answer': sections['response'],\n",
    "            'reasoning': sections['reasoning'],\n",
    "            'sources': source_ids,\n",
    "            'raw_response': cleaned_response\n",
    "        }\n",
    "\n",
    "    def run(self, question):\n",
    "        \"\"\"Main method to run the chain and parse result.\"\"\"\n",
    "        \n",
    "        response = self.chain.invoke({\"question\": question})\n",
    "        print(\"Past chain call\")\n",
    "        print(f\"Raw LLM response: {response}\")  \n",
    "        \n",
    "        parsed = self.parse_structured_response(response)\n",
    "        print(\"Past parser call\")\n",
    "        print(f\"Parsed response: {parsed}\")  \n",
    "        \n",
    "        if not self.return_sources:\n",
    "            return parsed\n",
    "\n",
    "    \n",
    "        source_docs = self.vector_store.get_relevant_documents(question, top_k=self.top_k)\n",
    "        parsed['source_documents'] = source_docs\n",
    "        parsed['source_texts'] = [doc.page_content for doc in source_docs]\n",
    "        return parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummerizationStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self.prompt=rompt = ChatPromptTemplate.from_messages( [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])\n",
    "    \n",
    "    def run(self,document):\n",
    "        chain = create_stuff_documents_chain(self.llm, self.prompt)\n",
    "        result = chain.invoke({\"context\": [document]})\n",
    "        print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57ad9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionStrategy(TaskStrategy):\n",
    "    def __init__(self, llm, vector_store, embedder, top_k=5, return_sources=True):\n",
    "        self.llm = llm\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.set_embedder_model(embedder)\n",
    "        self.top_k = top_k\n",
    "        self.return_sources = return_sources\n",
    "        self.prompt= ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are a helpful assistant tasked with generating question-answer pairs for study purposes.\n",
    "\n",
    "                Text:\n",
    "                {context}\n",
    "\n",
    "                Generate 2-3 meaningful questions and their answers based only on the above text. \n",
    "\n",
    "                IMPORTANT: Format your output exactly as shown below with no additional text, explanations, or formatting:\n",
    "\n",
    "                Q1: [question text]\n",
    "                A1: [answer text]\n",
    "\n",
    "                Q2: [question text]\n",
    "                A2: [answer text]\n",
    "\n",
    "                Q3: [question text]\n",
    "                A3: [answer text]\n",
    "\n",
    "                Rules:\n",
    "                - Use only information from the provided text\n",
    "                - Do not invent facts or add external knowledge\n",
    "                - Each question should be on a single line\n",
    "                - Each answer should be on a single line\n",
    "                - Do not include any text before the first Q1 or after the last answer\n",
    "                - Do not use bullet points, numbering, or other formatting\n",
    "                - Always \"\"\")\n",
    "\n",
    "    def parse_qa_pairs(self,qa_output):\n",
    "        \"\"\"Parse the QA output into structured format\"\"\"\n",
    "        qa_pairs = []\n",
    "        \n",
    "        # Split by lines and process\n",
    "        lines = qa_output.strip().split('\\n')\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            \n",
    "            # Look for question pattern\n",
    "            q_match = re.match(r'Q(\\d+):\\s*(.+)', line)\n",
    "            if q_match and i + 1 < len(lines):\n",
    "                q_num = q_match.group(1)\n",
    "                question = q_match.group(2).strip()\n",
    "                \n",
    "                # Look for corresponding answer\n",
    "                next_line = lines[i + 1].strip()\n",
    "                a_match = re.match(f'A{q_num}:\\\\s*(.+)', next_line)\n",
    "                \n",
    "                if a_match:\n",
    "                    answer = a_match.group(1).strip()\n",
    "                    qa_pairs.append({\n",
    "                        'question': question,\n",
    "                        'answer': answer\n",
    "                    })\n",
    "                    i += 2  # Skip both Q and A lines\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return qa_pairs\n",
    "    def run(self,docs, batch_size=2):\n",
    "        \"\"\"Generate QA pairs from a list of Document chunks\"\"\"\n",
    "        qa_pairs = []\n",
    "        qa_chain = self.prompt | self.llm | StrOutputParser()\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                # Generate QA output\n",
    "                qa_output = qa_chain.invoke({\"context\": doc.page_content})\n",
    "                \n",
    "                # Parse the output into structured format\n",
    "                parsed_qa = self.parse_qa_pairs(qa_output)\n",
    "                \n",
    "                qa_pairs.append({\n",
    "                    \"pdf_id\": doc.metadata.get(\"pdf_id\"),\n",
    "                    \"chunk_id\": doc.metadata.get(\"chunk_id\"),\n",
    "                    \"text\": doc.page_content,\n",
    "                    \"qa_output\": qa_output,  # Keep raw output for debugging\n",
    "                    \"parsed_qa\": parsed_qa   # Add parsed structured data\n",
    "                })\n",
    "                \n",
    "                print(f\"✅ Generated {len(parsed_qa)} QA pairs for chunk {doc.metadata.get('chunk_id', 'unknown')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to generate QA for chunk {doc.metadata}: {e}\")\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f073",
   "metadata": {},
   "source": [
    "## Classes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "785f0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Chunks: 71\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "paths=[\"Market Research Report_extracted_text.json\", 'PMS Market Research_extracted_text.json']\n",
    "docs=JSONPreprocessor()\n",
    "data=docs.process_documents_from_files(paths)\n",
    "individual_documents = [ Document(page_content=pdf.page_content, metadata={\"pdf_id\": i})\n",
    "    for i, pdf in enumerate(data) if pdf.page_content\n",
    "]\n",
    "chunked_docs=docs.chunk_documents(individual_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14c74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_embedder=MultilingualEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb5d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5194a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/f9lh6rmd4q1648_pfl1636zw0000gn/T/ipykernel_11398/3805475503.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=self.model_name, temperature=0.3, num_ctx=4096)\n"
     ]
    }
   ],
   "source": [
    "llm=OLLAMA_LLM('qwen3:8b','llm_cache').load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS] Created index with 71 vectors of dim 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.FAISS at 0x38181ad90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_store = FAISS()\n",
    "faiss_store.set_embedder_model(multilingual_embedder)\n",
    "faiss_store.create_vector_store(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6210988",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = ChattingStrategy(llm, faiss_store, multilingual_embedder,top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a95652e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n",
      "Past chain call\n",
      "Raw LLM response: <think>\n",
      "Okay, the user is asking about the carbon footprint of solar panels. Let me check the provided context to see if there's any information related to that.\n",
      "\n",
      "Looking through the sources, Source 1 talks about renewable energy platforms and solar-focused systems but doesn't mention carbon footprints. Source 2 is about project management systems for PV projects, so no relevant info there. Source 3 seems to be about pricing for a service, not related. Source 4 discusses project management features for solar projects, like document control and collaboration, but again, no carbon footprint data. Source 5 mentions Oracle Aconex pricing and features, which also isn't related.\n",
      "\n",
      "None of the sources provide data on the carbon footprint of solar panels. The context is focused on project management tools and systems for solar projects, not environmental impact metrics. Therefore, I can't answer the question based on the given context. I need to inform the user that the information isn't available in the provided sources.\n",
      "</think>\n",
      "\n",
      "RESPONSE:\n",
      "The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.\n",
      "\n",
      "REASONING:\n",
      "The question about the carbon footprint of solar panels is unrelated to the context provided. The sources discuss technical tools for project management, document control, and system pricing, but none mention environmental metrics, lifecycle analysis, or carbon emissions associated with solar panels.\n",
      "\n",
      "SOURCES:\n",
      "1, 2, 3, 4, 5\n",
      "Past parser call\n",
      "Parsed response: {'answer': 'The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.', 'reasoning': 'The question about the carbon footprint of solar panels is unrelated to the context provided. The sources discuss technical tools for project management, document control, and system pricing, but none mention environmental metrics, lifecycle analysis, or carbon emissions associated with solar panels.', 'sources': [1, 2, 3, 4, 5], 'raw_response': 'RESPONSE:\\nThe provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.\\n\\nREASONING:\\nThe question about the carbon footprint of solar panels is unrelated to the context provided. The sources discuss technical tools for project management, document control, and system pricing, but none mention environmental metrics, lifecycle analysis, or carbon emissions associated with solar panels.\\n\\nSOURCES:\\n1, 2, 3, 4, 5'}\n",
      "[DEBUG] Query embedding final shape: (1, 384)\n",
      "[DEBUG] Index dimension: 384\n"
     ]
    }
   ],
   "source": [
    "response = strategy.run(\"What is the carbon footprint of solar panels?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcf52d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.',\n",
       " 'reasoning': 'The question about the carbon footprint of solar panels is unrelated to the context provided. The sources discuss technical tools for project management, document control, and system pricing, but none mention environmental metrics, lifecycle analysis, or carbon emissions associated with solar panels.',\n",
       " 'sources': [1, 2, 3, 4, 5],\n",
       " 'raw_response': 'RESPONSE:\\nThe provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.\\n\\nREASONING:\\nThe question about the carbon footprint of solar panels is unrelated to the context provided. The sources discuss technical tools for project management, document control, and system pricing, but none mention environmental metrics, lifecycle analysis, or carbon emissions associated with solar panels.\\n\\nSOURCES:\\n1, 2, 3, 4, 5',\n",
       " 'source_documents': [Document(metadata={'similarity': 0.35302403569221497}, page_content='Renewable and Solar -Focused Platforms During our research, we also explored platforms that are specifically designed for the renewable energy sector and others that are solar PV -focused . At first, they seemed promising because they’re built with energy systems in mind. However, after testing and reviewing them, we found that they don’t meet the type of project management needs we’re aiming for , although they market them selves as they have project management tools. Platforms like Ra Power Management (RaPM) , SenseHawk, and Payac a are examples of solar - focused systems. These tools are mainly designed for monitoring the performance of solar plants — such as tracking electricity production, system health, faults, and maintenance alerts. While they’re excellent for operations and post -installation monitoring , they do not support document approvals, workflows, submittals, or collaboration between stakeholde rs like contractors, consultants, and clients. In short, these are more like monitoring or asset management tools , not project management systems'),\n",
       "  Document(metadata={'similarity': 0.3210790753364563}, page_content='MARKET RESEARCH REPORT ON PROJECT MANAGEMENT SYSTEMS Benchmarking Tools for Document Control, Approvals, and Team Collaboration in PV proje cts. Mahi nour Mohammad Abstract This report explores existing project management systems to identify gaps and opportunities for developing a platform tailored to photovoltaic (PV) projects.'),\n",
       "  Document(metadata={'similarity': 0.24218502640724182}, page_content='Quota recharges $1.99 100000 chars . • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . $2.99 200000 chars . $7.99 500000 chars . $9.99 1 M chars . $37.99 5.5M chars . $279.99 50M chars .'),\n",
       "  Document(metadata={'similarity': 0.23647484183311462}, page_content='In too many solar and construction projects, teams juggle emails, spreadsheets, and generic cloud folders just to get a simple drawing reviewed. The result? Lost versions, unanswered questions, and schedule hiccups. This report cuts through the chaos by co mparing five best -in-class platforms —two generalists and three construction -focused —against the 12 must -have features for any professional solar project management syste m. Key Features We’re Tracking 1. Assigning Roles The ability to designate specific users or groups (e.g., “Project Manager,” “Reviewer,” “Electrician Team”) to tasks, submittals, or documents —so everyone knows who’s responsible for what. 2. Document Approval Workflow A structured process that routes documents or deliverables through one or more review and approval steps (with defined approvers, due -dates, and status transitions) before they’re considered “approved.” 3. Versioning Keeping track of each time a file is updated or replaced, so you can see prior iterations (e.g., “Drawing Rev A,” “Rev B”) and roll back if n eeded. 4. Markup Versions The ability to annotate or mark up each version of a document or drawing —circling areas, adding comments, highlighting changes —often directly on the file itself. 5'),\n",
       "  Document(metadata={'similarity': 0.2218591272830963}, page_content=\"Pricing: Just like PMWeb, Oracle Aconex does not publicly list pricing because it's offered as an enterprise solution , and pricing is tailored to the size, complexity, and duration of your project. Factors Affecting Aconex Pricing: • Project size and duration (short -term vs long -term) • Number of users and collaborators • Scope of modules (document control, BIM, cost management, workflow automation) • Cloud storage volume • Required integrations (e.g., ERP systems) • Training, implementation, and support level Estimated Pricing Range (from user reports and RFPs): • Per Project License (Annual): Starting around $15,000 –$30,000+ • Enterprise Multi -Project License: Can go $100,000+/year • Per User (if applied): ~$100 –$150/user/month (not always the model used) Pros: 1- Centralized Document Management 2- Seamless Collaboration Across Organizations 3- Customizable Workflows 4- On-Site Issue Management\")],\n",
       " 'source_texts': ['Renewable and Solar -Focused Platforms During our research, we also explored platforms that are specifically designed for the renewable energy sector and others that are solar PV -focused . At first, they seemed promising because they’re built with energy systems in mind. However, after testing and reviewing them, we found that they don’t meet the type of project management needs we’re aiming for , although they market them selves as they have project management tools. Platforms like Ra Power Management (RaPM) , SenseHawk, and Payac a are examples of solar - focused systems. These tools are mainly designed for monitoring the performance of solar plants — such as tracking electricity production, system health, faults, and maintenance alerts. While they’re excellent for operations and post -installation monitoring , they do not support document approvals, workflows, submittals, or collaboration between stakeholde rs like contractors, consultants, and clients. In short, these are more like monitoring or asset management tools , not project management systems',\n",
       "  'MARKET RESEARCH REPORT ON PROJECT MANAGEMENT SYSTEMS Benchmarking Tools for Document Control, Approvals, and Team Collaboration in PV proje cts. Mahi nour Mohammad Abstract This report explores existing project management systems to identify gaps and opportunities for developing a platform tailored to photovoltaic (PV) projects.',\n",
       "  'Quota recharges $1.99 100000 chars . • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . $2.99 200000 chars . $7.99 500000 chars . $9.99 1 M chars . $37.99 5.5M chars . $279.99 50M chars .',\n",
       "  'In too many solar and construction projects, teams juggle emails, spreadsheets, and generic cloud folders just to get a simple drawing reviewed. The result? Lost versions, unanswered questions, and schedule hiccups. This report cuts through the chaos by co mparing five best -in-class platforms —two generalists and three construction -focused —against the 12 must -have features for any professional solar project management syste m. Key Features We’re Tracking 1. Assigning Roles The ability to designate specific users or groups (e.g., “Project Manager,” “Reviewer,” “Electrician Team”) to tasks, submittals, or documents —so everyone knows who’s responsible for what. 2. Document Approval Workflow A structured process that routes documents or deliverables through one or more review and approval steps (with defined approvers, due -dates, and status transitions) before they’re considered “approved.” 3. Versioning Keeping track of each time a file is updated or replaced, so you can see prior iterations (e.g., “Drawing Rev A,” “Rev B”) and roll back if n eeded. 4. Markup Versions The ability to annotate or mark up each version of a document or drawing —circling areas, adding comments, highlighting changes —often directly on the file itself. 5',\n",
       "  \"Pricing: Just like PMWeb, Oracle Aconex does not publicly list pricing because it's offered as an enterprise solution , and pricing is tailored to the size, complexity, and duration of your project. Factors Affecting Aconex Pricing: • Project size and duration (short -term vs long -term) • Number of users and collaborators • Scope of modules (document control, BIM, cost management, workflow automation) • Cloud storage volume • Required integrations (e.g., ERP systems) • Training, implementation, and support level Estimated Pricing Range (from user reports and RFPs): • Per Project License (Annual): Starting around $15,000 –$30,000+ • Enterprise Multi -Project License: Can go $100,000+/year • Per User (if applied): ~$100 –$150/user/month (not always the model used) Pros: 1- Centralized Document Management 2- Seamless Collaboration Across Organizations 3- Customizable Workflows 4- On-Site Issue Management\"]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe4fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain any information about the carbon footprint of solar panels. The sources focus on project management systems, monitoring tools, and pricing details for renewable energy and solar PV projects, but none address environmental impact metrics such as carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb48ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summerization=SummerizationStrategy(llm, faiss_store, multilingual_embedder,top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff954633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to create a concise summary of this market research report on document translation tools. Let me start by reading through the entire text again to make sure I understand the key points.\n",
      "\n",
      "The report evaluates several tools: Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL. They tested these tools for handling Arabic, French, and English, focusing on layout preservation, OCR support, translation accuracy, and pricing. The test cases included text-based documents, scanned documents, tables (text and image), and documents with stamps/signatures.\n",
      "\n",
      "The key findings mention that no single tool meets all requirements. Doctranslator is good for English to Arabic but lacks OCR. Doctranslate.io has OCR but poor layout and handles Arabic numerals poorly. TranslaDocs doesn't support OCR or Arabic. SmallPDF has limited translation. Doclingo does OCR well for Arabic to English but struggles with image tables. DeepL is great for English OCR but no Arabic support.\n",
      "\n",
      "The report also suggests features to improve tools, like editing capabilities, conversion features, image translation, selective OCR, AI-powered features, process modes, and a split-view interface. Pricing plans for the tools vary, with some offering free tiers and others paid subscriptions with different credit limits and costs.\n",
      "\n",
      "I need to condense all this into a concise summary. Start with the purpose of the report, then mention the tools evaluated, key features tested, main findings, and the recommended features. Also, include the pricing models and the conclusion that no tool is perfect, but there are opportunities for improvement.\n",
      "\n",
      "Wait, the user wants a concise summary, so I should avoid too much detail. Highlight the main points: evaluation of tools, their strengths and weaknesses, the recommended features, and the pricing. Make sure to mention the languages (Arabic, French, English) and the file types (PDF, Word, etc.). Also, note that the tools have varying support for OCR and layout preservation, and the need for enhanced features like AI and editing tools. The summary should be clear and to the point, capturing the essence without getting bogged down in specifics.\n",
      "</think>\n",
      "\n",
      "**Summary of Market Research Report: Document Translation Tools Analysis**  \n",
      "\n",
      "This report evaluates six document translation tools (Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL) for their ability to handle multilingual (Arabic, French, English) and multi-format (PDF, Word, Excel, scanned images) translations while preserving layout, formatting, and OCR accuracy. Key criteria included layout preservation, Arabic support, translation speed/accuracy, OCR performance, and pricing.  \n",
      "\n",
      "**Key Findings:**  \n",
      "- **Doctranslator**: Free, good for English→Arabic layout but lacks OCR and struggles with mixed languages.  \n",
      "- **Doctranslate.io**: Offers OCR but has poor layout preservation, slow processing, and fails with Arabic numerals.  \n",
      "- **TranslaDocs/SmallPDF**: No OCR or Arabic support, limiting utility.  \n",
      "- **Doclingo**: Strong OCR for Arabic→English but struggles with complex tables and RTL preservation.  \n",
      "- **DeepL**: Excellent for English OCR but lacks Arabic support.  \n",
      "\n",
      "**Recommended Enhancements:**  \n",
      "- **Editing Features**: PDF annotation, splitting/merging, and page deletion.  \n",
      "- **OCR Flexibility**: Selective OCR activation for text vs. image-based content.  \n",
      "- **AI Integration**: Chat with PDF, summarization, tone customization, and domain-specific translations.  \n",
      "- **Conversion Tools**: Bi-directional format conversion (Word/PDF/Excel).  \n",
      "- **Split-View Interface**: Real-time comparison of original and translated documents.  \n",
      "\n",
      "**Pricing Insights:**  \n",
      "- Free tiers exist (e.g., Doctranslator), while paid plans range from $0.01/char to $57.49/month for advanced features.  \n",
      "- Tools like Doclingo and DeepL offer tiered subscriptions with varying credit limits and file size allowances.  \n",
      "\n",
      "**Conclusion:** No tool fully meets all requirements for accurate, efficient, and cost-effective multilingual translation. Enhancing OCR flexibility, AI capabilities, and user-editing tools could improve market competitiveness.\n",
      "<think>\n",
      "Okay, I need to help the user by analyzing the provided text about project management platforms. The user has given a detailed comparison of Monday.com, Wrike, PMWeb, Aconex, and Procore, along with a feature matrix. The goal is to synthesize this information and provide a structured summary.\n",
      "\n",
      "First, I'll start by understanding the key points from the text. The user is comparing different project management tools, focusing on their suitability for construction and engineering projects. The platforms are evaluated based on various features like role assignment, document approval workflows, versioning, markup tools, financial oversight, etc. There's also a mention of renewable energy-focused platforms that aren't suitable for the user's needs.\n",
      "\n",
      "The feature matrix provided gives a numerical score (from 0% to 100%) for each platform on different functionalities. The user might want a summary that highlights the strengths and weaknesses of each platform, making it easier to choose the right tool based on their specific needs.\n",
      "\n",
      "I should start by outlining each platform's strengths. For example, Procore is highlighted for its construction-specific tools like automated submittals and OCR-based document processing. Aconex is noted for structured communication and document control. PMWeb has strong cost control and scheduling. Monday.com and Wrike are more general but lack some construction-specific features.\n",
      "\n",
      "Next, I need to address the renewable energy platforms mentioned, explaining why they aren't suitable despite their monitoring capabilities. Then, the feature matrix should be summarized, perhaps in a table format for clarity, showing which platforms excel in which areas.\n",
      "\n",
      "I should also mention the trade-offs, like the learning curve for PMWeb or the steep learning curve for Procore. The conclusion should tie everything together, emphasizing the best fit for different project management needs within the construction industry.\n",
      "\n",
      "I need to make sure the summary is clear, concise, and highlights the key differences between the platforms. Avoid technical jargon where possible, and structure the information in a way that's easy to follow. Also, check if there are any inconsistencies in the data provided, like the 80% for PMWeb's bids management versus the 100% in the matrix. Wait, in the text, PMWeb's bids management is mentioned as 80%, but in the matrix, it's 80% for PMWeb and 80%+100% for Procore. Maybe there's a typo or inconsistency here. I should note that in the summary to clarify.\n",
      "\n",
      "Finally, ensure that the summary addresses the user's potential needs, such as whether they prioritize document control, submittal tracking, or financial oversight, and recommend the appropriate platform based on those priorities.\n",
      "</think>\n",
      "\n",
      "### Synthesis of Project Management Platforms for Construction & Engineering Projects\n",
      "\n",
      "---\n",
      "\n",
      "#### **Key Platforms Overview**\n",
      "1. **Monday.com**  \n",
      "   - **Strengths**: Intuitive interface, flexible dashboards, and task tracking.  \n",
      "   - **Weaknesses**: Lacks advanced construction-specific workflows (e.g., submittal tracking, OCR-based document processing).  \n",
      "   - **Best For**: General project management with basic task tracking, but not ideal for complex construction workflows.  \n",
      "\n",
      "2. **Wrike**  \n",
      "   - **Strengths**: Excellent task tracking, collaboration tools, and customizable workflows.  \n",
      "   - **Weaknesses**: Limited support for construction-specific features like formal transmittals or submittal coordination.  \n",
      "   - **Best For**: Teams needing robust task management but lacking deep construction-specific functionality.  \n",
      "\n",
      "3. **PMWeb**  \n",
      "   - **Strengths**: Strong cost control, scheduling, and portfolio-wide visibility. Offers comprehensive project management tools.  \n",
      "   - **Weaknesses**: Steeper learning curve and limited support for submittal workflows (0% in the matrix).  \n",
      "   - **Best For**: Projects requiring detailed financial oversight and scheduling, though it may require additional integration for submittal tracking.  \n",
      "\n",
      "4. **Aconex**  \n",
      "   - **Strengths**: Structured communication, formal document control, and transmittal management.  \n",
      "   - **Weaknesses**: Less focus on construction-specific tools like OCR or submittal automation.  \n",
      "   - **Best For**: Large-scale infrastructure projects emphasizing formal document control and collaboration.  \n",
      "\n",
      "5. **Procore**  \n",
      "   - **Strengths**: Construction-focused toolkit (e.g., automated submittals, AI-powered drawing management, OCR-based document processing).  \n",
      "   - **Weaknesses**: Steep learning curve and limited customization compared to other platforms.  \n",
      "   - **Best For**: Complex construction projects requiring end-to-end submittal tracking, drawing management, and bid handling.  \n",
      "\n",
      "---\n",
      "\n",
      "#### **Renewable Energy Platforms (Not Suitable for Construction)**\n",
      "- **Examples**: Ra Power Management (RaPM), SenseHawk, Payaca.  \n",
      "- **Limitations**:  \n",
      "  - Focus on monitoring solar plant performance (e.g., electricity production, maintenance alerts).  \n",
      "  - Lack critical construction-specific features: workflow approvals, version control, submittal coordination, and role-based task assignment.  \n",
      "  - Not designed for project management but rather for operational monitoring.  \n",
      "\n",
      "---\n",
      "\n",
      "#### **Feature Matrix Summary**\n",
      "| Feature | Monday.com | Wrike | PMWeb | Aconex | Procore |\n",
      "|--------|------------|-------|-------|--------|---------|\n",
      "| **Assigning roles** | 10% | 85% | 80% | 80% | 80% |\n",
      "| **Document approval workflow** | 60% | 100% | 100% | 100% | 100% |\n",
      "| **Versioning** | 100% | 100% | 100% | 100% | 100% |\n",
      "| **Markup versions** | 50% | 100% | 0% | 90% | 100% |\n",
      "| **Auto-changing status after submittals** | 60% | 75% | 100% | 80% | 100% |\n",
      "| **Financial oversight** | 20% | 20% | 100% | 90% | 90% |\n",
      "| **Bids management** | 0% | 0% | 80% | 80% | 100% |\n",
      "| **Document management system (DMS)** | 0% | 0% | 80% | 100% | 80% |\n",
      "| **Dashboards** | 100% | 100% | 50% | 80% | 100% |\n",
      "| **Reporting** | 80% | 90% | 100% | 100% | 100% |\n",
      "| **Notifications** | 80% | 90% | 100% | 100% | 100% |\n",
      "| **Project archiving** | 30% | 100% | 100% | 100% | 70% |\n",
      "\n",
      "**Key Takeaways from the Matrix**:  \n",
      "- **Procore** excels in **construction-specific workflows** (e.g., submittals, OCR, bid management) and **document control**.  \n",
      "- **Aconex** leads in **formal document control** and **structured communication**.  \n",
      "- **PMWeb** is strong in **financial oversight** and **scheduling** but lacks submittal tracking.  \n",
      "- **Wrike** and **Monday.com** are general-purpose tools with limited construction-specific features.  \n",
      "\n",
      "---\n",
      "\n",
      "#### **Recommendations Based on Priorities**\n",
      "- **For Submittal Tracking & Drawing Management**: **Procore** (AI-powered OCR, automated workflows).  \n",
      "- **For Formal Document Control & Collaboration**: **Aconex** (structured transmittals, DMS).  \n",
      "- **For Financial Oversight & Scheduling**: **PMWeb** (cost control, portfolio visibility).  \n",
      "- **For General Task Management**: **Wrike** or **Monday.com** (flexible dashboards, task tracking).  \n",
      "\n",
      "---\n",
      "\n",
      "#### **Conclusion**\n",
      "The construction industry requires platforms tailored to its unique workflows, such as submittal tracking, document control, and OCR-based processing. While **Procore** and **Aconex** are the most specialized for these needs, **PMWeb** offers strong financial and scheduling tools. General-purpose tools like **Wrike** and **Monday.com** are better suited for teams with less complex requirements. Renewable energy platforms, though useful for monitoring, lack the depth needed for construction project management. Choosing the right tool depends on balancing specific project needs with ease of use and integration capabilities.\n"
     ]
    }
   ],
   "source": [
    "for doc in individual_documents:\n",
    "    summerization.run(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b61ae329",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question=QuestionStrategy(llm, faiss_store, multilingual_embedder,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffb0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 3 QA pairs for chunk unknown\n",
      "✅ Generated 3 QA pairs for chunk unknown\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pdf_id': 0,\n",
       "  'chunk_id': None,\n",
       "  'text': \"MARKET RESEARCH REPORT: ANALYSIS OF DOCUMENT TRANSLATION TOOLS Evaluating Leading Solutions for Multilingual Document Translation Mah inour Mohammad\\nIntroduction This market research report analyzes competitors offering document translation tools that support PDF, Word, Excel, and scanned images while preserving layout and formatting. The focus is on tools that handle Arabic, French, and English languages, catering to both B2B and B2C markets. The key features evaluated include layout preservation, Arabic support and quality, translation accuracy and speed, pricing model, and Optical Character Recognition (OCR) support. To assess these tools, a series of test cases were conducted for each language, including: 1. Text -based documents: Evaluating basic translation accuracy, layout preservation, handling of number lists, bullet points, and right -to-left (RTL) and left -to-right (LTR) conversions. 2. Scanned documents: Testing OCR performance, particularly for Arabic, and preservation of bullet points, number lists, and order. 3. Tables (as text): Checking if tables remain aligned and readable, with attention to merged cells and number formatting. 4. Tables (as images): Evaluating the same as above with the addition of OCR. 5. Scanned documents with stamps or signatures: Testing performance with logos and non - translatable elements. The tools tested are: Doctranslator , Doctranslate.io , TranslaDocs , SmallPDF , Doclingo , and DeepL . Files used for testing : 1- Translating Test Arabic 2- Translating Test French 3- Translating Test En glish Methodology The evaluatio n involved subjecting each tool to the aforementioned test cases across Arabic, French, and English. Performance was analyzed based on the key features critical to the target audience's needs, ensuring a comprehensive assessment of each tool's capabilities .\\nExecutive Summary This section provides a high -level overview of the findings from the market research. Key Findings • Doctranslator : Offers free services with good layout preservation for English to Arabic transla tions and effective handling of text directionality. However, it lacks OCR capabilities and struggles with mixed language content. • Doctranslate.io : Provides OCR functionality but suffers from slow processing time s and poor layout preservation. It also fails to translate Arabic numerals and mismanages mixed text directions. • TranslaDocs : Does not support OCR or Arabic language, limiting its applicability. • SmallPDF : Offers limited translation support and no OCR capabilities. • Doclingo : Supports OCR with good performance for Arabic to English scanned documents but struggles with complex image tables and RTL preservation in English to Arabic translations. • DeepL : Excellent for English OCR but does not s upport Arabic, making it unsuitable for this use case. Features to be added: The analysis reveals that no single tool fully meets all requirements for accurate, efficient, and cost - effective document translation across Arabic, French, and English, particularly for preserving layout and handling OCR for scanned documents. Based on the evaluation, several opportunities exist to enhance document translation tools to better serve B2B and B2C markets. The following recommended features aim to address ga ps identified in the tested tools and improve functionality, user experience, and translation quality: • Editing Capabilities 1. PDF Editing: Enable users to add text, shapes, images, and freehand annotations to PDFs, facilitating document customization and correction post -translation. 2. PDF Annotations: Provide tools to write, draw, and highlight directly on PDFs, enhancing collaboration and docu ment review processes. 3. Split PDF: Allow users to split a PDF into multiple PDFs, with each page saved as a separate file. Include advanced options, such as specifying page ranges or programmatic splitting (similar to Python's split() function), to offer fl exibility.\\n4. Merge PDF: Enable merging multiple PDFs into a single document, streamlining document consolidation. 5. Delete Pages: Provide the ability to remove specific pages from a PDF, improving document management efficiency. • Conversion Features Support bi directional conversion between Word, PowerPoint, Excel, and PDF formats, ensuring seamless transitions between document types while preserving formatting and content integrity. • Image Translation Allow users to upload standalone images for translation, cat ering to scenarios where documents are provided as image files rather than PDFs or text -based formats. • Selective OCR Activation Offer an option to enable or disable OCR within uploaded documents, allowing users to choose whether to translate only text -based content or include image -based text, optimizing processing speed and accuracy. • AI-Powered Features 1. Chat with PDF: Implement an interactive feature where users can query PDF content using a chosen Large Language Model (LLM). Offer a selection of LLMs ca tegorized by subscription plans to accommodate varying user needs and budgets. 2. AI PDF Summarization: Provide automated summarization of PDF content, enabling users to quickly grasp key points without reading entire documents. 3. AI Question Generator: Generat e relevant questions based on PDF content, supporting educational, training, or analytical use cases. 4. AI Instructions: Allow users to input custom prompts or guidelines for the AI engine to follow during translation, ensuring translations align with specif ic requirements or preferences. 5. Domain -Specific Translation: Incorporate smart detection or a predefined list of domains (e.g., banking, accounting, law, physics) to enhance translation accuracy for specialized documents. 6. Tone Customization: Enable smart d etection or manual selection of tone (e.g., formal, friendly) to tailor translations to the intended audience or context.\\n• Process Modes 1. Professional Translation: The AI automatically selects the optimal style and format for the translation based on the do cument’s context, audience, and purpose. For example, legal documents would adopt a formal tone with precise terminology, while marketing materials might use a persuasive, audience -friendly style, ensuring contextually appropriate and high - quality translat ions. 2. Paraphrase: Rephrase text or specific sections of a document while preserving the original meaning. This mode is ideal for simplifying complex text, adapting content for different audiences, or avoiding repetitive phrasing, such as rephrasing a techn ical manual for non - expert readers . • Split -View Translation Interface Provide a split -screen interface displaying the original document on the left and the translated document on the right, with synchronized page -by-page scrolling similar to a PDF viewer. This feature enables users to compare, review, and edit translations in real time, enhancing accuracy and usability for both professional and casual users. These recommended features address critical gaps in current tools, such as limited editing capabilit ies, lack of flexible OCR options, insufficient support for domain -specific or tone - adjusted translations, absence of advanced translation modes, and the need for intuitive review interfaces. Implementing these enhancements could significantly improve the functionality and market competitiveness of document translation tools.\\nResults Comparison Table Tool Layout Preservation Arabic Support OCR Support Pricing Notes Doctranslator Good (Eng→Ar) Yes No Free Issues with mixed languages, table layout breaks Ar→Eng Doctranslate.i o Poor Yes Yes (slow) $0.01 –$0.02/1k chars, $0.10/image Does not translate Arabic numerals TranslaDocs Good No No Free Limited information SmallPDF No Limited No Free Limited translation support Doclingo Good Yes Yes $2/100k chars, $280/50M chars Fails with complex image tables, no RTL preservation Eng→Ar DeepL Good in English Poor in Arabic yes Yes (English only ) Starts at $17.5/month for 10M chars Excellent for English, no Arabic support\\nPricing Plans for Paid Tool s Tool Plan Name Cost Key Features Benefits Notes Doctranslate.io Topup -50 $4.99 50 translation credit • 10 cent/ page . • Credit expiration : permanent . • Perfect for quick, one -time needs. 1image/PDF page (A page is estimated to have 200 words): Costs 02 credits Topup -120 $9.99 120 translation credits Topup -260 $19.99 260 translation credits Topup -750 $49.99 750 translation credits Personal Subscription $199 4000 credits/year • ~6 cent/ page . • Credit expiration : 1 year . • Best usage for frequent, individual use. Team Subscription $999 25000 credits/ye ar • 4 cent/ page . • Credit expiration : 1 year / 3month . • Best value for high -volume translations. $99 2000 credits/ 3 month DeepL Pro Starter ~$8.74/user/month (Paid annually ) Up to 5 users • 5 files/month . • 5 editable file translations per user/month in tota l. • Upload files up to 10 MB • Tone /informal tone is available . Pro Advanced $28.74/user/ month (Paid annually ) For individuals & teams • 20 files/month . • 5 editable file translations per user/month in tota l. • Upload files up to 20 MB . • Tone /informal tone is available . Pro Ultimate $57.49 /user/ month (Paid annually ) For individuals & teams • 20 files/month . • 5 editable file translations per user/month in tota l. • Upload files up to 20 MB . • Tone /informal tone is available . Doclingo Premium 7-Day $2.29/7 -Day • 250,000 chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . Premium + 7- Day $2.99/7-Day • 500,000 chars. Monthly limit • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . Premium 30- Day $7.59/M • 1M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . Premium + 30- Day $9.99/M • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . Premium 365- Day $49.99 /Y • 1M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude .\\nDetailed Findings Doctranslator • Pros: o Free of charge . o Preserves layout effectively for English to Arabic translations, particularly tables . o Handles RTL/LTR directions well . • Cons: o No OCR support . o Poor handling of mixed Arabic/English lines . o Table layout breaks when translating from Arabic to English . Translated files can be found here: DocTrasnlator Doctranslate.io • Pros: o Offers OCR capabilities (slow and image -based) . • Cons: o Poor layout preservation, especially for tables . o Does not translate Arabic n umerals . o Inadequate handling of RTL/LTR mix . o Processing time: 1 –3 minutes . Translated files can be found here: doctransla.io Premium + 30- Day $88.9/M • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . Quota recharges $1.99 100000 chars . • 2M chars. Monthly limit. • No daily limit . • No number of tra ns. Limit. • File size: 500 MB . • Trans. Engine : ChatGPT/Gemini /Deepseek/Claude . $2.99 200000 chars . $7.99 500000 chars . $9.99 1 M chars . $37.99 5.5M chars . $279.99 50M chars .\\nTranslaDocs • Cons: o No OCR support . o No Arabic language support . SmallPDF • Cons: o No OCR support . Doclingo • Pros: o Supports OCR . o Good performance for Arabic to English scanned documents . • Cons: o Fails with complex image tables . o Does not preserve RTL when translating from English to Arabic . DeepL • Pros: o Excellent performance for English OCR . • Cons: o Does not support Arabic OCR. Translated documents can be found here : DeepL Summary of Findings: The tools demonstrated varied performance across key criteria. Some excelled in specific areas, such as translation accuracy or affordability, while others lagged in advanced features like robust language support or seamless integration with existing workf lows. Pricing models also ranged widely, catering to different user segments from cost -conscious individuals to enterprises requiring premium functionalities.\",\n",
       "  'qa_output': \"<think>\\nOkay, let's tackle this query. The user wants me to generate 2-3 meaningful question-answer pairs based on the provided market research report about document translation tools. The key points are to use only the text given, avoid any external knowledge, and format exactly as specified.\\n\\nFirst, I need to understand the text thoroughly. The report evaluates several tools: Doctranslator, Doctranslate.io, TranslaDocs, SmallPDF, Doclingo, and DeepL. Each has different features, pricing, and performance in areas like layout preservation, OCR support, and language handling. The executive summary and detailed findings highlight their pros and cons.\\n\\nFor the questions, I should focus on the main findings and features. Let me think of possible questions. \\n\\nOne could be about which tool excels in English OCR but lacks Arabic support. DeepL is mentioned as excellent for English OCR but doesn't support Arabic. That's a clear point.\\n\\nAnother question might be about the pricing model of Doctranslate.io. The text lists different plans with costs and features, so asking about their pricing plans would be relevant.\\n\\nA third question could relate to the recommended features for improving tools, like AI-powered features. The report suggests adding things like Chat with PDF and AI summarization, so that's a good area to ask.\\n\\nI need to ensure each question is on a single line, answers are concise, and no markdown is used. Also, check that all info is from the text and no invented facts. Let me verify each point again to make sure.\\n</think>\\n\\nQ1: Which tool excels in English OCR but lacks Arabic support?  \\nA1: DeepL excels in English OCR but does not support Arabic.  \\n\\nQ2: What is the pricing model for Doctranslate.io's Team Subscription plan?  \\nA2: The Team Subscription plan costs $999 per year, offering 25,000 credits/year with a 4 cent/page rate and a 1-year credit expiration.  \\n\\nQ3: What feature is recommended to enhance document translation tools for better user experience?  \\nA3: Adding AI-powered features like Chat with PDF, AI PDF Summarization, and domain-specific translation is recommended to improve functionality and user experience.\",\n",
       "  'parsed_qa': [{'question': 'Which tool excels in English OCR but lacks Arabic support?',\n",
       "    'answer': 'DeepL excels in English OCR but does not support Arabic.'},\n",
       "   {'question': \"What is the pricing model for Doctranslate.io's Team Subscription plan?\",\n",
       "    'answer': 'The Team Subscription plan costs $999 per year, offering 25,000 credits/year with a 4 cent/page rate and a 1-year credit expiration.'},\n",
       "   {'question': 'What feature is recommended to enhance document translation tools for better user experience?',\n",
       "    'answer': 'Adding AI-powered features like Chat with PDF, AI PDF Summarization, and domain-specific translation is recommended to improve functionality and user experience.'}]},\n",
       " {'pdf_id': 1,\n",
       "  'chunk_id': None,\n",
       "  'text': 'MARKET RESEARCH REPORT ON PROJECT MANAGEMENT SYSTEMS Benchmarking Tools for Document Control, Approvals, and Team Collaboration in PV proje cts. Mahi nour Mohammad Abstract This report explores existing project management systems to identify gaps and opportunities for developing a platform tailored to photovoltaic (PV) projects.\\nIn too many solar and construction projects, teams juggle emails, spreadsheets, and generic cloud folders just to get a simple drawing reviewed. The result? Lost versions, unanswered questions, and schedule hiccups. This report cuts through the chaos by co mparing five best -in-class platforms —two generalists and three construction -focused —against the 12 must -have features for any professional solar project management syste m. Key Features We’re Tracking 1. Assigning Roles The ability to designate specific users or groups (e.g., “Project Manager,” “Reviewer,” “Electrician Team”) to tasks, submittals, or documents —so everyone knows who’s responsible for what. 2. Document Approval Workflow A structured process that routes documents or deliverables through one or more review and approval steps (with defined approvers, due -dates, and status transitions) before they’re considered “approved.” 3. Versioning Keeping track of each time a file is updated or replaced, so you can see prior iterations (e.g., “Drawing Rev A,” “Rev B”) and roll back if n eeded. 4. Markup Versions The ability to annotate or mark up each version of a document or drawing —circling areas, adding comments, highlighting changes —often directly on the file itself. 5. Auto -Changing Status After Submittals Automatically updating an i tem’s status (e.g., from “Draft” to “Submitted,” or “Submitted” to “Review in Progress”) once a submittal is sent, a reviewer takes action, or another trigger occurs. 6. Financial Oversight Tools that let you monitor budget vs. actuals, forecast costs, tra ck commitments (contracts, POs, change orders), and generate financial reports —all in one place. 7. Bids Management Capabilities to create bid packages, invite prequalified subcontractors, collect their proposals, compare line -item pricing side -by-side, and track who’s submitted or declined. 8. Document Manageme nt System (DMS) A central repository for all project documents with robust organization (folders/tags), access controls, full -text search, and lifecycle management (upload → approval → archive). 9. Dashboards Visual, at -a-glance summaries (charts, gauges, scorecards) that present key project data—progress, costs, outstanding approvals, schedule milestones —to help stakeholders quickly understand project health.\\n10. Reporting The ability to generate custom or out -of-the-box reports (tabular or graphical) on any aspect of the project —submittal s tatus, budget forecast vs. actual, RFI turnaround times, document register summaries, etc. 11. Notifications Automated alerts —via email, in -app pop -ups, or mobile push —that let users know when something needs their attention (new assignment, status change, overdue task). 12. Project Archiving A method to archive completed projects (or individual items) so that active workspaces stay clean, while all files, workflows, and histories are preserved in a read -only state for future reference . Platforms to be explored: 1- Monday.com 2- Wrike 3- PMweb 4- Aconex 5- Procore 1- Monday.com Monday.com is a chameleon -like work OS that adapts to nearly any process you throw at it . Structurally, you organize your account into five levels: Workspaces (typically your company or department), Boards (each a discrete project), Groups (milestones or phases), Items (tasks or deliverables), and Subitems (granular to -dos under each task). O nce your board exists, you pick from dozens of column types —status, date, numbers, files, and even custom formulas —to tailor exactly what data you capture. The magic is in Automations : think of them as “if this happens, do that” rules. Want to ping a revie wer when a status flips to “Submitted”? No problem. Need to archive a task when it’s marked “Done”? One click. Columns act like variables, automations like code blocks, and you string them together to model anything from basic reminders to complex routing. Structure an d Main Page: Once your board exists, you pick from dozens of column types —status, date, numbers, files, and even custom formulas —to tailor exactly what data you capture. You can also rename or duplicate them , as shown in the following figures.\\nMost of the columns in Monday.com just provide info to the viewer, in other words they do not have any action to do with unless you create it using automations. Automatio ns in Monday.com & How it is used Monday.com’s automations are very powerful —they let you trigger almost any action when something happens (most commonly, a status or value change). It acts exactly like an “if” statement , with no “else”. And columns are like variables in it. They can also be saved to use later. This will get clearer in a minute but let’s first define how Monday.com manages users assigned to an item (task). Role Assignment A person column in Monday.com only assigns users (previously ad ded to the project) to the task, by adding a person to the list (more than one is allowed), it notifies him that he had been assigned to <item (task) name>. So, a people column will not define whether it is an assignee or a reviewer. But how do we notify a reviewer to review a submitted document? Using Automations. A person column is created, renamed to <Reviewers>, add users to review, then create an automation. This works with the help of the status column, where you can add as many statuses as you want. They can only change manually. The following GIF shows an example of an automation to notify a reviewer. Here, the assignee is expected to change the status manually after “submitting” the document (we’ll get back to what that exact ly means later).\\nFeed -Style Update s Monday.com has a Facebook -like \"Updates\" page, where team members can see all activity on the boards they’re added to. The image below shows how the update page for an item looks. Update pages are great for collaboration and communication within a team working on the same board. Users can mention each other to send updates or trigger notifications — which is another handy way to notify a reviewer. Approvals in Monday.com So, Approvals in Monday.com can happen in two ways: either through automations (for example, when a status changes to something like “Approved” or “Declined” and triggers a notification), or through mentions in the update feeds. The following image shows a not ification for a mention in an update. The owner also can add people as guests (viewers). Versioning & Markup s Monday.com supports file versioning and commenting, but doesn’t support markups. Each version of a file can have its own comments. Users who subscribe to a b oard get notified about file comments — since the app treats them as updates, not formal approvals.\\nInstead of traditional markups, Monday.com lets users comment directly on spots within files and mention others — kind of like sticky notes inside a PDF. The image below shows how this works. Managing documents Monday.com doesn’t have full document management — meaning, there’s no centralized place to organize all project documents. It leans more toward being a task management tool than a document repository. The re is a file log, but it’s specific to each task (item). As status are not seen as real status, just words, after a status is done, it is still open for discussions and updates. The only way to “close” it is archiving it, which sends it to the “archived it ems” file. This can easily be automated as follows: when status changes to done , archive item . It can also be moved to a group, may be called completed, with another automation. The following image shows this automation from the automation board, where pre viously made ones are saved for later usage. Workflow Builder Beyon d single automations, Monday offers a block -style Workflow builder for complex routines — multi -step processes with triggers, conditions, and actions. It’s perfect for repeatable projects, letting you package entire logic flows (e.g., onboarding sequences, r ecurring audits) into reusable templates. Pricing • Free Plan • For up to 2 users • Includes unlimited boards and docs • 200+ templates and 8 column types • Limited to 1,000 items and 500 MB of storage\\n• Basic Plan – Approx. $9/user/month • Unlimited items • 5 GB file sto rage • Prioritized customer support • Dashboard based on 1 board • Standard Plan – Approx. $12/user/month • Timeline, Gantt, and calendar views • Automations (250 actions/month) • Integrations (250 actions/month) • Dashboard combining up to 5 boards • Pro Plan – Approx. $19/user/month • Time tracking • Advanced reporting with dashboards • Private boards and docs • Automations & integrations (25,000 actions/month) • Dashboard combining up to 10 boards • Enterprise Plan – Custom pricing • Enterprise -grade security & governance • Advanced reporting & analytics • Multi -level permissions • 24/7 premium support • Dashboard combining up to 50 boards Conclusion Monday.com excels when you need a flexible “build -it-yourself” platform —its hierarchy and automation engine let you sculpt nearly any process. Just be ready to invest time configuring those rules, because it doesn’t ship with construction -grade approval or document -centered feature s out of the box. Pros: 1- Simple UI. 2- Highly Customizable. 3- Powerful Automation Engine. 4- Structured Hierarchy. 5- File Versioning & Inline Comments.\\nCons: 1- Initial Setup Complexity. 2- Limited Document Management. 3- Columns are just labels with no built -in logic. 4- No Native Review/Approval Workflow. 5- Dependence on Manual Discipline, Task progress and transitions rely on users remembering to change statuses or update tasks. 6- Flat N otification System, a notification triggered is just an alert —it doesn’t track whether the recipient acted on it or acknowledged it. Evaluating Monday.com: Feature Supported in Monday.com? Assigning roles ❌ No (only users with permissions) Document approval workflow Partially (via automation) Versioning ✅ Yes Markup versions ❌ No (spot commenting) Auto -changing status ❌ No Financial oversight Limited (Budget column + automations) Bids management ❌ No native feature Document management system ❌ No Dashboards ✅ Yes Reporting ✅ Yes Notifications ✅ Yes Project archiving ✅ Yes\\n2- Wrike Overview Wrike structures work in four layers —Spaces , Folders , Projects , and Tasks —letting you mirror your organization’s hierarchy or project phases however you like. Out of the box, each task comes with a Status field that doubles as a “workflow”: you define your own status labels (e.g., Draft, In Review, Approved), assign approvers to each status, and Wrike automatically notifies the right people when a task moves along. Beyond approvals, Wrike’s standout featur es include Blueprints (reusable templates for projects or folders), Backlog & Workload views for capacity planning (drag tasks from a backlog into a user’s timeline), and Task Locks that prevent changes once a task is signed off. Communication happens in-context via the Updates Panel on tasks and projects, and its built -in automation engine (“WHEN… THEN…”) can further streamline assignments, status changes, and alerts. Assigning Role s When creating an item ( a project/ a task), there is less fields that describe it (only in free plan, they can be customized in others), but they are more effective. As they have actions that can be done without creating automations. Wrike allows you to define job roles for users, primarily for organizing . However, tasks cannot be assigned directly to a job rol e, but they can be assi gned to individuals or “user groups ” (will get clearer in Workloads and Backlogs section).\\nWorkflows in wrike: Status field describes the item is a key in wrike, they are often also called “workflows”. They can be modified and added as needed to make approvals easier. Workflows are so powerfull, as you can create a worfkow for each project or even for each task, as reviewers are more likely to change from a task to another. Approvals in Wrike : Approvals here are not just a checkbox or a status change — they are part of a workflow logic that lets the platform understand wh o should review what, when, and how. There are three main wasy approvals work in wrike: 1- Status -based Approvals: To explain this one lets first look at Status field in wrike (often also called “workflows”). Workflows in wrike can be modified and added as n eeded to make approvals easier. When a task’s status is updated to any of these, Wrike automatically triggers the relevant people to review — such as the project manager, team lead, or specific stakeholders. Each status can have specific individuals who a re either assigned or notified, making this automation flexible and powerful. 2- Manual Approval Request by Assignees: In some cases, the person who creates the task (e.g., product manager or team lead) may initiate the approval process themselves. They can define who will review, when the deadline is, and then the assignee simply uploads the deliverable and submits it for review.\\n3- Manual Approval Requests by Task Creators: In some cases, the person who creates the task ( e.g., product manager or team lead) may initiate the approval process themselves. They can define who will review, when the deadline is, and then the assignee simply uploads the deliverable and submits it for review . Visual Editing in File approvals: When an approver reviews a PDF or an image, they can use built -in markup tools to circle, highlight, or annotate directly on the file. They can also leave a note at the exact spot where changes are needed. Wrike not only supports full file versioning but a lso comparing two versions aside with marking up both. Which makes reviewing process much better and easier. Task Locks After Approval Once a task goes through the approval process and receives a green light, Wrike a llows that task to be locked — meaning no further edits can be made unless explicitly allowed. This is a subtle but powerful feature, especially for teams working on contracts, final deliverables, or compliance -sensitive materials. Locking tasks ensures th at once something is signed off, it remains in its approved state, free from accidental changes or updates.\\nBlueprints in Wrike (commonly defined as workflows, but not here) Blueprints in Wrike are template -based structures that allow teams to create consistent tasks, projects, or folders without immediately activating or assigning them. Think of them as predefined frameworks that help standardize recurring work —such as campaign plans, onboarding checklists, or sprint cycles —without cluttering the live workspace. • Storage : Blueprints are stored in a dedicated “Blueprints” section, separate from active projects. • Creation : You can create a blueprint for tasks, projects, or folders. These can include custom fields, assignees, durations, dependencies, and more. • Convers ion: When you\\'re ready, you can convert a blueprint into a live item —scheduled or unscheduled, assigned or not. • Notifications : Using blueprints allows you to prepare work without triggering notifications, and you can reuse them repeatedly to save time and ensure consistency. Backlogs and Workloads: The Backlog is a key planning tool in Wrike. It’s the space where that gather s all “ideas ”, tasks, or requests that are not yet scheduled or assigned. When a task exists but you don\\'t yet know who will do it, when it will be done, or how it should be executed — you simply leave it unassigned, and Wrike will recognize it to be in the backlog box (this will get clearer in a minute). Where do we assign people tasks? In the workload The Workload View in Wrike prov ides a visual representation of team members\\' assigned tasks across a timelin e. Workloads are displayed in a table -like view: • X-axis: Represents time — days, weeks, or custom slots. • Y-axis: Lists users or job roles. • Slots: Represent blocks of time (usually 8 hours) assigned to each user. Once a task is ready to be executed, it can be moved from the backlog to the Workload section. When you drag a task from the backlog and drop it into a user\\'s timeline ( e.g., assign a design task to Kristen for Thursday), that task is now officially assigned and the user is notified.\\nLinking both of them: The owner/PM can structure the projects/tasks as needed, even if there are tasks that are known to be done but not yet scheduled. If it\\'s known which t eam is responsible for it, the task can be assigned to a job role , and no notifications will be sent —helping teams brainstorm without cluttering everyone\\'s inbox. Tasks are also allowed to be easily moved from one person to another, as demonstrated in th e following image. Communication and Updates Pan els Wrike does have updates panel, for both tasks and projects — where users can leave comments, mention others, upload attachments, and track communication over time. But there is no home feeds page like Monday.com, which gives it more formal touch. Automations in Wrike: Wrike’s automation engine is highly customizable, designed to streamline workflows and reduce the need for manual intervention. It follows a rule-based automation system where you can set up custom rules using a “WHEN... THEN...” structure. This allows for automating actions such as task assignments, status changes, and notifications based on predefined conditions.\\nPricing • Free Plan • For teams up to 5 users • Task and subtask management • Board and spreadsheet views • Real-time activity stream • 2 GB storage total • Team Plan – Approx. $9.80/user/month • For up to 25 users • Unlimited tasks, projects, and custom fields • Interactive Gantt charts • Automations (50 actions/user/month) • 20 GB storage total • Business Plan – Approx. $24.80/user/month • For teams of 5 –200 users • Custom workflows, request forms, and dashboards • Time tracking and approvals • Automations (200 actions/user/month) • 50 GB storage total • Enterprise Plan – Custom pricing • For larger organizations needing advanced se curity • SSO & 2FA, compliance policies • Admin permissions & audit reports • Automations (1,000 actions/user/month) • 100 GB+ storage total • Pinnacle Plan – Custom pricing (premium enterprise tier) • Advanced reporting, budgeting, and forecasting • Locked spaces an d job roles • Advanced integrations and analytics • Custom user types and permissions • 100 GB+ storage and top -tier automation\\nPros: 1- Native status -based approvals with automatic notifications 2- Task locking after approval. 3- Blueprints fo r reusable Templates. 4- Backlog and Workload views for better planni ng. 5- Highly Customizable Automation Cons: 1- No centralized DMS —attachments are tied to individual tasks 2- Limited Customization in the Free Plan. 3- No Ability to Assign Tasks to Job Roles in Workloads or notifying them on reviewal, they must be created as a user group. 4- Approval requests still require manual status c hanges 5- Absence of a Chat Feature. Feature Supported? Assigning roles ✅ Yes Document approval workflow ✅Yes Versioning ✅Yes Markup versions ✅Yes Auto changing status after submittals Partially (via automation) Financial oversight Limited (Budget field ) Bids management ❌No Native Features Document management system ❌ No Dashboards ✅ Yes (drag and drop so, so easy) Reporting ✅Yes Notifications ✅Yes Project archiving ✅Yes\\nConstruction -Focused PMS s: Havi ng explored versatile, general -purpose tools like Monday.com and Wrike, it’s clear that while they offer tremendous flexibility, they often require heavy customization to meet the nuanced needs of construction projects. Now, let’s shift our focus to purpose -built, construction -centric platforms — PMWeb, Aconex, and Procore. These solutions come with industry -tailored modules out of the box (think robust contract management, formal transmittals, and site -driven workflows), so you can hit the gro und ru nning without reinventing core processes . 3- PMWeb PMW eb is a purpose -built, cloud -based Project Lifecycle Management platform crafted specifically for capital construction projects —government infrastructure, large facilities, and expansive EPC endeavors. Acting as the “central brain” of your project, PMWeb u nifies budgets, contracts, schedules, RFIs, submittals, and close -out activities under one roof. Its intuitive interface and modular architecture keep data organized, approvals automated, and teams aligned from kickoff through final handover. Who Uses PMWeb? • Owners (like government agencies or corporations managing large portfolios) • Project Managers and Consultants • Contractors and EPC firms Modules vs Tools in PMWeb PMWeb’s power comes from two building blocks —Modules , which tackle major processes, and Tools , which add specialized functionality across those modules. Modules (major functional areas): • Cost Management – Budgets, contracts, change orders, forecasting • Schedules – Gantt charts, critical path, resource leveli ng • Engineering Forms – RFIs, submittals, drawing reviews • Visual Workflow – Drag -and-drop process automation • Portfolio – Top-down view of multiple projects Tools (cross -module utilities): Custom form builder, timesheets, BIM integration, vendor prequalifica tion, risk analysis, document manager, resource management, and more.\\n2. Deep Dive into Modules 2.1 Cost Management PMWeb’s Cost Management module is built around one core philosophy: complete financial control . From the moment you set your first budget to the final project close -out, this module gives you the visibility and tools you need to keep co sts accurate, accountable, and aligned with your plan. 2.1.1. Budgeting and Forecasting • Baseline Budgets Every project begins with a Baseline Budget —the original financial roadmap against which all spending and changes are measured. PMWeb lets you refine that roadmap over time by creating new budget versions whenever costs shift or scope evolves. Only one version —the “Approved” baseline —remains active for performance tracking, while earlier drafts are archived. • Budget Entry Methods PMWeb makes it e asy to build your budget, offering two straightforward approaches: • Manual Entry lets you line items directly into PMWeb’s templates. • Excel Integration allows simple copy -and-paste from your spreadsheets, so you can work in familiar tools before committ ing numbers to the system. 2.1.2. Budget Requests When the unexpected happens —scope changes, market swings, or new funding needs —you don’t have to redo your entire budget. Instead, PMWeb’s Budget Request feature lets you propose targeted adjustments to your approved plan. Whether you’re adding a new cost item, shifting funds between cost codes, or rebalancing one area against another, each request follows a built -in approval workflow. Stakeholder’s review, comment, and authorize changes, creating a transparent, traceable record of every financial decision. Once approved, the working budget updates instantly, while the frozen baseline remains intact for ongoing performance comparisons. 2.1.3 Cost Breakdown Structure (CBS) To see exactly where your money goes, PMWeb organizes costs using a Cost Breakdown Struc ture—think of it as a tree with branches at every level of detail:\\n• Top Level : The entire project and its total budget. • Lower Levels : Phases, disciplines, tasks, and sub -tasks, each with its own cost line. This hierarchical setup lets you drill down into granular figures or roll them up into a consolidated view —perfect for both high -level summaries and detailed audits. • Financial System Integration PMWeb doesn’t operate in a silo. You can map each CBS code directly to your organization’s account ing or ERP codes, ensuring that project costs flow seamlessly into your corporate financial systems. This alignment eliminates duplicate data entry, cuts down on errors, and reinforces consistent reporting across platforms. 2.2. Contract and Commitment Ma nagement In any capital project, the relationship between the owner, consultant, and contractor rests on clear, enforceable agreements —and PMWeb’s Contract and Commitment Management module ensures those agreements are tracked and controlled from start to f inish. A prime contract sits at the heart of that relationship. This master agreement between the project owner and the general contractor defines exactly what work will be performed, when it must be completed, how much it will cost, and how payments will be structured. In PMWeb, every critical detail of a prime contract is captured and organized: the total contract value, the full scope of work, the agreed payment schedule, the contractor’s information, and even the exact CBS (Cost Breakdown Structure) cod es tied to each line item. 2.3. Visual Workflows in PMWeb: Workflows can automate tasks, approvals, and notifications, improving overall productivity and communication among team members.\\nKey Benefits & Features: • Role -Based Assignments: You define who does what. Whether it’s the Project Manager approving budgets or the Cost Controller reviewing change requests, each task is automatically sent to the right person. • Flexible Process Design: Build your workflow exactly as your team works it —sequencing steps like Budget Approval, Change Order Review, and Invoice Processing, inserting decision points for “approve” or “reject,” and even allowing parallel reviews when multiple stakeholders need to weigh in. • Smart Approval Rules: Tie approvals to financial thresholds and organizational hierarchies so that small expenses breeze through quickly, while larger commitments rise to senior leadership for sign -off. • Approval hierarchies: PMWeb’s approval hierarchies ensure that every document or subm ission follows a clear, pre -defined review path. Once a user uploads a file and clicks “Save,” the system automatically routes it to the next approver in the chain. If any reviewer rejects the file, PMWeb sends it back to the previous stage for correction, maintaining a strict sequence and eliminating confusion over who’s responsible for each step.\\nWhen a reviewer logs in, their dashboard highlights any pending approvals at their level, complete with notifications and links to the uploaded document Note: If an approval stall —because someone is unavailable or misses the deadline —the system can be configured to automatically escalate the request to the next higher authority. 2.4. Custom forms in PM Web & Document Managem ent System These forms can be configured to include an attachment section that links directly to the Document Management System (DMS) . This integration ensures that any file uploaded through a custom form is automatically stored and organized within the central DMS repository. The documents can be tagged with metadata, associated with the correct project or workflow, and retrieved later from either the DMS or the specific form they were attached to. Conclusion PMWeb delivers enterprise -grade financial and contract con trols alongside flexible approval workflows, making it ideal for large capital programs where precision and traceability are non - negotiable. Its depth comes with complexity —expect a learning curve —but once configured, it centralizes everything from bids th rough closeout in one powerful platform.\\nPricing PMWeb does not publicly list its pricing on its website, because its licensing is highly customized based on the client\\'s needs . Pricing typically varies depending on: Factors That Affect PMWeb Pricing: • Number of named users (usually sold as concurrent or named licenses) • Scope of modules selected (e.g., document management, cost management, scheduling, etc.) • Type of deployment: Cloud (SaaS) vs. On-premise • Level of customization , integration, and implementation support needed • Training, onboarding , and support services included Estimated Prici ng Range (based on industry sources and client reports): • Startup Costs: $30,000 – $60,000+ for small teams • Per User (Enterprise): $100 –$200 per user/month (approximate, depending on modules and services) • Implementation & Setup: Can cost tens of thousands of dollars , especially for complex enterprise rollouts Pros: 1. Strong Financial Control Mechanisms 2. Adaptive Workflow Automation 3. Embedded Bid Management Capabilities 4. Automated Approval Hierarchies 5. Comprehensive Document Management System Integration 6. Unified Contract Management Platform Cons: 1. Dependent on Consistent Data Practices 2. Requires a Steep Learning Curve 3. Lack of Markup Support for Submittals 4. Subpar User Interface Design 5. Limited Automation Capabilities 6. No Task Managem ent 7. Basic Dashboard Functionality\\nFeature Supported? Assigning roles ✅ Yes Document approval workflow ✅Yes Versioning ✅Yes Markup versions ❌ No Auto changing status after submittals ✅Yes (no status but moves to next stage) Financial oversight ✅Strongly Bids management ✅Yes Document management system ✅Yes Dashboards ✅ Yes (but so basic) Reporting ✅Yes Notifications ✅Yes Project archiving ?? Couldn’t tell\\n4- Aconex (by ORACLE) Aconex, developed by Oracle, is a cloud -based platform designed for managing documents and improving collaboration across teams and organizations. It’s especially popular in construction, engineering, and large infrastructure projects because it keeps everything in one place — from document management to workflow tracking and team communication. At its heart lies the Document Register , a cent ralized “filing cabinet” where every file —drawings, specs, submittals, contracts —must be uploaded before sharing. From there, Aconex’s powerful Workflows and Transmittals orchestrate multi -step reviews across companies, while the Tasks Panel , Mail , and Documents modules keep your team informed and accountable. On the job site, Aconex Field brings issue tracking and inspections to mobile users, ensuring real -time updates and a complete audit trail. Document Regis ter: Think of the Document Register as Aconex’s main repository . It\\'s where all the project documents are stored, and it’s a must to upload any document here before it can be shared with others. When you upload a document, you add key details like: • Document number • Title • Revision • Status This ensures everyone involved in the project has access to the most up -to-date and accurate information, keeping everything organized. Each company working on the project will have its own private section within the document register, which gives them control over who sees what. Workflows in Aconex: One of Aconex’s best features is its workflow system . It lets you create multi -step approval processes — whether it’s within your team or across multiple organizations. This means you can send documents for review, assign specific people to approve them, and track where things are in the process. Work flows are customizable, and you can save them as templates to reuse whenever needed. This makes managing documents and approvals much smoother. For example, when you upload a new design drawing, you can start a workflow where the engineering manager review s it, then the quality team checks it, and finally, it gets sent for construction approval.\\n• Creating a workflow • Initiating a workflow : Transmittals: A transmittal is how you formally share documents with other organizations or project participants. Once a do cument is uploaded to the register, you can start a transmittal, choose who to send it to, and include the reason for sending it. You can even link a transmittal to a workflow so the document gets reviewed and approved before it’s sent out. • No workflow : If the document is sent without a workflow, the recipient’s document register is updated automatically. • With workflow : If a workflow is attached, the document will only be uploaded after it’s been approved.\\nTasks Panel The Tasks Panel is the first screen users see when logging into Aconex. It helps users stay organized by listing outstanding items like: • Mails (communications) • Docu ments awaiting action • Packages • Supplier submissions Tasks are categorized based on document status — for example: • Awaiting Review • Overdue • Unread Shares • Pending Response These tasks are automatically generated when documents are sent for action , such as rev iew, approval, or information.\\nMail in Aconex: The Mail module is for everyday communication on the project — answering questions, discussing ideas, and sharing quick updates. While you can attach documents to em ails, remember that these attachments don’t get uploaded to the document register. The file is simply attached for viewing or download, and its metadata isn’t tracked. Mail is useful for: • Informal or one -time exchanges • Sensitive content meant for specific users only Documents Panel: Formal Communication & Tracking he Documents tab is where you’ll find all the received or sent documents. Documents are sorted based on the reason for issue , like: • For Review • For Information • Issued for Construction In this tab, you can also access two important sections: • Awaiting Your Approval — Documents you need to review. • Documents You Issued — Documents for which you’v e initiated workflows. It’s an easy way to keep track of everything that’s in the approval pipeline. The follo wing im ages show trackin g documents in review, whether for approval :\\nOr awaiting your approval : Aconex Field: Aconex Field is perfect for teams working on -site. It allows them to raise issues, conduct inspection s, and track quality in real -time. Field teams can create issues directly from the site, assign them to the right people, and track their resolution without waiting for emails or meetings. Key Features: • Issue Management : Raise issues on -site, add photos wi th markups, and assign them for resolution. • Custom Inspection Forms : Tailor inspection checklists to fit the specific project needs. • Real -Time Updates : As soon as an issue is assigned, the person responsible gets notified in - app and by email. • Audit Trail : Every action is logged, so you always know who did what and when. Typical Issue Resolution Workflow: 1. An issue is created with all necessary details and attachments. 2. The assignee gets notified and logs in to fix the issue. 3. They upload a photo or proof of the fix and mark the issue as “Resolved.” 4. The original person who raised the issue verifies the fix and closes it. It’s a seamless process that keeps things moving, especially when you need to stay on top of on -site tasks.\\nPricing: Just like PMWeb, Oracle Aconex does not publicly list pricing because it\\'s offered as an enterprise solution , and pricing is tailored to the size, complexity, and duration of your project. Factors Affecting Aconex Pricing: • Project size and duration (short -term vs long -term) • Number of users and collaborators • Scope of modules (document control, BIM, cost management, workflow automation) • Cloud storage volume • Required integrations (e.g., ERP systems) • Training, implementation, and support level Estimated Pricing Range (from user reports and RFPs): • Per Project License (Annual): Starting around $15,000 –$30,000+ • Enterprise Multi -Project License: Can go $100,000+/year • Per User (if applied): ~$100 –$150/user/month (not always the model used) Pros: 1- Centralized Document Management 2- Seamless Collaboration Across Organizations 3- Customizable Workflows 4- On-Site Issue Management\\nCons: 1- No low -code automations beyond core mo dules 2- No contract features . 3- Limited Task Management Capabilities 4- Outdated Design . Evaluating Aconex: Feature Supported? Assigning roles ✅ Yes (even across organizations) Document approval workflow ✅Yes Versioning ✅Yes Markup versions ✅Yes Auto changing status after submittals No statuses n eeded Financial oversight ✅Yes (Through the Connected Cost ) Bids management ✅Yes (fully automated through Tenders & Bids module ) Document management system ✅Yes (using Document Register ) Dashboards ✅ Yes (Dashboard Widgets and KPI views ) Reporting ✅Yes Notifications ✅Yes Project archiving ✅Yes\\n5- Procore Procore is a cloud -based construction management platform that brings every stakeholder —owners, contractors, architects, engineers —into a single, real -time collaboration space. Think of it as the central nervous system for your project: from initial prequalification and tendering through closeout, Procore keeps information flowing, approvals moving, and decisions transparent. Prequalification & Tender Management A construction project begins when the owner defines their vision and vets the market. During prequalification , owners deploy customized questionnaires —capturing financial stability, safety records, project exp erience, and more —and review past -performance data to identify which contractors are eligible to bid. Procore’s Prequalification tool lets you build these tailored forms and hosts them in a secure portal, streamlining both submission and review. Once a sho rtlist is set, the owner issues a bid package —a collection of: • Invitation to Bid (ITB) • Instructions to Bidders • Bill of Quantities (BOQ) • Technical specifications (compiled in the specification book) • Drawings • Contract conditions Procore’s Bid Management tools centralize all these documents: upload them once, notify invitees automatically, track who’s opened or confirmed, and manage any clarifications or addenda —all without chasing emails or juggling versions.\\nPreparing the Specification Book & Pre -Bid Submittal Log Before any bids come in, the Specification Book —the architect’s detailed “recipe” of materials, quality standards, testing protocols, and workmanship expectations —must be finalized and published in Procore. This static, authoritative docum ent serves both bidders and later the construction team. At this stage, Procore’s Submittal Builder can scan the spec book (using OCR) and extract every item tagged under “Submittals,” creating a draft submittal register that tells contractors, “Here’s exactly what you’ll need to provide once you win” . Sharing this early ensures that every bidder prices against the same list, making cost and schedule comparisons apples -to-apples. Bid Submission: What Contractors Actually Send When contractors respond, they focus on commercial readiness rather than technical deliverables. Their bid documents typically include: • Completed bid form and fully -priced BOQ • Scope clarifications • Proposed construction schedule • Key team resumes • Insurance certificates and safety plans • Any required method statements Procore lets bidders upload all attachments directly into the platform, automatically time -stamping each submission and instantly notifying the owner —so there’s no version confusion or lost files. Awarding the Contract: Based on Bid Documents, Not Submittals With all bid s in hand, Procore\\'s Bid Management tool offers features like bid leveling for side -by- side comparison of submissions and a centralized dashboard to monitor all bid activities.\\nThe owner and consultant evaluate price, timing, experience, and any value -adds. The contract is awarded to the bidder who best meets those criteria . Once signed, the real work begins: turning that draft submitt al register into active tasks. Construction -Phase Submittals: Execution & Review Once the contract is signe d, your draft submittal register transforms into Procore’s central Submittal’s workspace, where every required deliverable —shop drawings, product data sheets, material samples, QA/QC reports, mock -ups, and, later, O&M manuals and as -built drawings —is catalo gued and tracked. Behind the scenes, Procore’s Submittal Builder (which parsed your specification book) hands off this list, so you never have to manually create each line item. Each submittal entry is paired with a Workflow Template , defining exactly who must review or approve —and in what sequence. As soon as a submittal is “Sent for Review,” Procore: • Auto -routes the package to the first reviewer . • Sends notifications (email + in -app) to each reviewer when it’s their tur n, and reminders if due dates slip. • Tracks status in real time, updating fields like “Submitted for Review,” “Revise & Resubmit,” or “Approved” the moment an approver clicks their decision. Within each submittal , you can attach PDFs, CAD expo rts, or images and even use Procore’s built -in Markup tools to annotate directly on drawings —no external software required. Every action is recorded in the Change History , building a timestamped audit trail from initial submission through final approval, a nd if you need to revise, Procore’s Version Control links new uploads back to the original, ensuring field teams always retrieve the latest, approved version. Drawing Management: Overlaying, Versioning, and Revisions As submittals advance through the review process, th e corresponding drawings are continually updated. Procore’s Drawings tool serves as your single source of truth for every plan set and detail sheet. Upload a multi -page PDF and Procore automatically splits it into individual sheets —savin g\\nyou hours of manual slicing. When revised drawings arrive, Procore’s OCR engine recognizes sheet numbers and titles, links new files as formal revisions, and preserves a complete revision history, so you can always see exactly which file is “Rev A” vers us “Rev B.” The real magic comes with Overlay , which lets field teams compare any two revisions on -screen. By toggling between or sliding one sheet atop the other, users instantly spot changes —no lengthy side-by-side manual reviews needed. And because each drawing revision can be linked to its corresponding submittal, reviewers know they’re always looking at the exact version under review, closing the loop between document control and approval workflows. Pros: 1- AI-Powered Drawing Splitting & Indexing 2- Auto -Tracked Submittal Workflows with Full Audit Trail 3- Bids management 4- Submittal s tool using OCR 5- Overlaying drawings Cons: 1- Steep learning curve 2- Limited customization compar ed to other pla tforms. Evaluating Procore Feature Supported in Procore? Assigning roles ✅ Yes Document approval workflow ✅ Yes Versioning ✅ Yes Markup versions ✅ Yes Auto changing status after submit tals ✅ Yes Financial oversight ✅ Yes Bids management ✅ Yes Document management system ✅ Yes Dashboards ✅ Yes Reporting ✅ Yes Notifications ✅ Yes Project archiving Partially\\nRenewable and Solar -Focused Platforms During our research, we also explored platforms that are specifically designed for the renewable energy sector and others that are solar PV -focused . At first, they seemed promising because they’re built with energy systems in mind. However, after testing and reviewing them, we found that they don’t meet the type of project management needs we’re aiming for , although they market them selves as they have project management tools. Platforms like Ra Power Management (RaPM) , SenseHawk, and Payac a are examples of solar - focused systems. These tools are mainly designed for monitoring the performance of solar plants — such as tracking electricity production, system health, faults, and maintenance alerts. While they’re excellent for operations and post -installation monitoring , they do not support document approvals, workflows, submittals, or collaboration between stakeholde rs like contractors, consultants, and clients. In short, these are more like monitoring or asset management tools , not project management systems. They don’t offer: • Workflow approvals • Version control • Submittal coordination • Role -based task assignment • Dashboards for tracking deadlin es and project progress Which are all essential for managing complex engineering or construction projects. Conclusion This comparative analysis of Monday.com, Wrike, PMWeb, Aconex, and Procore reveals a diverse landscape of project management platforms, each with its own strengths, trade -offs, and degrees of specialization for the construction industry. Monday.com and Wrike , while initially designed as general -purpose project management tools, offer intuitive interfaces, flexible dashboards, and excellent task tracking capabilities. However, they fall short on advanced construction -specific workflows such as submittal tracking, formal transmittals, or OCR -based document processing. In contrast, PMWeb, Aconex, and Procore are built with the complexities of construction in mind. Procore stands out for its rich construction -focused toolkit, including automated submittals, AI - powered drawing management, and integrated bid handling. Aconex excels in st ructured communication, transmittals, and formal document control, making it a go -to choice for large -scale infrastructure projects. PMWeb offers impressive breadth with strong cost control, scheduling, and portfolio -wide visibility, though it may come wit h a steeper learning curve.\\nTo synthesize these findings, a feature matrix has been developed that quantifies how well each platform supports a core set of functionalities —from role assignment and approval workflows to bid management, markup tools, notific ations, and financial oversight. The matrix not only highlights individual platform strengths but also helps identify potential gaps and overlaps. Feature Monday.com Wrike PMWeb Aconex Procore 1. Assigning roles 10% 85% 80% 80% 80% 2. Document approval workflow 60% 100% 100% 100% 100% 3. Versioning 100% 100% 100% 100% 100% 4. Markup versions 50% 100% 0% 90% 100% 5. Auto -changing status after submittals 60% 75% 100% 80% 100% 6. Financial oversight 20% 20% 100% 90% 90% 7. Bids management 0% 0% 80% 80% +100% 8. Document management system (DMS) 0% 0% 80% 100% 80% 9. Dashboards 100% 100% 50% 80% 100% 10. Reporting 80% 90% 100% 100% 100% 11. Notifications 80% 90% 100% 100% 100% 12. Project archiving 30% 100% 100% 100% 70%',\n",
       "  'qa_output': '<think>\\nOkay, I need to generate 2-3 meaningful questions and their answers based on the provided text. Let me start by reading through the text carefully to understand the key points.\\n\\nThe text compares several project management platforms: Monday.com, Wrike, PMWeb, Aconex, and Procore. Each has different strengths and weaknesses, especially in construction-specific features like submittals, workflows, document management, and bid handling. There\\'s also mention of renewable energy platforms that are more monitoring tools than project management systems.\\n\\nFirst, I should identify the main features discussed. The feature matrix at the end shows percentages for each platform\\'s support of various functionalities. For example, PMWeb has high scores in versioning and document management, while Monday.com and Wrike are weaker in construction-specific workflows. Procore excels in automated submittals and AI-powered drawing management. Aconex is noted for structured communication and formal document control. The text also highlights that some platforms like Ra Power Management are more for monitoring than project management.\\n\\nNow, I need to form questions that are based on this information. Let me think of possible questions. One could be about which platform excels in bid management. Another might ask about the main limitation of Monday.com and Wrike. A third could be about the key difference between Procore and Aconex. I need to ensure each question is based solely on the text and that the answers are accurate according to the provided data.\\n\\nLet me check the feature matrix again. For bid management, PMWeb and Aconex have 80% and 80%+100% respectively. Wait, the text says Aconex has 80% and Procore has 100%? Wait, looking back, the text says for PMWeb, bids management is 80%, Aconex is 80%+100%? Wait, the matrix shows for PMWeb, bids management is 80%, Aconex is 80%+100%? Wait, the original text mentions that Aconex excels in structured communication, transmittals, and formal document control. The feature matrix for bids management: PMWeb 80%, Aconex 80%+100%? Wait, the text says \"Aconex excels in structured communication, transmittals, and formal document control, making it a go-to choice for large-scale infrastructure projects.\" But the matrix for bids management shows PMWeb at 80%, Aconex at 80%+100%? Wait, maybe there\\'s a typo. Let me check again.\\n\\nLooking at the feature matrix:\\n\\nFor bids management:\\n- PMWeb: 80%\\n- Aconex: 80%+100% (maybe that\\'s a typo, but according to the text, Aconex has 80% and Procore has 100%? Wait, the text says \"Procore stands out for its rich construction-focused toolkit, including automated submittals, AI-powered drawing management, and integrated bid handling.\" So Procore\\'s bid management is 100%? The matrix shows for Procore, bids management is 100%, and Aconex is 80%+100%? Wait, maybe the matrix has a typo. Let me check the original text again.\\n\\nIn the text, under \"Evaluating Procore,\" it says \"Bids management ✅ Yes\" and under \"Evaluating Aconex,\" it says \"Bids management ✅ Yes\" but the matrix for bids management shows PMWeb at 80%, Aconex at 80%+100%, and Procore at 100%. Wait, the matrix might have a formatting issue. Let me recheck.\\n\\nThe feature matrix is:\\n\\n1. Assigning roles 10% 85% 80% 80% 80%\\n2. Document approval workflow 60% 100% 100% 100% 100%\\n3. Versioning 100% 100% 100% 100% 100%\\n4. Markup versions 50% 100% 0% 90% 100%\\n5. Auto-changing status after submittals 60% 75% 100% 80% 100%\\n6. Financial oversight 20% 20% 100% 90% 90%\\n7. Bids management 0% 0% 80% 80% +100%\\n8. Document management system (DMS) 0% 0% 80% 100% 80%\\n9. Dashboards 100% 100% 50% 80% 100%\\n10. Reporting 80% 90% 100% 100% 100%\\n11. Notifications 80% 90% 100% 100% 100%\\n12. Project archiving 30% 100% 100% 100% 70%\\n\\nSo for bids management, PMWeb is 80%, Aconex is 80%+100% (maybe that\\'s a typo, but according to the text, Aconex has bids management as \"✅ Yes\" in the evaluation section. Wait, the text says under \"Evaluating Aconex\": \"Bids management ✅ Yes\" and \"Document management system (DMS) ✅ Yes\". So in the matrix, for bids management, Aconex is 80%+100%? That might be a formatting error. Let me check the original text again.\\n\\nIn the text, under \"Evaluating Procore,\" it says \"Bids management ✅ Yes\" and \"Document management system (DMS) ✅ Yes\". For Aconex, the evaluation says \"Bids management ✅ Yes\" and \"Document management system (DMS) ✅ Yes\". So in the matrix, for bids management, PMWeb is 80%, Aconex is 80%+100% (maybe that\\'s a typo, but according to the text, Aconex has bids management as \"✅ Yes\" which would correspond to 100%? Maybe the matrix has a formatting error where \"80%+100%\" is meant to be 100%? Or perhaps the matrix is indicating that Aconex has 80% and Procore has 100%? Wait, the matrix for bids management shows PMWeb at 80%, Aconex at 80%+100%, and Procore at 100%. That seems inconsistent. Maybe the matrix is trying to say that Aconex has 80% and Procore has 100%? Or perhaps the \"+100%\" is a mistake. This is a bit confusing, but I need to proceed based on the text provided.\\n\\nAnother point is that the text mentions that Procore has AI-powered drawing management and integrated bid handling, which would imply that Procore\\'s bid management is strong. Aconex is noted for structured communication and formal document control, which might not directly relate to bid management. The text also says that PMWeb has strong cost control, scheduling, and portfolio-wide visibility, but a steeper learning curve.\\n\\nSo possible questions could be:\\n\\n1. Which platform excels in bid management according to the feature matrix?\\n2. What is a key limitation of Monday.com and Wrike in the context of construction projects?\\n3. How does Procore differ from Aconex in terms of construction-specific features?\\n\\nNow, I need to check the answers based on the text. For the first question, the feature matrix shows PMWeb at 80%, Aconex at 80%+100% (maybe that\\'s a typo for 100%), and Procore at 100%. The text says Procore has integrated bid handling, so Procore might be the answer. But the matrix might have an error. Alternatively, the text says PMWeb has 80% for bids management, Aconex has 80%+100% (maybe meaning 80% and 100%?), and Procore has 100%. So Procore would be the one with 100% in bid management. \\n\\nFor the second question, the text says Monday.com and Wrike are weaker in construction-specific workflows like submittal tracking, formal transmittals, or OCR-based document processing. \\n\\nFor the third question, Procore has AI-powered drawing management and automated submittals, while Aconex is more about structured communication and formal document control. \\n\\nI need to make sure the answers are based strictly on the text provided. Let me verify each point again.\\n</think>\\n\\nQ1: Which platform excels in bid management according to the feature matrix?  \\nA1: Procore  \\n\\nQ2: What is a key limitation of Monday.com and Wrike in the context of construction projects?  \\nA2: They lack advanced construction-specific workflows such as submittal tracking, formal transmittals, or OCR-based document processing.  \\n\\nQ3: How does Procore differ from Aconex in terms of construction-specific features?  \\nA3: Procore focuses on automated submittals, AI-powered drawing management, and integrated bid handling, while Aconex excels in structured communication, transmittals, and formal document control.',\n",
       "  'parsed_qa': [{'question': 'Which platform excels in bid management according to the feature matrix?',\n",
       "    'answer': 'Procore'},\n",
       "   {'question': 'What is a key limitation of Monday.com and Wrike in the context of construction projects?',\n",
       "    'answer': 'They lack advanced construction-specific workflows such as submittal tracking, formal transmittals, or OCR-based document processing.'},\n",
       "   {'question': 'How does Procore differ from Aconex in terms of construction-specific features?',\n",
       "    'answer': 'Procore focuses on automated submittals, AI-powered drawing management, and integrated bid handling, while Aconex excels in structured communication, transmittals, and formal document control.'}]}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Question.run(individual_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
